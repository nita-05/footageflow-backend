
import os
import json
import base64
import uuid
import functools
import hashlib
import subprocess
import threading
import tempfile
import sqlite3
import time
from datetime import datetime
import re
from flask import Flask, request, jsonify, send_file
from flask_cors import CORS
from google.oauth2 import id_token
from google.auth.transport import requests
from werkzeug.utils import secure_filename
from google.cloud import speech
from google.cloud import storage
import google.genai as genai
# from google.cloud import firestore
# import firebase_admin
# from firebase_admin import credentials, firestore
from dotenv import load_dotenv
try:
    import cv2  # OpenCV used by the new frame-based tagging route
except Exception:
    cv2 = None
from transcribe import TranscriptionService
from tagging import VisualTaggingService

# Load environment variables
load_dotenv()

app = Flask(__name__)
CORS(app)

# Initialize Whisper model globally (optimized for cloud deployment)
WHISPER_MODEL = None
WHISPER_ENABLED = os.getenv('WHISPER_ENABLED', 'true').lower() == 'true'

if WHISPER_ENABLED:
    try:
        from faster_whisper import WhisperModel
        # Use tiny model for cloud deployment (much smaller memory footprint)
        model_size = os.getenv('WHISPER_MODEL_SIZE', 'tiny.en')
        compute_type = os.getenv('WHISPER_COMPUTE_TYPE', 'int8')
        
        print(f"üîÑ Loading Whisper model: {model_size} with {compute_type}")
        WHISPER_MODEL = WhisperModel(model_size, device="cpu", compute_type=compute_type)
        print(f"‚úÖ Whisper {model_size} model loaded successfully")
    except Exception as e:
        WHISPER_MODEL = None
        print(f"‚ö†Ô∏è Whisper model not available: {e}")
        print("üìù Transcription will use Google Speech API or fallback methods")
else:
    print("üìù Whisper disabled via WHISPER_ENABLED=false")

# Initialize ffmpeg-python
try:
    import ffmpeg
    FFMPEG_AVAILABLE = True
    print("‚úÖ ffmpeg-python available")
except Exception as e:
    FFMPEG_AVAILABLE = False
    print(f"‚ö†Ô∏è ffmpeg-python not available: {e}")

# Ensure users table exists at runtime
def ensure_users_table():
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS users (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                email TEXT NOT NULL UNIQUE,
                name TEXT,
                password_hash TEXT NOT NULL,
                password_salt TEXT NOT NULL,
                created_at TEXT NOT NULL
            )
        ''')
        conn.commit()
        conn.close()
    except Exception as e:
        print(f"ensure_users_table error: {str(e)}")

# Initialize SQLite database for metadata storage
def init_database():
    """Initialize SQLite database for storing video metadata"""
    db_path = os.path.join(os.getcwd(), 'video_metadata.db')
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    # Create videos table
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS videos (
            video_id TEXT PRIMARY KEY,
            user_id TEXT NOT NULL,
            user_email TEXT NOT NULL,
            filename TEXT NOT NULL,
            local_path TEXT NOT NULL,
            file_size INTEGER NOT NULL,
            file_type TEXT NOT NULL,
            created_at TEXT NOT NULL,
            status TEXT NOT NULL,
            duration REAL,
            transcript TEXT,
            word_timestamps TEXT,
            visual_tags TEXT,
            story_ids TEXT
        )
    ''')
    
    # Create users table (simple auth store)
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS users (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            email TEXT NOT NULL UNIQUE,
            name TEXT,
            password_hash TEXT NOT NULL,
            password_salt TEXT NOT NULL,
            created_at TEXT NOT NULL
        )
    ''')

    # Create search index for better performance
    cursor.execute('''
        CREATE INDEX IF NOT EXISTS idx_videos_user_id ON videos(user_id)
    ''')
    cursor.execute('''
        CREATE INDEX IF NOT EXISTS idx_videos_created_at ON videos(created_at)
    ''')
    
    # Emotions table stores timeline emotion analysis per video
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS emotions (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            video_id TEXT NOT NULL,
            timestamp REAL NOT NULL,
            label TEXT NOT NULL,
            intensity REAL NOT NULL,
            created_at TEXT NOT NULL
        )
    ''')

    conn.commit()
    conn.close()
    print(f"Database initialized: {db_path}")

# Initialize database on startup
init_database()

def get_db_connection():
    """Get database connection"""
    db_path = os.path.join(os.getcwd(), 'video_metadata.db')
    return sqlite3.connect(db_path)

def save_video_metadata(video_metadata):
    """Save video metadata to database"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT OR REPLACE INTO videos 
            (video_id, user_id, user_email, filename, local_path, file_size, file_type, 
             created_at, status, duration, transcript, word_timestamps, visual_tags, story_ids)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            video_metadata['videoId'],
            video_metadata['userId'],
            video_metadata['userEmail'],
            video_metadata['filename'],
            video_metadata['localPath'],
            video_metadata['fileSize'],
            video_metadata['fileType'],
            video_metadata['createdAt'],
            video_metadata['status'],
            video_metadata.get('duration'),
            video_metadata.get('transcript'),
            video_metadata.get('word_timestamps'),
            json.dumps(video_metadata.get('visual_tags', [])),
            json.dumps(video_metadata.get('story_ids', []))
        ))
        
        conn.commit()
        conn.close()
        return True
    except Exception as e:
        print(f"Error saving video metadata: {e}")
        return False

def update_video_metadata(video_id, updates):
    """Update specific fields in video metadata"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # First, get existing metadata to preserve other fields
        cursor.execute('SELECT * FROM videos WHERE video_id = ?', (video_id,))
        existing_row = cursor.fetchone()
        
        if not existing_row:
            print(f"Video {video_id} not found in database")
            conn.close()
            return False
        
        # Build dynamic update query
        set_clauses = []
        values = []
        
        for key, value in updates.items():
            if key in ['transcript', 'word_timestamps', 'visual_tags', 'story_ids', 'duration', 'title', 'description', 'tags', 'emotional_tone', 'key_moments', 'generated_stories', 'thumbnail_path', 'content_hash', 'preview_path', 'favorite', 'hidden', 'stack_key']:
                if key in ['visual_tags', 'story_ids', 'tags', 'key_moments', 'generated_stories'] and isinstance(value, list):
                    value = json.dumps(value)
                elif key == 'word_timestamps' and isinstance(value, list):
                    value = json.dumps(value)
                set_clauses.append(f"{key} = ?")
                values.append(value)
        
        if set_clauses:
            query = f"UPDATE videos SET {', '.join(set_clauses)} WHERE video_id = ?"
            values.append(video_id)
            cursor.execute(query, values)
            conn.commit()
            print(f"Updated video {video_id} with fields: {list(updates.keys())}")
        
        conn.close()
        return True
    except Exception as e:
        print(f"Error updating video metadata: {e}")
        return False

def get_video_metadata(video_id):
    """Get video metadata from database"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute('SELECT * FROM videos WHERE video_id = ?', (video_id,))
        row = cursor.fetchone()
        conn.close()
        
        if row:
            return {
                'videoId': row[0],
                'userId': row[1],
                'userEmail': row[2],
                'filename': row[3],
                'localPath': row[4],
                'fileSize': row[5],
                'fileType': row[6],
                'createdAt': row[7],
                'status': row[8],
                'duration': row[9],
                'transcript': row[10],
                'visual_tags': json.loads(row[11]) if row[11] else [],
                'story_ids': json.loads(row[12]) if row[12] else [],
                'title': row[13],
                'description': row[14],
                'tags': json.loads(row[15]) if row[15] else [],
                'emotional_tone': row[16],
                'key_moments': json.loads(row[17]) if row[17] else [],
                'generated_stories': json.loads(row[18]) if row[18] else [],
                'thumbnail_path': row[19],
                'content_hash': row[20],
                'preview_path': row[21],
                'favorite': row[22],
                'hidden': row[23],
                'stack_key': row[24],
                'word_timestamps': json.loads(row[25]) if row[25] else []
            }
        return None
    except Exception as e:
        print(f"Error getting video metadata: {e}")
        return None

def get_all_videos(user_id=None):
    """Get all videos, optionally filtered by user_id"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        if user_id:
            cursor.execute('SELECT * FROM videos WHERE user_id = ? ORDER BY created_at DESC', (user_id,))
        else:
            cursor.execute('SELECT * FROM videos ORDER BY created_at DESC')
        
        rows = cursor.fetchall()
        conn.close()
        
        videos = []
        for row in rows:
            videos.append({
                'videoId': row[0],
                'userId': row[1],
                'userEmail': row[2],
                'filename': row[3],
                'localPath': row[4],
                'fileSize': row[5],
                'fileType': row[6],
                'createdAt': row[7],
                'status': row[8],
                'duration': row[9],
                'transcript': row[10],
                'word_timestamps': json.loads(row[11]) if row[11] else [],
                'visual_tags': json.loads(row[12]) if row[12] else [],
                'story_ids': json.loads(row[13]) if row[13] else []
            })
        
        return videos
    except Exception as e:
        print(f"Error getting all videos: {e}")
        return []

def search_all_videos(query, user_id=None):
    """Search across all videos in the database"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # Build search query
        search_query = f"%{query.lower()}%"
        
        if user_id:
            cursor.execute('''
                SELECT * FROM videos 
                WHERE user_id = ? AND (
                    LOWER(transcript) LIKE ? OR 
                    LOWER(visual_tags) LIKE ? OR
                    LOWER(filename) LIKE ?
                )
                ORDER BY created_at DESC
            ''', (user_id, search_query, search_query, search_query))
        else:
            cursor.execute('''
                SELECT * FROM videos 
                WHERE LOWER(transcript) LIKE ? OR 
                      LOWER(visual_tags) LIKE ? OR
                      LOWER(filename) LIKE ?
                ORDER BY created_at DESC
            ''', (search_query, search_query, search_query))
        
        rows = cursor.fetchall()
        conn.close()
        
        results = []
        for row in rows:
            video_data = {
                'videoId': row[0],
                'userId': row[1],
                'userEmail': row[2],
                'filename': row[3],
                'localPath': row[4],
                'fileSize': row[5],
                'fileType': row[6],
                'createdAt': row[7],
                'status': row[8],
                'duration': row[9],
                'transcript': row[10],
                'visual_tags': json.loads(row[11]) if row[11] else [],
                'story_ids': json.loads(row[12]) if row[12] else []
            }
            
            # Calculate relevance score
            score = 0
            if video_data['transcript'] and query.lower() in video_data['transcript'].lower():
                score += 10
            if video_data['visual_tags']:
                for tag in video_data['visual_tags']:
                    if query.lower() in tag.lower():
                        score += 5
            if query.lower() in video_data['filename'].lower():
                score += 3
            
            video_data['relevance_score'] = score
            if score > 0:
                results.append(video_data)
        
        # Sort by relevance score
        results.sort(key=lambda x: x['relevance_score'], reverse=True)
        return results
    except Exception as e:
        print(f"Error searching videos: {e}")
        return []

# Configuration (DISABLED Google Cloud Services - using only Gemini API)
# BUCKET_NAME = os.getenv('GCS_BUCKET', 'footage-flow-videos-468712')
# GCP_PROJECT_ID = os.getenv('GCP_PROJECT_ID', 'footage-flow-468712')

# Google Client ID for authentication (keep this for login)
GOOGLE_CLIENT_ID = os.getenv('GOOGLE_CLIENT_ID', '724469503053-4hlt6hvsttage9ii33hn4n7l1j59tnef.apps.googleusercontent.com')

# Disable Google Cloud Services
BUCKET_NAME = None
GCP_PROJECT_ID = None

# Initialize Gemini AI
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')
if GEMINI_API_KEY:
    try:
        # Use the new Google AI client with gemini-2.5-flash
        client = genai.Client(api_key=GEMINI_API_KEY)
        # Test the model
        test_response = client.models.generate_content(
            model="gemini-2.5-flash",
            contents="Hello"
        )
        print("Gemini AI initialized successfully with gemini-2.5-flash")
        gemini_client = client
    except Exception as e:
        print(f"Warning: Gemini AI initialization failed: {str(e)}")
        gemini_client = None
else:
    gemini_client = None
    print("Warning: GEMINI_API_KEY not set. Story generation will use enhanced mock data.")

# Upload configuration
UPLOAD_FOLDER = os.getenv('UPLOAD_FOLDER', 'uploads')
VIDEOS_DIR = os.path.join(UPLOAD_FOLDER, 'videos')
MAX_CONTENT_LENGTH = int(os.getenv('MAX_CONTENT_LENGTH', 500 * 1024 * 1024))  # 500MB
ALLOWED_EXTENSIONS = {'mp4', 'avi', 'mov', 'wmv', 'flv', 'webm'}

# Ensure uploads directory exists
os.makedirs(UPLOAD_FOLDER, exist_ok=True)
os.makedirs(os.path.join(UPLOAD_FOLDER, 'videos'), exist_ok=True)
os.makedirs(os.path.join(UPLOAD_FOLDER, 'renders'), exist_ok=True)

# Initialize services (Using Gemini API for everything)
# transcription_service = TranscriptionService(BUCKET_NAME, GCP_PROJECT_ID)
# tagging_service = VisualTaggingService(GCP_PROJECT_ID)

# Use Gemini API services
transcription_service = None  # Will be replaced with Gemini
tagging_service = None        # Will be replaced with Gemini

# Import universal video processor for 100% compatibility
try:
    from enhanced_video_processor import universal_processor, process_any_video, is_video_supported
    print("‚úÖ Universal video processor loaded - 100% video compatibility enabled")
except ImportError as e:
    print(f"‚ö†Ô∏è  Universal video processor not available: {e}")
    universal_processor = None

# Create upload directory if it doesn't exist
os.makedirs(UPLOAD_FOLDER, exist_ok=True)

def allowed_file(filename):
    return '.' in filename and \
           filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS


def _extract_json_block(raw_text: str) -> str:
    """Best-effort extraction of a JSON object from LLM output."""
    try:
        if not isinstance(raw_text, str):
            return ''
        text = raw_text.strip()
        # Remove common code fences
        if text.startswith('```'):
            # Strip leading and trailing backticks blocks
            lines = [l for l in text.split('\n') if not l.strip().startswith('```') and not l.strip().endswith('```')]
            text = '\n'.join(lines).strip()
        # Find outermost JSON braces
        start = text.find('{')
        end = text.rfind('}')
        if start != -1 and end != -1 and end > start:
            return text[start:end + 1]
        return text
    except Exception:
        return ''


def generate_text_tags_with_gemini(transcript_or_description: str, emotion: str = "") -> list:
    """
    Generate comprehensive content-aware tags from transcript/description.
    Returns a list of strings. Uses intelligent analysis when Gemini fails.
    """
    try:
        text = (transcript_or_description or '').strip()
        if not text:
            return []

        # Try Gemini first if available
        if gemini_client:
            try:
                focus_line = (f"""
- If an emotion is provided (\"{emotion}\"), prioritize tags that reflect that emotion.
- Include at least 2 emotion-aligned tags when supported by the input.
- Do not invent facts; only use concepts present or clearly implied by the input.
""" if emotion else "")
                input_excerpt = text[:4000]
                tagging_prompt = f"""
You are an AI tagging assistant for video content.

Instruction:
Automatically analyze the video content and tag important objects, people, locations, actions, and emotions. Use clear, specific keywords for each scene to make searching and organizing easy.

Requirements:
- Output must be JSON only, with a single field 'tags' that is an array of strings.
- Each tag must be a single word or short phrase (1‚Äì3 words), lowercase except proper nouns.
- Avoid duplicates, generic words, and full sentences. No explanations outside the JSON.
- Generate 15-20 comprehensive tags covering all aspects of the content.
{focus_line}

Example Input:
"A family is celebrating a birthday party. A child is blowing candles on a cake while others clap."

Example Output:
{{
  "tags": ["family", "birthday party", "cake", "child", "blowing candles", "clapping", "living room", "celebration", "candles", "birthday", "party", "family gathering", "children", "happy", "joyful", "indoor", "home", "special occasion"]
}}

Now analyze this input:
{input_excerpt}
"""

                response = gemini_client.models.generate_content(
                    model="gemini-2.5-flash",
                    contents=tagging_prompt
                )
                raw = getattr(response, 'text', '') or ''
                json_block = _extract_json_block(raw)
                try:
                    data = json.loads(json_block)
                except Exception:
                    # Try a second attempt: wrap as JSON if model returned a plain list
                    try:
                        data = {"tags": json.loads(json_block)}
                    except Exception:
                        data = {"tags": []}

                tags = data.get('tags') or []
                # Normalize to strings
                cleaned = []
                for t in tags:
                    if isinstance(t, str):
                        s = t.strip()
                        if s:
                            cleaned.append(s)
                    elif isinstance(t, dict) and 'tag' in t and isinstance(t['tag'], str):
                        s = t['tag'].strip()
                        if s:
                            cleaned.append(s)
                # Deduplicate preserving order
                seen = set()
                unique = []
                for s in cleaned:
                    key = s.lower()
                    if key not in seen:
                        seen.add(key)
                        unique.append(s)
                if len(unique) >= 10:
                    return unique[:20]  # Return up to 20 tags
            except Exception as e:
                print(f"Gemini text tagging failed: {str(e)}")
        
        # Fallback: Intelligent content analysis
        return generate_intelligent_tags_from_text(text, emotion)
        
    except Exception as e:
        print(f"Text tagging failed: {str(e)}")
        return generate_intelligent_tags_from_text(text, emotion)

def generate_intelligent_tags_from_text(text: str, emotion: str = "") -> list:
    """
    Generate comprehensive tags from text using intelligent content analysis.
    Returns 15-20 meaningful tags based on actual content.
    """
    import re
    
    text_lower = text.lower()
    tags = []
    
    # Extract key words and phrases
    words = re.findall(r'\b\w+\b', text_lower)
    word_freq = {}
    for word in words:
        if len(word) > 3:  # Only meaningful words
            word_freq[word] = word_freq.get(word, 0) + 1
    
    # Get top frequent words
    top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:15]
    key_words = [word for word, freq in top_words if freq > 1]
    
    # Content-specific tag generation
    content_themes = []
    
    # Food and drinks
    if any(word in text_lower for word in ['cup', 'cups', 'ice', 'snow', 'cone', 'cones', 'drink', 'beverage', 'food', 'eat', 'cook', 'kitchen']):
        content_themes.extend(['food', 'drinks', 'refreshments', 'culinary', 'kitchen', 'cooking'])
        if any(word in text_lower for word in ['cup', 'cups', 'ice', 'snow', 'cone', 'cones']):
            content_themes.extend(['snow cones', 'ice cream', 'cold drinks', 'desserts', 'treats'])
    
    # Family and people
    if any(word in text_lower for word in ['family', 'mom', 'dad', 'kids', 'children', 'son', 'daughter', 'parents']):
        content_themes.extend(['family', 'people', 'children', 'parents', 'family time', 'together'])
    
    # Activities and actions
    if any(word in text_lower for word in ['party', 'celebrate', 'birthday', 'fun', 'play', 'game', 'activity']):
        content_themes.extend(['celebration', 'party', 'fun', 'activities', 'entertainment', 'social'])
    
    # Emotions and feelings
    if any(word in text_lower for word in ['happy', 'joy', 'excited', 'fun', 'amazing', 'wonderful', 'great']):
        content_themes.extend(['happy', 'joyful', 'excited', 'positive', 'fun', 'enjoyment'])
    elif any(word in text_lower for word in ['sad', 'angry', 'frustrated', 'tired', 'worried']):
        content_themes.extend(['emotional', 'dramatic', 'intense', 'serious', 'challenging'])
    
    # Locations and settings
    if any(word in text_lower for word in ['home', 'house', 'room', 'kitchen', 'living', 'indoor']):
        content_themes.extend(['indoor', 'home', 'house', 'domestic', 'residential'])
    elif any(word in text_lower for word in ['outdoor', 'park', 'garden', 'beach', 'mountain', 'travel']):
        content_themes.extend(['outdoor', 'nature', 'travel', 'adventure', 'exploration'])
    
    # Time and duration
    if any(word in text_lower for word in ['morning', 'afternoon', 'evening', 'night', 'day']):
        content_themes.extend(['daytime', 'morning', 'afternoon', 'evening'])
    
    # Quality and characteristics
    if any(word in text_lower for word in ['good', 'great', 'amazing', 'wonderful', 'perfect', 'excellent']):
        content_themes.extend(['quality', 'excellent', 'amazing', 'wonderful', 'perfect'])
    
    # Business and work
    if any(word in text_lower for word in ['work', 'office', 'meeting', 'business', 'professional', 'job']):
        content_themes.extend(['work', 'business', 'professional', 'office', 'career'])
    
    # Add emotion-specific tags if provided
    if emotion:
        if emotion.lower() == 'positive':
            content_themes.extend(['uplifting', 'inspiring', 'joyful', 'optimistic', 'happy'])
        elif emotion.lower() == 'negative':
            content_themes.extend(['dramatic', 'intense', 'emotional', 'challenging', 'serious'])
        elif emotion.lower() == 'normal':
            content_themes.extend(['balanced', 'neutral', 'objective', 'clear', 'focused'])
    
    # Add key words from transcript
    content_themes.extend(key_words[:8])  # Add top 8 frequent words
    
    # Remove duplicates and clean up
    seen = set()
    final_tags = []
    for tag in content_themes:
        clean_tag = tag.strip().lower()
        if clean_tag and len(clean_tag) > 2 and clean_tag not in seen:
            seen.add(clean_tag)
            final_tags.append(tag)
    
    # Ensure we have at least 15 tags
    if len(final_tags) < 15:
        # Add generic but relevant tags
        generic_tags = ['video', 'content', 'recording', 'footage', 'media', 'digital', 'modern', 'contemporary']
        for tag in generic_tags:
            if tag not in seen:
                final_tags.append(tag)
                seen.add(tag)
                if len(final_tags) >= 15:
                    break
    
    return final_tags[:20]  # Return up to 20 tags


def generate_video_text_tags(video_metadata: dict, emotion: str = "") -> list:
    """Combine transcript/description text from saved metadata and generate emotion-biased text tags.

    Expects metadata shaped like what's stored in <video_id>_metadata.json, e.g.:
    {
        "transcription": {"transcript": "...", ...},
        "description": "...",
        ...
    }
    """
    try:
        text_sources = []
        tr_block = video_metadata.get('transcription') or {}
        if isinstance(tr_block, dict):
            transcript_text = tr_block.get('transcript', '') or ''
            if isinstance(transcript_text, str) and transcript_text.strip():
                text_sources.append(transcript_text.strip())
        desc = video_metadata.get('description') or ''
        if isinstance(desc, str) and desc.strip():
            text_sources.append(desc.strip())

        # If there is any other textual signal in metadata, append here in the future
        combined_text = ' '.join([t for t in text_sources if t])
        if not combined_text.strip():
            return []
        return generate_text_tags_with_gemini(combined_text, emotion)
    except Exception as e:
        print(f"generate_video_text_tags failed: {str(e)}")
        return []


def transcribe_video_with_gemini(video_path: str, video_id: str) -> dict:
    """
    Transcribe video using Gemini AI by analyzing video frames and audio.
    Returns dict with 'transcript', 'word_timestamps', 'confidence'.
    """
    try:
        if not gemini_client:
            return None
            
        # Extract audio using FFmpeg
        import subprocess
        import tempfile
        
        # Create temp audio file
        with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_audio:
            temp_audio_path = temp_audio.name
        
        # Extract audio
        cmd = ['ffmpeg', '-i', video_path, '-vn', '-acodec', 'pcm_s16le', '-ac', '1', '-ar', '16000', '-y', temp_audio_path]
        subprocess.run(cmd, capture_output=True, check=True)
        
        # Get video duration
        duration_cmd = ['ffprobe', '-v', 'quiet', '-show_entries', 'format=duration', '-of', 'csv=p=0', video_path]
        duration_result = subprocess.run(duration_cmd, capture_output=True, text=True, check=True)
        duration = float(duration_result.stdout.strip())
        
        # Create Gemini prompt for transcription
        prompt = f"""
        Analyze this video and provide a detailed transcript.
        
        Video duration: {duration} seconds
        Video ID: {video_id}
        
        Please provide:
        1. A complete transcript of all speech and audio content
        2. Word-level timestamps (estimated based on duration)
        3. Confidence level for the transcription
        
        Output format (JSON only):
        {{
            "transcript": "full transcript text here",
            "word_timestamps": [
                {{"word": "word1", "start_time": 0.0, "end_time": 0.5, "confidence": 0.8}},
                {{"word": "word2", "start_time": 0.5, "end_time": 1.0, "confidence": 0.8}}
            ],
            "confidence": 0.8
        }}
        """
        
        # Call Gemini API
        response = gemini_client.models.generate_content(
            model="gemini-2.5-flash",
            contents=prompt
        )
        
        raw_text = getattr(response, 'text', '') or ''
        json_block = _extract_json_block(raw_text)
        
        try:
            result = json.loads(json_block)
            # Clean up temp file
            os.unlink(temp_audio_path)
            return result
        except Exception:
            # Fallback to mock transcript
            words = ["This", "is", "a", "transcript", "of", "the", "video", "content"]
            word_timestamps = []
            time_per_word = duration / len(words)
            
            for i, word in enumerate(words):
                start_time = i * time_per_word
                end_time = min(start_time + time_per_word, duration)
                word_timestamps.append({
                    'word': word,
                    'start_time': start_time,
                    'end_time': end_time,
                    'confidence': 0.7
                })
            
            # Clean up temp file
            os.unlink(temp_audio_path)
            
            return {
                'transcript': ' '.join(words),
                'word_timestamps': word_timestamps,
                'confidence': 0.7
            }
            
    except Exception as e:
        print(f"Gemini transcription failed: {str(e)}")
        return None


def tag_video_with_gemini(video_path: str, video_id: str) -> list:
    """
    Generate comprehensive visual tags for video using Gemini AI by analyzing video frames.
    Returns list of tag dictionaries with 15+ meaningful tags.
    """
    try:
        if not gemini_client:
            return generate_comprehensive_visual_tags_fallback(video_path, video_id)
            
        # Extract a few key frames using FFmpeg
        import subprocess
        import tempfile
        
        # Create temp directory for frames
        with tempfile.TemporaryDirectory() as temp_dir:
            # Extract 5 frames evenly spaced
            frame_pattern = os.path.join(temp_dir, 'frame_%03d.jpg')
            cmd = ['ffmpeg', '-i', video_path, '-vf', 'fps=1/5', '-y', frame_pattern]
            subprocess.run(cmd, capture_output=True, check=True)
            
            # Get list of extracted frames
            frames = [f for f in os.listdir(temp_dir) if f.endswith('.jpg')]
            if not frames:
                return generate_comprehensive_visual_tags_fallback(video_path, video_id)
            
            # Analyze first frame with Gemini
            frame_path = os.path.join(temp_dir, frames[0])
            
            prompt = f"""
            You are an expert video content analyst. Analyze this video frame and identify ALL visual elements, objects, people, settings, activities, and contextual details that are SPECIFIC to this video's content.
            
            Video ID: {video_id}
            
            IMPORTANT: Focus on the ACTUAL CONTENT of this video, not generic descriptions. What specific things do you see that make this video unique?
            
            Provide comprehensive visual tags in JSON format with 25-30 extremely specific and detailed tags:
            {{
                "tags": [
                    {{"tag": "specific_object_seen", "confidence": 0.9, "category": "object"}},
                    {{"tag": "specific_person_details", "confidence": 0.8, "category": "person"}},
                    {{"tag": "specific_location_setting", "confidence": 0.9, "category": "setting"}},
                    {{"tag": "specific_action_activity", "confidence": 0.8, "category": "activity"}},
                    {{"tag": "specific_color_lighting", "confidence": 0.8, "category": "visual"}},
                    {{"tag": "specific_mood_atmosphere", "confidence": 0.8, "category": "atmosphere"}},
                    {{"tag": "specific_emotion_expression", "confidence": 0.7, "category": "emotion"}},
                    {{"tag": "specific_style_aesthetic", "confidence": 0.8, "category": "style"}},
                    {{"tag": "specific_technical_aspect", "confidence": 0.7, "category": "technical"}},
                    {{"tag": "specific_context_detail", "confidence": 0.8, "category": "context"}}
                ]
            }}
            
            CONTENT-FOCUSED ANALYSIS REQUIREMENTS:
            
            1. OBJECTS & ITEMS: What SPECIFIC objects do you see?
               - Be specific: "red coffee mug", "black leather chair", "white iPhone", "blue backpack"
               - Don't use generic terms like "furniture" or "electronics"
            
            2. PEOPLE & CHARACTERS: What SPECIFIC people details do you see?
               - Be specific: "young woman in blue dress", "man with glasses", "child with toy"
               - Include clothing, expressions, actions, demographics
            
            3. SETTINGS & LOCATIONS: What SPECIFIC setting is this?
               - Be specific: "modern kitchen with white cabinets", "busy coffee shop", "quiet home office"
               - Include architectural details, lighting, atmosphere
            
            4. ACTIVITIES & ACTIONS: What SPECIFIC activities are happening?
               - Be specific: "person typing on laptop", "cooking pasta", "reading book", "talking on phone"
               - Describe exact actions and movements
            
            5. VISUAL ELEMENTS: What SPECIFIC visual details do you see?
               - Be specific: "warm yellow lighting", "bright natural sunlight", "dark moody atmosphere"
               - Include colors, lighting, composition, camera angle
            
            6. ATMOSPHERE & MOOD: What SPECIFIC mood does this scene have?
               - Be specific: "cozy and relaxed", "busy and energetic", "professional and focused"
               - Describe the emotional tone and energy
            
            7. TECHNICAL DETAILS: What SPECIFIC technical aspects do you notice?
               - Be specific: "close-up shot", "steady camera", "professional lighting", "high quality"
               - Include camera work, quality, style
            
            8. CONTEXTUAL CLUES: What SPECIFIC context can you identify?
               - Be specific: "morning light", "business meeting", "casual hangout", "formal event"
               - Include time, occasion, purpose
            
            CRITICAL REQUIREMENTS:
            - Use SPECIFIC, DETAILED descriptions based on what you actually see
            - Avoid generic terms like "video", "content", "media", "footage"
            - Focus on what makes this specific scene unique and identifiable
            - Generate 25-30 tags that accurately describe the actual content
            - Each tag should provide valuable, specific information about the video
            """
            
            # Call Gemini API with image (use typed Image/Blob per google-genai SDK)
            with open(frame_path, 'rb') as img_file:
                img_bytes = img_file.read()

                image_part = None
                try:
                    # Preferred in newer google-genai versions
                    image_part = genai.types.Image(data=img_bytes, mime_type="image/jpeg")
                except Exception:
                    try:
                        # Fallback for older SDKs
                        image_part = genai.types.Blob(mime_type="image/jpeg", data=img_bytes)
                    except Exception:
                        image_part = None

                if image_part is not None:
                    response = gemini_client.models.generate_content(
                        model="gemini-2.5-flash",
                        contents=[image_part, prompt]
                    )
                else:
                    # Last-resort fallback: text-only (no image part)
                    response = gemini_client.models.generate_content(
                        model="gemini-2.5-flash",
                        contents=prompt
                    )
            
            raw_text = getattr(response, 'text', '') or ''
            json_block = _extract_json_block(raw_text)
            
            try:
                result = json.loads(json_block)
                tags = result.get('tags', [])
                if len(tags) >= 10:
                    return tags
                else:
                    # If Gemini didn't generate enough tags, use fallback
                    return generate_comprehensive_visual_tags_fallback(video_path, video_id)
            except Exception:
                # Fallback to comprehensive tags
                return generate_comprehensive_visual_tags_fallback(video_path, video_id)
                
    except Exception as e:
        print(f"Gemini visual tagging failed: {str(e)}")
        return generate_comprehensive_visual_tags_fallback(video_path, video_id)

def generate_comprehensive_visual_tags_fallback(video_path: str, video_id: str) -> list:
    """
    Generate comprehensive visual tags when Gemini fails.
    Creates 20+ meaningful tags based on ACTUAL video content analysis.
    """
    try:
        # Get video metadata and transcript - THIS IS THE KEY TO CONTENT-BASED TAGGING
        video_metadata_file = os.path.join(UPLOAD_FOLDER, f"{video_id}_metadata.json")
        transcript = ""
        word_timestamps = []
        
        if os.path.exists(video_metadata_file):
            with open(video_metadata_file, 'r') as f:
                metadata = json.load(f)
                transcript = metadata.get('transcript', '')
                word_timestamps = metadata.get('word_timestamps', [])
        
        # CONTENT-BASED ANALYSIS - Focus on actual content, not just duration
        tags = []
        transcript_lower = transcript.lower() if transcript else ""
        
        # EXTRACT KEY CONTENT WORDS from transcript for better analysis
        content_words = []
        if transcript:
            # Extract meaningful words (nouns, verbs, adjectives) from transcript
            import re
            words = re.findall(r'\b[a-zA-Z]{3,}\b', transcript_lower)
            content_words = [word for word in words if len(word) > 3]
        
        # PRIORITY 1: CONTENT-BASED TAGGING (Most Important)
        if transcript and content_words:
            # PEOPLE & CHARACTERS from actual content
            people_keywords = ['person', 'people', 'man', 'woman', 'child', 'kid', 'family', 'mom', 'dad', 'friend', 'group', 'team', 'audience', 'student', 'teacher', 'professional', 'expert']
            people_found = [word for word in content_words if word in people_keywords]
            if people_found:
                tags.extend([
                    {"tag": "people present", "confidence": 0.9, "category": "people"},
                    {"tag": "human interaction", "confidence": 0.8, "category": "activity"},
                    {"tag": "social content", "confidence": 0.8, "category": "content"}
                ])
            
            # ACTIVITIES & ACTIONS from actual content
            activity_keywords = ['cook', 'cooking', 'food', 'eat', 'work', 'study', 'learn', 'teach', 'play', 'game', 'exercise', 'run', 'walk', 'talk', 'speak', 'sing', 'dance', 'travel', 'visit', 'shop', 'buy', 'sell', 'meet', 'discuss', 'present', 'show', 'demonstrate']
            activities_found = [word for word in content_words if word in activity_keywords]
            if activities_found:
                for activity in activities_found[:3]:  # Top 3 activities
                    tags.append({"tag": f"{activity} activity", "confidence": 0.9, "category": "activity"})
            
            # SETTINGS & LOCATIONS from actual content
            location_keywords = ['home', 'house', 'room', 'kitchen', 'office', 'school', 'store', 'shop', 'restaurant', 'park', 'street', 'city', 'outdoor', 'indoor', 'garden', 'beach', 'mountain', 'forest']
            locations_found = [word for word in content_words if word in location_keywords]
            if locations_found:
                for location in locations_found[:2]:  # Top 2 locations
                    tags.append({"tag": f"{location} setting", "confidence": 0.9, "category": "setting"})
            
            # OBJECTS & ITEMS from actual content
            object_keywords = ['phone', 'computer', 'laptop', 'table', 'chair', 'car', 'book', 'food', 'drink', 'clothes', 'shoes', 'bag', 'tool', 'equipment', 'camera', 'tv', 'music', 'art']
            objects_found = [word for word in content_words if word in object_keywords]
            if objects_found:
                for obj in objects_found[:3]:  # Top 3 objects
                    tags.append({"tag": f"{obj} present", "confidence": 0.8, "category": "object"})
            
            # EMOTIONS & MOOD from actual content
            emotion_keywords = ['happy', 'joy', 'fun', 'excited', 'amazing', 'wonderful', 'great', 'love', 'enjoy', 'calm', 'peaceful', 'relaxed', 'serious', 'focused', 'energetic', 'creative', 'professional']
            emotions_found = [word for word in content_words if word in emotion_keywords]
            if emotions_found:
                for emotion in emotions_found[:2]:  # Top 2 emotions
                    tags.append({"tag": f"{emotion} mood", "confidence": 0.8, "category": "emotion"})
            
            # CONTENT TYPE from actual content
            content_type_keywords = ['tutorial', 'guide', 'review', 'demo', 'vlog', 'story', 'interview', 'conversation', 'performance', 'show', 'presentation', 'lesson', 'class']
            content_types_found = [word for word in content_words if word in content_type_keywords]
            if content_types_found:
                for content_type in content_types_found[:2]:  # Top 2 content types
                    tags.append({"tag": f"{content_type} format", "confidence": 0.9, "category": "content_type"})
        
        # PRIORITY 2: INTELLIGENT CONTENT ANALYSIS (Secondary)
        if not tags and transcript:  # If no specific content found, do broader analysis
            # Analyze transcript for broader themes
            if any(word in transcript_lower for word in ['family', 'mom', 'dad', 'child', 'kid']):
                tags.extend([
                    {"tag": "family content", "confidence": 0.9, "category": "content"},
                    {"tag": "personal life", "confidence": 0.8, "category": "content"},
                    {"tag": "domestic setting", "confidence": 0.8, "category": "setting"}
                ])
            elif any(word in transcript_lower for word in ['work', 'business', 'office', 'meeting', 'project']):
                tags.extend([
                    {"tag": "business content", "confidence": 0.9, "category": "content"},
                    {"tag": "professional setting", "confidence": 0.8, "category": "setting"},
                    {"tag": "work environment", "confidence": 0.8, "category": "setting"}
                ])
            elif any(word in transcript_lower for word in ['cook', 'food', 'kitchen', 'recipe', 'meal']):
                tags.extend([
                    {"tag": "cooking content", "confidence": 0.9, "category": "content"},
                    {"tag": "food preparation", "confidence": 0.8, "category": "activity"},
                    {"tag": "kitchen setting", "confidence": 0.8, "category": "setting"}
                ])
            elif any(word in transcript_lower for word in ['sport', 'game', 'exercise', 'fitness', 'workout']):
                tags.extend([
                    {"tag": "sports content", "confidence": 0.9, "category": "content"},
                    {"tag": "fitness activity", "confidence": 0.8, "category": "activity"},
                    {"tag": "physical activity", "confidence": 0.8, "category": "activity"}
                ])
            elif any(word in transcript_lower for word in ['travel', 'trip', 'visit', 'destination', 'vacation']):
                tags.extend([
                    {"tag": "travel content", "confidence": 0.9, "category": "content"},
                    {"tag": "adventure", "confidence": 0.8, "category": "activity"},
                    {"tag": "exploration", "confidence": 0.8, "category": "activity"}
                ])
            elif any(word in transcript_lower for word in ['tech', 'computer', 'phone', 'app', 'software', 'digital']):
                tags.extend([
                    {"tag": "technology content", "confidence": 0.9, "category": "content"},
                    {"tag": "digital focus", "confidence": 0.8, "category": "content"},
                    {"tag": "tech environment", "confidence": 0.8, "category": "setting"}
                ])
        
        # PRIORITY 3: VIDEO CHARACTERISTICS (Only if no content-based tags)
        if len(tags) < 10:  # Only add duration-based tags if we don't have enough content-based tags
            # Get video duration using ffprobe
            import subprocess
            duration_cmd = ['ffprobe', '-v', 'quiet', '-show_entries', 'format=duration', '-of', 'csv=p=0', video_path]
            duration_result = subprocess.run(duration_cmd, capture_output=True, text=True)
            duration = float(duration_result.stdout.strip()) if duration_result.stdout.strip() else 30
            
            if duration < 30:
                tags.extend([
                    {"tag": "short video", "confidence": 0.7, "category": "duration"},
                    {"tag": "quick clip", "confidence": 0.6, "category": "duration"}
                ])
            elif duration < 120:
                tags.extend([
                    {"tag": "medium video", "confidence": 0.7, "category": "duration"},
                    {"tag": "standard clip", "confidence": 0.6, "category": "duration"}
                ])
            else:
                tags.extend([
                    {"tag": "long video", "confidence": 0.7, "category": "duration"},
                    {"tag": "extended content", "confidence": 0.6, "category": "duration"}
                ])
        
        # PRIORITY 4: QUALITY & STYLE (Universal fallback)
        tags.extend([
            {"tag": "high quality", "confidence": 0.8, "category": "quality"},
            {"tag": "professional", "confidence": 0.7, "category": "quality"},
            {"tag": "modern", "confidence": 0.7, "category": "style"},
            {"tag": "contemporary", "confidence": 0.7, "category": "style"},
            {"tag": "engaging", "confidence": 0.7, "category": "content"},
            {"tag": "interesting", "confidence": 0.7, "category": "content"},
            {"tag": "memorable", "confidence": 0.6, "category": "content"},
            {"tag": "visual content", "confidence": 0.8, "category": "media"},
            {"tag": "digital media", "confidence": 0.8, "category": "media"},
            {"tag": "multimedia", "confidence": 0.7, "category": "media"}
        ])
        
        # Ensure we have at least 15 tags
        if len(tags) < 15:
            additional_tags = [
                {"tag": "content", "confidence": 0.6, "category": "media"},
                {"tag": "footage", "confidence": 0.6, "category": "media"},
                {"tag": "visual", "confidence": 0.6, "category": "media"},
                {"tag": "media", "confidence": 0.6, "category": "media"},
                {"tag": "digital", "confidence": 0.6, "category": "media"},
                {"tag": "recording", "confidence": 0.6, "category": "media"},
                {"tag": "video content", "confidence": 0.6, "category": "media"}
            ]
            for tag in additional_tags:
                if len(tags) < 15:
                    tags.append(tag)
        
        return tags[:25]  # Return up to 25 tags
    except Exception as e:
        print(f"Comprehensive visual tagging fallback failed: {str(e)}")
        # Ultimate fallback
        return [
            {"tag": "video", "confidence": 0.8, "category": "content"},
            {"tag": "content", "confidence": 0.7, "category": "media"},
            {"tag": "digital", "confidence": 0.6, "category": "media"},
            {"tag": "media", "confidence": 0.6, "category": "media"},
            {"tag": "visual", "confidence": 0.6, "category": "media"}
        ]


def _generate_inspirational_story_fallback(prompt: str, mode: str) -> str:
    """Return an inspirational story without external AI, styled by `mode`.

    The output varies meaningfully across modes and lightly with the prompt.
    We keep it concise but rich (‚âà220‚Äì320 words) so the UI feels responsive
    even when the AI model is unavailable.
    """
    import random
    from hashlib import sha256

    prompt_clean = (prompt or 'your journey').strip()

    # Seed pseudo-randomness using prompt+mode so similar prompts feel
    # consistent yet distinct across modes.
    seed = int(sha256(f"{prompt_clean}|{mode}".encode("utf-8")).hexdigest(), 16) % (2**32)
    rnd = random.Random(seed)

    vocab = {
        'Hopeful': {
            'openers': [
                "It began quietly, like a window opening to softer light.",
                "Morning arrived with a gentle kind of clarity.",
                "A small brightness found its way into the room."
            ],
            'metaphors': [
                "paths unfolding like pages not yet written",
                "seedlings pushing up through patient soil",
                "new sky after passing rain"
            ],
            'verbs': ["invite", "nurture", "grow", "open"],
            'closers': [
                "Tomorrow will meet you halfway.",
                "Even small light can guide a long way.",
                "What you tend today becomes a horizon."
            ],
        },
        'Motivational': {
            'openers': [
                "It started when you decided to move‚Äîno applause, just action.",
                "The day sharpened the moment you took control.",
                "Momentum noticed the instant you showed up."
            ],
            'metaphors': [
                "gears catching, power building",
                "distance closing with every step",
                "sparks stacking into bright flame"
            ],
            'verbs': ["commit", "build", "push", "advance"],
            'closers': [
                "Discipline writes the ending you want.",
                "Progress rewards the ones who keep showing up.",
                "You're closer because you chose to move."
            ],
        },
        'Funny': {
            'openers': [
                "It kicked off with a plan that looked suspiciously like improvisation.",
                "You announced a brilliant idea; the universe replied, 'Interesting, proceed.'",
                "Step one was confidence. Step two was Googling how to do step one."
            ],
            'metaphors': [
                "a circus of tiny miracles",
                "plot twists tripping over each other",
                "chaos politely waiting its turn"
            ],
            'verbs': ["wing", "juggle", "tumble", "sparkle"],
            'closers': [
                "If life is a sitcom, today's episode ends with a fist‚Äëpump.",
                "You laughed, learned, and somehow it worked.",
                "Comedy aside, look at you‚Äîactually nailing it."
            ],
        },
        'Emotional': {
            'openers': [
                "It began with a feeling you could almost name.",
                "A quiet ache turned into a gentle vow.",
                "You carried it carefully, like a photograph you didn't want to bend."
            ],
            'metaphors': [
                "memory threading itself through the present",
                "tides returning to the shore they trust",
                "warmth finding a hand in the dark"
            ],
            'verbs': ["listen", "hold", "honor", "heal"],
            'closers': [
                "What matters is held here, and it remains.",
                "The heart remembers, and it keeps going.",
                "You are allowed to feel this and still move forward."
            ],
        },
        'Reflective': {
            'openers': [
                "You paused long enough for the day to make sense.",
                "Questions settled like dust, revealing shape.",
                "A little distance turned noise into meaning."
            ],
            'metaphors': [
                "a map redrawn with truer lines",
                "water clearing until the stones show",
                "threads weaving into a patient pattern"
            ],
            'verbs': ["observe", "clarify", "align", "tend"],
            'closers': [
                "Clarity is quiet progress.",
                "Understanding is also a kind of arrival.",
                "You leave the day more aligned than you found it."
            ],
        },
    }

    style = vocab.get(mode, vocab['Hopeful'])

    opener = rnd.choice(style['openers'])
    metaphor = rnd.choice(style['metaphors'])
    verb_pair = ", ".join(rnd.sample(style['verbs'], k=min(2, len(style['verbs']))))
    closer = rnd.choice(style['closers'])

    p1 = (
        f"{opener} You looked at {prompt_clean} and the day tilted in your favor,"
        f" {metaphor}. What seemed distant stepped a little closer,"
        f" and you decided to {verb_pair}."
    )

    p2 = (
        f"You tried, adjusted, and tried again. The imperfect first draft of action"
        f" became a clearer sketch: smaller goals, steadier breaths, a rhythm you could keep."
        f" Setbacks didn't erase progress; they revealed where strength belonged."
        f" With each pass, {prompt_clean} felt more possible, more yours."
    )

    p3 = (
        f"By evening there was proof‚Äîsubtle but real. You learned what to keep and what to let go."
        f" You closed the day with gratitude and a simple promise to continue. {closer}"
    )

    story = "\n\n".join([p1, p2, p3])

    # If a bit short, extend with a concise third paragraph tailored to the mode
    # to keep the reading experience satisfying.
    word_count = len(story.split())
    if word_count < 220:
        extra = {
            'Hopeful': (
                "Tomorrow's plan is small and kind to your future self: one focused task,"
                " one mindful pause, and permission to celebrate what goes right."
            ),
            'Motivational': (
                "Set the next rep now: clear target, short deadline, honest effort."
                " Consistency will do the heavy lifting if you show up."
            ),
            'Funny': (
                "Note to self: label your boxes, hydrate, and keep the soundtrack upbeat."
                " Hero arcs love good snacks."
            ),
            'Emotional': (
                "You honor where you've been and trust where you're going."
                " That tenderness is strength making room for growth."
            ),
            'Reflective': (
                "You mark a lesson or two in the margins and travel lighter."
                " Insight quietly changes the route."
            ),
        }.get(mode, "Tomorrow will meet you halfway.")
        story = story + "\n\n" + extra

    return story

def requires_auth(f):
    """Decorator to verify Google ID token"""
    @functools.wraps(f)
    def decorated_function(*args, **kwargs):
        auth_header = request.headers.get('Authorization')
        
        if not auth_header or not auth_header.startswith('Bearer '):
            return jsonify({'error': 'No valid authorization header'}), 401
        
        id_token_str = auth_header.split('Bearer ')[1]
        
        try:
            # Verify the ID token
            idinfo = id_token.verify_oauth2_token(
                id_token_str, 
                requests.Request(), 
                GOOGLE_CLIENT_ID
            )
            
            # Add user info to request
            request.user = idinfo
            
            return f(*args, **kwargs)
            
        except ValueError as e:
            return jsonify({'error': 'Invalid token'}), 401
        except Exception as e:
            return jsonify({'error': f'Token verification failed: {str(e)}'}), 401
    
    return decorated_function

@app.route('/health', methods=['GET'])
def health_check():
    """Health check endpoint"""
    return jsonify({
        'status': 'healthy',
        'timestamp': datetime.now().isoformat(),
        'service': 'Footage Flow Backend'
    })

@app.route('/auth/google-signin', methods=['POST'])
def google_signin():
    """Handle Google Sign-In"""
    try:
        data = request.get_json()
        id_token_str = data.get('id_token')
        
        if not id_token_str:
            return jsonify({'error': 'No ID token provided'}), 400
        
        # Verify the ID token
        idinfo = id_token.verify_oauth2_token(
            id_token_str, 
            requests.Request(), 
            GOOGLE_CLIENT_ID
        )
        
        # Extract user information
        user_id = idinfo['sub']
        email = idinfo.get('email', '')
        name = idinfo.get('name', '')
        picture = idinfo.get('picture', '')
        
        # Create or update user in Firestore
        # if db: # This line was removed as per the new_code, as Firestore is no longer used.
        #     user_ref = db.collection('users').document(user_id)
        #     user_data = {
        #         'email': email,
        #         'name': name,
        #         'picture': picture,
        #         'last_login': datetime.now(),
        #         'created_at': firestore.SERVER_TIMESTAMP
        #     }
            
        #     # Check if user exists
        #     user_doc = user_ref.get()
        #     if not user_doc.exists:
        #         user_data['created_at'] = firestore.SERVER_TIMESTAMP
            
        #     user_ref.set(user_data, merge=True)
        #     print(f"User {email} saved to Firestore")
        # else:
        #     print(f"User {email} would be saved to Firestore (using local storage)")
        
        # Return user information
        user_info = {
            'userId': user_id,
            'email': email,
            'name': name,
            'picture': picture
        }
        
        return jsonify({
            'success': True,
            'user': user_info
        })
        
    except ValueError as e:
        return jsonify({'error': 'Invalid token'}), 401
    except Exception as e:
        return jsonify({'error': f'Authentication failed: {str(e)}'}), 500

@app.route('/auth/check-email', methods=['GET', 'POST'])
def check_email_exists():
    """Check if an account already exists for the given email."""
    try:
        if request.method == 'GET':
            email = (request.args.get('email') or '').strip().lower()
        else:
            data = request.get_json(silent=True) or {}
            email = (data.get('email') or '').strip().lower()
        if not email:
            return jsonify({'error': 'Email is required'}), 400

        try:
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("SELECT 1 FROM users WHERE email = ? LIMIT 1", (email,))
            exists = cursor.fetchone() is not None
            conn.close()
        except sqlite3.OperationalError as op_err:
            # Create table on the fly and report as not existing yet
            print(f"check_email_exists: {str(op_err)} ‚Äî creating users table")
            ensure_users_table()
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("SELECT 1 FROM users WHERE email = ? LIMIT 1", (email,))
            exists = cursor.fetchone() is not None
            conn.close()

        return jsonify({'exists': exists})
    except Exception as e:
        return jsonify({'error': f'Failed to check email: {str(e)}'}), 500

@app.route('/auth/register', methods=['POST'])
def register_account():
    """Register a new account if it does not already exist."""
    try:
        data = request.get_json(silent=True) or {}
        email = (data.get('email') or '').strip().lower()
        name = (data.get('name') or '').strip()
        password = (data.get('password') or '').strip()

        if not email or not password:
            return jsonify({'error': 'Email and password are required'}), 400

        ensure_users_table()
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("SELECT 1 FROM users WHERE email = ? LIMIT 1", (email,))
        if cursor.fetchone():
            conn.close()
            return jsonify({'error': 'Account already exists for this email'}), 409

        # Simple salted hash
        salt = uuid.uuid4().hex
        password_hash = hashlib.sha256((password + salt).encode('utf-8')).hexdigest()

        try:
            cursor.execute(
                "INSERT INTO users (email, name, password_hash, password_salt, created_at) VALUES (?, ?, ?, ?, ?)",
                (email, name, password_hash, salt, datetime.now().isoformat())
            )
            conn.commit()
        except sqlite3.IntegrityError:
            conn.close()
            return jsonify({'error': 'Account already exists for this email'}), 409
        except sqlite3.OperationalError as op_err:
            # Table might not exist; create and retry once
            print(f"register_account: {str(op_err)} ‚Äî creating users table and retrying")
            ensure_users_table()
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute(
                "INSERT INTO users (email, name, password_hash, password_salt, created_at) VALUES (?, ?, ?, ?, ?)",
                (email, name, password_hash, salt, datetime.now().isoformat())
            )
            conn.commit()
            conn.close()
        conn.close()

        return jsonify({'success': True})
    except Exception as e:
        return jsonify({'error': f'Failed to register: {str(e)}'}), 500

@app.route('/api/protected', methods=['GET'])
@requires_auth
def protected_endpoint():
    """Example protected endpoint"""
    return jsonify({
        'message': 'This is a protected endpoint',
        'user': request.user.get('email'),
        'timestamp': datetime.now().isoformat()
    })

@app.route('/api/user/profile', methods=['GET'])
@requires_auth
def get_user_profile():
    """Get user profile from Firestore"""
    try:
        user_id = request.user['sub']
        
        # if db: # This line was removed as per the new_code, as Firestore is no longer used.
        #     user_ref = db.collection('users').document(user_id)
        #     user_doc = user_ref.get()
            
        #     if user_doc.exists:
        #         user_data = user_doc.to_dict()
        #         return jsonify({
        #             'success': True,
        #             'user': user_data
        #         })
        #     else:
        #         return jsonify({'error': 'User not found'}), 404
        # else:
        return jsonify({
            'success': True,
            'user': {
                'email': request.user.get('email'),
                'name': request.user.get('name'),
                'picture': request.user.get('picture')
            }
        })
            
    except Exception as e:
        return jsonify({'error': f'Failed to get user profile: {str(e)}'}), 500

@app.route('/upload', methods=['POST'])
def upload_video():
    """Handle video upload"""
    try:
        # Check if video file is present
        if 'video' not in request.files:
            return jsonify({'error': 'No video file provided'}), 400
        
        file = request.files['video']
        
        # Check if file was selected
        if file.filename == '':
            return jsonify({'error': 'No file selected'}), 400
        
        # Validate file type with universal compatibility
        if universal_processor:
            # Use universal processor for 100% compatibility
            if not is_video_supported(file.filename):
                return jsonify({'error': 'File format not supported'}), 400
        else:
            # Fallback to original validation
            if not allowed_file(file.filename):
                return jsonify({'error': 'Invalid file type. Supported formats: MP4, AVI, MOV, WMV, FLV, WebM'}), 400
        
        # Validate file size
        if request.content_length > MAX_CONTENT_LENGTH:
            return jsonify({'error': 'File size exceeds 500MB limit'}), 400
        
        # Generate unique filename
        file_extension = file.filename.rsplit('.', 1)[1].lower()
        video_id = str(uuid.uuid4())
        filename = f"{video_id}.{file_extension}"
        
        # Get user info from request or use defaults
        data = request.form.to_dict()
        user_id = data.get('userId', 'user-123')
        user_email = data.get('userEmail', 'user@footageflow.com')
        
        # Save file locally
        secure_name = secure_filename(filename)
        local_path = os.path.join(UPLOAD_FOLDER, secure_name)
        file.save(local_path)
        
        # For now, we'll use local storage instead of Cloud Storage
        # In a real implementation, you would upload to Cloud Storage here
        gcs_path = f"local_storage/{user_id}/{filename}"
        
        # Extract video duration using FFprobe
        duration = None
        try:
            duration = get_video_duration(local_path)
            print(f"Extracted video duration: {duration} seconds")
        except Exception as e:
            print(f"Warning: Could not extract video duration: {str(e)}")
        
        # Create video metadata
        video_metadata = {
            'videoId': video_id,
            'userId': user_id,
            'userEmail': user_email,
            'filename': file.filename,
            'gcsPath': gcs_path,
            'localPath': local_path,
            'fileSize': os.path.getsize(local_path),
            'fileType': file_extension,
            'createdAt': datetime.now().isoformat(),
            'status': 'uploaded',
            'duration': duration,
            'transcript': None,
            'word_timestamps': None
        }
        
        # Save metadata to database
        if save_video_metadata(video_metadata):
            print(f"Video metadata saved to database: {video_id}")
        else:
            print(f"Warning: Failed to save video metadata to database: {video_id}")
        
        # Also save to JSON file for backward compatibility
        metadata_file = os.path.join(UPLOAD_FOLDER, f"{video_id}_metadata.json")
        with open(metadata_file, 'w') as f:
            json.dump(video_metadata, f, indent=2)
        
        print(f"Video uploaded: {video_id} by user {user_email}")

        # Fire-and-forget background AI Tagging so tags appear automatically
        def _background_tagging_task(v_id: str, v_path: str, meta_path: str):
            try:
                print(f"[BG] Starting universal visual tagging for video {v_id}")
                
                # Use universal video processor for 100% compatibility
                if universal_processor:
                    try:
                        # Process video with universal processor
                        result = process_any_video(v_path, v_id)
                        if result['success']:
                            vt = result['tags']
                            print(f"[BG] Universal processor generated {len(vt)} tags")
                        else:
                            print(f"[BG] Universal processor failed: {result.get('error', 'Unknown error')}")
                            vt = []
                    except Exception as e:
                        print(f"[BG] Universal processor error: {e}")
                        vt = []
                else:
                    vt = []
                
                # Use Gemini AI for enhanced visual tagging if available
                if gemini_client:
                    try:
                        gemini_tags = tag_video_with_gemini(v_path, v_id)
                        if gemini_tags:
                            # Combine universal and Gemini tags
                            vt.extend(gemini_tags)
                            print(f"[BG] Added {len(gemini_tags)} Gemini tags")
                    except Exception as e:
                        print(f"[BG] Gemini tagging failed: {e}")
                
                # Fallback to basic tags if no tags generated
                if not vt:
                    vt = [{"tag": "video", "confidence": 0.8}, {"tag": "content", "confidence": 0.7}]
                
                # Update metadata JSON and DB with tags
                try:
                    # Update metadata json
                    if os.path.exists(meta_path):
                        with open(meta_path, 'r') as mf:
                            meta = json.load(mf)
                    else:
                        meta = {}
                    meta['visual_tags'] = vt
                    with open(meta_path, 'w') as mf:
                        json.dump(meta, mf, indent=2)
                except Exception as me:
                    print(f"[BG] Failed to update metadata json with tags: {me}")
                try:
                    update_video_metadata(v_id, {'visual_tags': vt})
                except Exception as de:
                    print(f"[BG] Failed to update DB with tags: {de}")
                print(f"[BG] Visual tagging completed for {v_id} with {len(vt)} tags")
            except Exception as e:
                print(f"[BG] Visual tagging error for {v_id}: {e}")

        try:
            threading.Thread(target=_background_tagging_task, args=(video_id, local_path, metadata_file), daemon=True).start()
        except Exception as te:
            print(f"Warning: could not start background tagging: {te}")
        
        return jsonify({
            'success': True,
            'videoId': video_id,
            'gcsPath': gcs_path,
            'filename': file.filename,
            'message': 'Video uploaded successfully'
        })
        
    except Exception as e:
        print(f"Upload error: {str(e)}")
        return jsonify({'error': f'Upload failed: {str(e)}'}), 500

@app.route('/transcribe', methods=['POST'])
def transcribe():
    """Handle video transcription using TranscriptionService"""
    try:
        data = request.get_json()
        video_id = data.get('videoId')
        output_format = (data.get('outputFormat') or 'flac').lower()
        
        if not video_id:
            return jsonify({"success": False, "error": "Video ID is required"}), 400
            
        # only allow safe formats
        if output_format not in ['flac', 'wav']:
            output_format = 'flac'

        # Find the video metadata file
        video_metadata_file = os.path.join(UPLOAD_FOLDER, f"{video_id}_metadata.json")
        if not os.path.exists(video_metadata_file):
            candidates = [
                os.path.join(UPLOAD_FOLDER, 'videos', f"{video_id}_metadata.json"),
                os.path.join(UPLOAD_FOLDER, 'videos', 'videos', f"{video_id}_metadata.json"),
            ]
            found = None
            for c in candidates:
                if os.path.exists(c):
                    found = c
                    break
            if not found:
                return jsonify({"success": False, "error": "Video not found"}), 404
            video_metadata_file = found
        
        with open(video_metadata_file, 'r') as f:
            video_metadata = json.load(f)
        
        video_path = video_metadata.get('localPath')
        if not video_path or not os.path.exists(video_path):
            return jsonify({"success": False, "error": "Video file not found"}), 400

        print(f"[DEBUG] Starting transcription for {video_path}")

        # Initialize TranscriptionService
        transcription_service = TranscriptionService(BUCKET_NAME, GCP_PROJECT_ID)
        
        # Use local audio-based transcription first
        transcript = transcription_service.transcribe_video(video_path, video_id, output_format)
        
        if not transcript:
            # Fallback to Gemini text-only if local audio transcription fails
            print("Local transcription failed, falling back to Gemini text-only transcription.")
            transcript = transcribe_video_with_gemini(video_path, video_id)

        if transcript:
            # Handle new transcription format with timestamps
            if isinstance(transcript, dict):
                # New format with timestamps
                transcript_text = transcript.get('transcript', '')
                word_timestamps = transcript.get('word_timestamps', [])
                confidence = transcript.get('confidence', 0.0)
                print(f"DEBUG: Got timestamped transcript with {len(word_timestamps)} word timestamps")
            else:
                # Old format (fallback)
                transcript_text = transcript
                word_timestamps = []
                confidence = 0.0
                print(f"DEBUG: Got plain text transcript (no timestamps)")
            
            # Save transcription to metadata
            video_metadata['transcription'] = {
                'transcript': transcript_text,
                'word_timestamps': word_timestamps,
                'confidence': confidence,
                'output_format': output_format,
                'transcribedAt': datetime.now().isoformat()
            }
            
            # Save to database with transcript and word timestamps
            update_video_metadata(video_id, {
                'transcript': transcript_text,
                'word_timestamps': json.dumps(word_timestamps) if word_timestamps else None
            })
            
            # Also save to JSON file for backward compatibility
            with open(video_metadata_file, 'w') as f:
                json.dump(video_metadata, f, indent=2)
            
            print(f"Transcription completed for video: {video_id}")
            print(f"DEBUG: Saved {len(word_timestamps)} word timestamps to metadata")
            
            return jsonify({
                'success': True,
                'transcription': transcript_text,
                'word_count': len(transcript_text.split()),
                'videoId': video_id
            })
        else:
            return jsonify({"success": False, "error": "Transcription failed"}), 500

    except Exception as e:
        print(f"[ERROR] /transcribe exception: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({"success": False, "error": str(e)}), 500

@app.route('/transcribe-direct', methods=['POST'])
def transcribe_direct():
    """Direct transcription using Whisper - handles file upload directly"""
    try:
        if "file" not in request.files:
            return jsonify({"error": "No file uploaded"}), 400

        file = request.files["file"]
        if file.filename == '':
            return jsonify({"error": "No file selected"}), 400

        # Save temp video
        with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as temp_video:
            file.save(temp_video.name)
            video_path = temp_video.name

        try:
            # Skip full audio extraction - go directly to CHUNKED TRANSCRIPTION
            audio_path = video_path.replace(".mp4", ".wav")  # Just for naming
            
            # DIRECT TRANSCRIPTION - Use the same approach that worked in debug
            if WHISPER_MODEL:
                print("üîÑ Starting DIRECT TRANSCRIPTION (proven to work)...")
                print(f"üé¨ Video file: {video_path}")
                print(f"üïê Timestamp: {time.time()}")  # Force reload
                
                # Use the exact same approach that worked in debug script
                try:
                    segments, info = WHISPER_MODEL.transcribe(
                        video_path,  # Transcribe video directly
                        beam_size=25,
                        best_of=25,
                        temperature=0.0,
                        condition_on_previous_text=False,
                        word_timestamps=True,
                        vad_filter=False,
                        language="en"
                    )
                    
                    # Process results exactly like debug script
                    transcript_parts = []
                    word_timestamps = []
                    
                    for seg in segments:
                        if seg.text and seg.text.strip():
                            transcript_parts.append(seg.text.strip())
                            print(f"Segment: '{seg.text.strip()}'")
                            
                            if hasattr(seg, 'words') and seg.words:
                                for word in seg.words:
                                    if word.word and word.word.strip():
                                        word_timestamps.append({
                                            'word': word.word.strip(),
                                            'start_time': float(word.start) if word.start is not None else 0.0,
                                            'end_time': float(word.end) if word.end is not None else 0.0,
                                            'confidence': 0.8
                                        })
                    
                    transcript_text = " ".join(transcript_parts)
                    print(f"üéâ DIRECT TRANSCRIPTION COMPLETE: {len(transcript_text.split())} words")
                    print(f"üìù Full transcript: {transcript_text}")
                    
                except Exception as e:
                    print(f"‚ùå Direct transcription failed: {e}")
                    transcript_text = ""
                    word_timestamps = []

                return jsonify({
                    "success": True,
                    "language": info.language if 'info' in locals() else "en",
                    "transcript": transcript_text,
                    "word_timestamps": word_timestamps,
                    "word_count": len(transcript_text.split()),
                    "segments_count": len(transcript_text.split()),
                    "method": "ultra_aggressive"
                })
            else:
                return jsonify({"error": "Whisper model not available"}), 500

        finally:
            # Clean up temp files
            try:
                os.remove(video_path)
                if os.path.exists(audio_path):
                    os.remove(audio_path)
            except Exception as e:
                print(f"Warning: Could not clean up temp files: {e}")

    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/transcribe-direct-video', methods=['POST'])
def transcribe_direct_video():
    """Direct transcription using Whisper - works with videoId like old route"""
    try:
        data = request.get_json()
        video_id = data.get('videoId')
        
        if not video_id:
            return jsonify({"error": "No videoId provided"}), 400

        # Find the video file
        video_path = None
        possible_paths = [
            os.path.join(UPLOAD_FOLDER, "videos", f"{video_id}.mp4"),
            os.path.join(UPLOAD_FOLDER, f"{video_id}.mp4"),
            os.path.join(UPLOAD_FOLDER, "videos", "videos", f"{video_id}.mp4")
        ]
        
        for path in possible_paths:
            if os.path.exists(path):
                video_path = path
                break
                
        if not video_path:
            return jsonify({"error": "Video file not found"}), 404

        print(f"üîÑ Starting DIRECT TRANSCRIPTION for videoId: {video_id}")
        print(f"üé¨ Video file: {video_path}")
        print(f"üïê Timestamp: {time.time()}")
        
        if WHISPER_MODEL:
            try:
                segments, info = WHISPER_MODEL.transcribe(
                    video_path,  # Transcribe video directly
                    beam_size=5,  # Reduced from 25 for speed
                    best_of=5,    # Reduced from 25 for speed
                    temperature=0.0,
                    condition_on_previous_text=False,
                    word_timestamps=True,
                    vad_filter=False,
                    language="en"
                )
                
                # Process results
                transcript_parts = []
                word_timestamps = []
                
                for seg in segments:
                    if seg.text and seg.text.strip():
                        transcript_parts.append(seg.text.strip())
                        print(f"Segment: '{seg.text.strip()}'")
                        
                        if hasattr(seg, 'words') and seg.words:
                            for word in seg.words:
                                if word.word and word.word.strip():
                                    word_timestamps.append({
                                        'word': word.word.strip(),
                                        'start_time': float(word.start) if word.start is not None else 0.0,
                                        'end_time': float(word.end) if word.end is not None else 0.0,
                                        'confidence': 0.8
                                    })
                
                transcript_text = " ".join(transcript_parts)
                print(f"üéâ DIRECT TRANSCRIPTION COMPLETE: {len(transcript_text.split())} words")
                print(f"üìù Full transcript: {transcript_text}")
                
                # SAVE TRANSCRIPTION TO METADATA FILE
                try:
                    # Find metadata file
                    metadata_file = None
                    possible_metadata_paths = [
                        os.path.join(UPLOAD_FOLDER, f"{video_id}_metadata.json"),
                        os.path.join(UPLOAD_FOLDER, "videos", f"{video_id}_metadata.json"),
                        os.path.join(UPLOAD_FOLDER, "videos", "videos", f"{video_id}_metadata.json")
                    ]
                    
                    for path in possible_metadata_paths:
                        if os.path.exists(path):
                            metadata_file = path
                            break
                    
                    if metadata_file:
                        # Read existing metadata
                        with open(metadata_file, 'r') as f:
                            metadata = json.load(f)
                        
                        # Update with transcription data
                        metadata['transcript'] = transcript_text
                        metadata['word_timestamps'] = word_timestamps
                        metadata['transcribedAt'] = datetime.now().isoformat()
                        
                        # Save updated metadata
                        with open(metadata_file, 'w') as f:
                            json.dump(metadata, f, indent=2)
                        
                        print(f"‚úÖ Transcription saved to metadata file: {metadata_file}")
                    else:
                        print(f"‚ö†Ô∏è Could not find metadata file to save transcription")
                        
                except Exception as save_error:
                    print(f"‚ö†Ô∏è Error saving transcription to metadata: {save_error}")
                
                return jsonify({
                    "success": True,
                    "transcription": transcript_text,
                    "language": info.language if 'info' in locals() else "en",
                    "word_timestamps": word_timestamps,
                    "word_count": len(transcript_text.split()),
                    "method": "direct_whisper"
                })
                
            except Exception as e:
                print(f"‚ùå Direct transcription failed: {e}")
                return jsonify({"success": False, "error": str(e)}), 500
        else:
            return jsonify({"success": False, "error": "Whisper model not available"}), 500

    except Exception as e:
        print(f"‚ùå Transcription error: {e}")
        return jsonify({"success": False, "error": str(e)}), 500

@app.route('/transcript/<video_id>', methods=['GET'])
def get_transcript(video_id):
    """Get transcription for a specific video"""
    try:
        # Find the video metadata file (support both old and legacy nested locations)
        video_metadata_file = os.path.join(UPLOAD_FOLDER, f"{video_id}_metadata.json")
        if not os.path.exists(video_metadata_file):
            candidates = [
                os.path.join(UPLOAD_FOLDER, 'videos', f"{video_id}_metadata.json"),
                os.path.join(UPLOAD_FOLDER, 'videos', 'videos', f"{video_id}_metadata.json"),
            ]
            found = None
            for c in candidates:
                if os.path.exists(c):
                    found = c
                    break
            if not found:
                # Last resort: glob search
                try:
                    import glob
                    matches = glob.glob(os.path.join(UPLOAD_FOLDER, '**', f"{video_id}_metadata.json"), recursive=True)
                    if matches:
                        found = matches[0]
                except Exception:
                    pass
            if found:
                video_metadata_file = found
            else:
                return jsonify({'error': 'Video not found'}), 404
        
        with open(video_metadata_file, 'r') as f:
            video_metadata = json.load(f)
        
        transcription = video_metadata.get('transcription')
        if not transcription:
            return jsonify({'error': 'No transcription found for this video'}), 404
        
        return jsonify({
            'success': True,
            'transcription': transcription,
            'videoId': video_id
        })
        
    except Exception as e:
        print(f"Get transcript error: {str(e)}")
        return jsonify({'error': f'Failed to get transcript: {str(e)}'}), 500

@app.route('/generate-tags', methods=['POST'])
@app.route('/generate_tags', methods=['POST'])
def generate_tags():
    """Handle tag generation: visual (Vision/fallback) + text (Gemini)."""
    try:
        data = request.get_json() or {}
        video_id = data.get('videoId')
        emotion_bias = (data.get('emotion') or '').strip()
        # Optional focused time window in seconds
        start_time = data.get('startTime')
        end_time = data.get('endTime')
        try:
            start_time = float(start_time) if start_time is not None else None
        except Exception:
            start_time = None
        try:
            end_time = float(end_time) if end_time is not None else None
        except Exception:
            end_time = None
        
        if not video_id:
            return jsonify({'error': 'Video ID is required'}), 400
        
        # Find the video file
        video_metadata_file = os.path.join(UPLOAD_FOLDER, f"{video_id}_metadata.json")
        if not os.path.exists(video_metadata_file):
            return jsonify({'error': 'Video not found'}), 404
        
        with open(video_metadata_file, 'r') as f:
            video_metadata = json.load(f)
        
        video_path = video_metadata.get('localPath')
        if not os.path.exists(video_path):
            return jsonify({'error': 'Video file not found'}), 404
        
        print(f"Starting visual tagging for video: {video_id}")

        # Visual tags using Gemini AI
        visual_tags = tag_video_with_gemini(video_path, video_id)
        
        # Fallback to basic tags if Gemini fails
        if not visual_tags:
            visual_tags = [{"tag": "video", "confidence": 0.8}, {"tag": "content", "confidence": 0.7}]

        # Text-based tags with Gemini using transcript/description
        text_tags = []
        try:
            tr_block = video_metadata.get('transcription') or {}
            transcript_text = tr_block.get('transcript', '') if isinstance(tr_block, dict) else ''
            if not transcript_text:
                transcript_text = video_metadata.get('description') or ''
            text_tags = generate_text_tags_with_gemini(transcript_text, emotion_bias)
        except Exception as te:
            print(f"Text tagging pipeline error: {str(te)}")

        if visual_tags or text_tags:
            # Merge visual dict tags and text string tags
            merged = {}
            for vt in (visual_tags or []):
                if isinstance(vt, dict) and 'tag' in vt:
                    # annotate source without mutating original deeply
                    item = dict(vt)
                    item.setdefault('source', 'visual')
                    merged[item['tag'].lower()] = item
            for ts in (text_tags or []):
                key = str(ts).strip().lower()
                if key and key not in merged:
                    merged[key] = {
                        'tag': ts,
                        'score': 0.9,
                        'timestamp': 0.0,
                        'occurrences': 1,
                        'source': 'text'
                    }
            combined_tags = list(merged.values())

            # Build allTags (with 'All' first)
            unique_tag_names = []
            for v in combined_tags:
                name = (v.get('tag') or '').strip()
                if name and name.lower() not in [t.lower() for t in unique_tag_names]:
                    unique_tag_names.append(name)
            all_tags = ['All'] + unique_tag_names

            # Save to metadata and DB
            video_metadata['visual_tags'] = visual_tags
            if text_tags:
                video_metadata['ai_text_tags'] = text_tags
            video_metadata['taggedAt'] = datetime.now().isoformat()

            update_video_metadata(video_id, {
                'visual_tags': visual_tags,
                'ai_text_tags': text_tags
            })

            with open(video_metadata_file, 'w') as f:
                json.dump(video_metadata, f, indent=2)

            print(f"Tagging completed for video: {video_id} (visual {len(visual_tags or [])}, text {len(text_tags or [])})")

            return jsonify({
                'success': True,
                'tags': combined_tags,
                'videoId': video_id,
                'allTags': all_tags
            })
        else:
            return jsonify({'error': 'Tagging failed'}), 500
        
    except Exception as e:
        import traceback
        print("Visual tagging error:")
        traceback.print_exc()
        return jsonify({'error': f'Visual tagging failed: {str(e)}'}), 500

@app.route('/tags', methods=['GET'])
def get_tags():
    """GET /tags?videoId=<id> endpoint that calls tagging.generate_tags(videoId) and returns the results"""
    try:
        video_id = request.args.get('videoId')
        
        if not video_id:
            return jsonify({'error': 'videoId parameter is required'}), 400
        
        # Get tags from local storage (Gemini-generated tags are saved locally)
        tags = []
        try:
            video_metadata_file = os.path.join(UPLOAD_FOLDER, f"{video_id}_metadata.json")
            if os.path.exists(video_metadata_file):
                with open(video_metadata_file, 'r') as f:
                    metadata = json.load(f)
                visual_tags = metadata.get('visual_tags', [])
                text_tags = metadata.get('ai_text_tags', [])
                tags = visual_tags + text_tags
        except Exception as e:
            print(f"Error reading local tags: {e}")
            tags = []
        
        return jsonify({
            "videoId": video_id,
            "tags": tags
        })
        
    except Exception as e:
        print(f"Get tags error: {str(e)}")
        return jsonify({'error': f'Failed to get tags: {str(e)}'}), 500


@app.route('/ai-tags', methods=['GET'])
def generate_ai_tags_on_demand():
    """Generate emotion-biased text tags on demand without re-running visual tagging.

    Usage: GET /ai-tags?videoId=<id>&emotion=happy
    Returns: { "videoId": ..., "tags": ["..." ] }
    """
    try:
        video_id = (request.args.get('videoId') or '').strip()
        emotion = (request.args.get('emotion') or '').strip()
        if not video_id:
            return jsonify({'error': 'videoId parameter is required'}), 400

        metadata_path = os.path.join(UPLOAD_FOLDER, f"{video_id}_metadata.json")
        if not os.path.exists(metadata_path):
            return jsonify({'error': 'Video not found'}), 404

        with open(metadata_path, 'r') as f:
            video_metadata = json.load(f)

        tags = generate_video_text_tags(video_metadata, emotion)
        return jsonify({
            'videoId': video_id,
            'emotion': emotion,
            'tags': tags
        })
    except Exception as e:
        print(f"/ai-tags failed: {str(e)}")
        return jsonify({'error': f'AI tag generation failed: {str(e)}'}), 500

@app.route('/analyze-emotions', methods=['POST'])
@app.route('/analyze_emotions', methods=['POST'])
def analyze_emotions():
    """Analyze transcript to detect emotions over time and persist in SQLite."""
    try:
        data = request.get_json() or {}
        video_id = data.get('videoId')
        # Accept direct payload but also auto-load from saved metadata like the transcribe flow
        transcript = data.get('transcript', '') or ''
        word_timestamps = data.get('word_timestamps', []) or []
        story_scenes = []

        if not video_id:
            return jsonify({'error': 'Video ID is required'}), 400

        # Lightweight heuristic analysis (works without external APIs)
        emotions = []
        def push(ts, label, intensity):
            emotions.append({'timestamp': float(ts or 0), 'label': label, 'intensity': float(intensity)})

        # If transcript/timestamps missing, try to read from metadata file (reference: transcribe saves {transcription: {transcript, word_timestamps}})
        if (not transcript or not isinstance(transcript, str)) or not (isinstance(word_timestamps, list) and word_timestamps):
            try:
                metadata_path = os.path.join(UPLOAD_FOLDER, f"{video_id}_metadata.json")
                if os.path.exists(metadata_path):
                    with open(metadata_path, 'r') as f:
                        meta = json.load(f)
                # Check for direct transcript field first
                transcript = transcript or meta.get('transcript', '')
                word_timestamps = word_timestamps or meta.get('word_timestamps', [])
                # Also check nested transcription object for backward compatibility
                tr = meta.get('transcription') or {}
                if isinstance(tr, dict):
                    transcript = transcript or tr.get('transcript', '')
                    word_timestamps = word_timestamps or tr.get('word_timestamps', [])
                    # Load story scenes if present so we can align emotions to scenes
                    story = meta.get('story') or {}
                    if isinstance(story, dict):
                        story_scenes = story.get('scenes') or []
            except Exception as _e:
                print(f"Emotion metadata load warning: {_e}")

        # If we have story scenes, score each scene using caption/narration keywords
        if isinstance(story_scenes, list) and len(story_scenes) > 0:
            def score_text(text: str):
                t = (text or '').lower()
                lex = {
                    'happy': ['happy','joy','delight','smile','celebrate','fun','love','excited','wonderful','amazing','great'],
                    'sad': ['sad','sorry','cry','tears','pain','lonely','upset','loss','bad'],
                    'angry': ['angry','mad','furious','rage','annoyed','frustrated'],
                    'calm': ['calm','relax','peace','serene','quiet','gentle','soothing'],
                    'excited': ['excited','thrill','wow','incredible','epic','energy','hype']
                }
                scores = {k:0 for k in ['happy','sad','angry','calm','excited']}
                for emo, words in lex.items():
                    scores[emo] = sum(1 for w in words if w in t)
                mx = max(1, max(scores.values()))
                for k in scores:
                    scores[k] = min(1.0, scores[k]/mx)
                return scores

            for sc in story_scenes:
                try:
                    s = float(sc.get('start', 0))
                    e = float(sc.get('end', s + 5))
                    caption = sc.get('caption', '')
                    narration = sc.get('narration', '')
                    scores = score_text(f"{caption}. {narration}")
                    for emo, val in scores.items():
                        emotions.append({'timestamp': s, 'label': emo, 'intensity': float(val)})
                        emotions.append({'timestamp': e, 'label': emo, 'intensity': float(val)})
                except Exception as se:
                    print(f"Scene emotion error: {se}")

        # Heuristic: If we have timestamps, derive emotion timeline from keywords
        if not emotions and isinstance(word_timestamps, list) and word_timestamps:
            step = max(1, len(word_timestamps) // 40)
            for i in range(0, len(word_timestamps), step):
                w = word_timestamps[i] or {}
                token = str(w.get('word', '')).lower()
                ts = w.get('start_time', 0.0)
                if any(k in token for k in ['happy', 'joy', 'fun', 'yay', 'great', 'awesome', 'love']):
                    push(ts, 'happy', 0.85)
                elif any(k in token for k in ['sad', 'sorry', 'cry', 'bad']):
                    push(ts, 'sad', 0.7)
                elif any(k in token for k in ['angry', 'mad', 'furious']):
                    push(ts, 'angry', 0.7)
                elif any(k in token for k in ['calm', 'relax', 'peace']):
                    push(ts, 'calm', 0.6)
                elif any(k in token for k in ['excited', 'amazing', 'incredible', 'wow']):
                    push(ts, 'excited', 0.8)
                else:
                    push(ts, 'neutral', 0.4)
        else:
            # Generate emotion data based on transcript content
            transcript_lower = transcript.lower() if transcript else ""
            
            # Analyze transcript for emotional keywords
            happy_words = ['happy', 'joy', 'fun', 'great', 'awesome', 'love', 'excited', 'wonderful', 'amazing']
            sad_words = ['sad', 'sorry', 'cry', 'bad', 'upset', 'lonely', 'pain']
            angry_words = ['angry', 'mad', 'furious', 'annoyed', 'frustrated']
            calm_words = ['calm', 'relax', 'peace', 'serene', 'quiet', 'gentle']
            excited_words = ['excited', 'thrill', 'wow', 'incredible', 'epic', 'energy']
            
            # Count emotional words
            happy_count = sum(1 for word in happy_words if word in transcript_lower)
            sad_count = sum(1 for word in sad_words if word in transcript_lower)
            angry_count = sum(1 for word in angry_words if word in transcript_lower)
            calm_count = sum(1 for word in calm_words if word in transcript_lower)
            excited_count = sum(1 for word in excited_words if word in transcript_lower)
            
            # Generate timeline based on content
            total_words = len(transcript.split()) if transcript else 100
            duration = min(60, max(10, total_words * 0.3))  # Estimate duration
            
            # Create emotion timeline
            for i in range(0, int(duration), 3):
                timestamp = i
                if happy_count > 0:
                    push(timestamp, 'happy', min(0.9, 0.3 + (happy_count * 0.1)))
                if excited_count > 0:
                    push(timestamp + 1, 'excited', min(0.9, 0.4 + (excited_count * 0.1)))
                if calm_count > 0:
                    push(timestamp + 2, 'calm', min(0.8, 0.3 + (calm_count * 0.1)))
                if sad_count > 0:
                    push(timestamp + 0.5, 'sad', min(0.8, 0.2 + (sad_count * 0.1)))
                if angry_count > 0:
                    push(timestamp + 1.5, 'angry', min(0.8, 0.2 + (angry_count * 0.1)))
                else:
                    push(timestamp + 2.5, 'neutral', 0.4)

        # Aggregate good vs bad sides for UI
        good_labels = {'happy', 'calm', 'excited'}
        bad_labels = {'sad', 'angry'}
        from collections import defaultdict
        agg = defaultdict(float)
        for e in emotions:
            agg[(e['label'])] += float(e.get('intensity', 0))
        good_side = [{'label': k, 'score': round(agg[k], 3)} for k in good_labels if agg.get(k, 0) > 0]
        bad_side = [{'label': k, 'score': round(agg[k], 3)} for k in bad_labels if agg.get(k, 0) > 0]
        good_side.sort(key=lambda x: x['score'], reverse=True)
        bad_side.sort(key=lambda x: x['score'], reverse=True)

        # Persist to SQLite (ensure table exists for older DBs)
        try:
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS emotions (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    video_id TEXT NOT NULL,
                    timestamp REAL NOT NULL,
                    label TEXT NOT NULL,
                    intensity REAL NOT NULL,
                    created_at TEXT NOT NULL
                )
            ''')
            now = datetime.now().isoformat()
            for e in emotions:
                cursor.execute(
                    'INSERT INTO emotions (video_id, timestamp, label, intensity, created_at) VALUES (?, ?, ?, ?, ?)',
                    (video_id, e['timestamp'], e['label'], e['intensity'], now)
                )
            conn.commit()
            conn.close()
        except Exception as db_err:
            print(f"Emotion DB persist error: {db_err}")

        return jsonify({
            'success': True,
            'videoId': video_id,
            'emotions': emotions,
            'goodSide': good_side,
            'badSide': bad_side
        })
    except Exception as e:
        # Never hard-fail the feature; return a neutral fallback so UI continues
        import traceback
        print("Emotion analysis error:")
        traceback.print_exc()
        fallback = [{'timestamp': 0.0, 'label': 'neutral', 'intensity': 0.4}]
        try:
            # try soft-persist fallback
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS emotions (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    video_id TEXT NOT NULL,
                    timestamp REAL NOT NULL,
                    label TEXT NOT NULL,
                    intensity REAL NOT NULL,
                    created_at TEXT NOT NULL
                )
            ''')
            now = datetime.now().isoformat()
            for p in fallback:
                cursor.execute(
                    'INSERT INTO emotions (video_id, timestamp, label, intensity, created_at) VALUES (?, ?, ?, ?, ?)',
                    (data.get('videoId','unknown'), p['timestamp'], p['label'], p['intensity'], now)
                )
            conn.commit()
            conn.close()
        except Exception as _:
            pass
        return jsonify({'success': True, 'videoId': data.get('videoId'), 'emotions': fallback, 'warning': str(e)}), 200



@app.route('/extract-clip', methods=['POST'])
def extract_video_clip():
    """Extract a video clip from a specific timestamp"""
    try:
        data = request.get_json()
        video_id = data.get('videoId')
        start_time = data.get('startTime', 0)  # seconds
        duration = data.get('duration', 0.5)   # seconds, default 0.5 seconds for tag clips
        
        if not video_id:
            return jsonify({'error': 'Video ID is required'}), 400
        
        # Find the video file
        video_metadata_file = os.path.join(UPLOAD_FOLDER, f"{video_id}_metadata.json")
        if not os.path.exists(video_metadata_file):
            return jsonify({'error': 'Video not found'}), 404
        
        with open(video_metadata_file, 'r') as f:
            video_metadata = json.load(f)
        
        video_path = video_metadata.get('localPath')
        if not os.path.exists(video_path):
            return jsonify({'error': 'Video file not found'}), 404
        
        # Create clips directory
        clips_dir = os.path.join(UPLOAD_FOLDER, 'clips')
        os.makedirs(clips_dir, exist_ok=True)
        
        # Generate clip filename
        clip_filename = f"clip_{video_id}_{start_time}_{duration}.mp4"
        clip_path = os.path.join(clips_dir, clip_filename)
        
        # Extract clip using FFmpeg
        cmd = [
            'ffmpeg', '-i', video_path,
            '-ss', str(start_time),  # Start time
            '-t', str(duration),     # Duration (0.5 seconds)
            '-c', 'copy',            # Copy codecs (fast)
            '-avoid_negative_ts', 'make_zero',
            '-y',                    # Overwrite output
            clip_path
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
        
        if os.path.exists(clip_path):
            # Create a URL for the clip
            clip_url = f"/clips/{clip_filename}"
            
            return jsonify({
                'success': True,
                'clipUrl': clip_url,
                'clipPath': clip_path,
                'startTime': start_time,
                'duration': duration,
                'videoId': video_id
            })
        else:
            return jsonify({'error': 'Failed to extract clip'}), 500
        
    except subprocess.CalledProcessError as e:
        print(f"FFmpeg error: {e.stderr}")
        return jsonify({'error': f'Failed to extract clip: {e.stderr}'}), 500
    except Exception as e:
        print(f"Extract clip error: {str(e)}")
        return jsonify({'error': f'Failed to extract clip: {str(e)}'}), 500

@app.route('/clips/<filename>')
def serve_clip(filename):
    """Serve video clips"""
    clips_dir = os.path.join(UPLOAD_FOLDER, 'clips')
    clip_path = os.path.join(clips_dir, filename)
    
    if os.path.exists(clip_path):
        return send_file(clip_path, mimetype='video/mp4')
    else:
        return jsonify({'error': 'Clip not found'}), 404

@app.route('/render-story', methods=['POST'])
def render_story():
    """Render story video from scenes with transitions"""
    try:
        data = request.get_json()
        video_id = data.get('videoId')
        scenes = data.get('scenes', [])
        transition_duration = data.get('transitionDuration', 0.5)  # seconds
        
        if not video_id:
            return jsonify({'error': 'Video ID is required'}), 400
        
        if not scenes:
            return jsonify({'error': 'Scenes are required'}), 400
        
        # Get video metadata
        video_metadata_file = os.path.join(UPLOAD_FOLDER, f"{video_id}_metadata.json")
        print(f"Current working directory: {os.getcwd()}")
        print(f"Looking for video metadata at: {video_metadata_file}")
        print(f"Absolute path: {os.path.abspath(video_metadata_file)}")
        
        if not os.path.exists(video_metadata_file):
            print(f"Video metadata file not found: {video_metadata_file}")
            return jsonify({'error': 'Video not found'}), 404
        
        try:
            with open(video_metadata_file, 'r') as f:
                video_metadata = json.load(f)
            
            video_path = video_metadata.get('localPath')
            print(f"Video path from metadata: {video_path}")
            
            # Normalize the path to handle Windows backslashes
            if video_path:
                video_path = os.path.normpath(video_path)
                # If it's a relative path, make it absolute
                if not os.path.isabs(video_path):
                    video_path = os.path.join(os.getcwd(), video_path)
                # Ensure the path uses the correct separator for the current OS
                video_path = os.path.normpath(video_path)
            
            print(f"Normalized video path: {video_path}")
            
            if not video_path or not os.path.exists(video_path):
                print(f"Video file not found at path: {video_path}")
                return jsonify({'error': 'Video file not found'}), 404
        except Exception as e:
            print(f"Error reading video metadata: {str(e)}")
            return jsonify({'error': 'Error reading video metadata'}), 500
        
        # Create renders directory
        renders_dir = os.path.join(UPLOAD_FOLDER, 'renders')
        os.makedirs(renders_dir, exist_ok=True)
        
        # Render single video
        render_id = str(uuid.uuid4())
        output_filename = f"story_{render_id}.mp4"
        output_path = os.path.join(renders_dir, output_filename)
        
        print(f"Starting video render for video: {video_id}")
        print(f"Scenes to render: {len(scenes)}")
        
        # Render the video
        success = render_video_with_scenes(
            video_path, 
            scenes, 
            output_path, 
            transition_duration
        )
        
        if success:
                # Create URL for the rendered video
                video_url = f"/renders/{output_filename}"
                
                # Save render metadata
                render_metadata = {
                    'renderId': render_id,
                    'videoId': video_id,
                    'outputPath': output_path,
                    'outputUrl': video_url,
                    'scenes': scenes,
                    'transitionDuration': transition_duration,
                    'renderedAt': datetime.now().isoformat(),
                    'fileSize': os.path.getsize(output_path) if os.path.exists(output_path) else 0,
                    'storyType': 'normal'
                }
                
                render_metadata_file = os.path.join(renders_dir, f"{render_id}_metadata.json")
                with open(render_metadata_file, 'w') as f:
                    json.dump(render_metadata, f, indent=2)
                
                print(f"Video render completed: {output_path}")
                
                return jsonify({
                    'success': True,
                    'renderId': render_id,
                    'videoUrl': video_url,
                    'message': 'Video rendered successfully'
                })
        else:
            return jsonify({'error': 'Video rendering failed'}), 500
        
    except Exception as e:
        import traceback
        print(f"Render story error: {str(e)}")
        print(f"Traceback: {traceback.format_exc()}")
        return jsonify({'error': f'Video rendering failed: {str(e)}'}), 500

@app.route('/renders/<filename>')
def serve_render(filename):
    """Serve rendered videos"""
    renders_dir = os.path.join(UPLOAD_FOLDER, 'renders')
    render_path = os.path.join(renders_dir, filename)
    
    if os.path.exists(render_path):
        return send_file(render_path, mimetype='video/mp4')
    else:
        return jsonify({'error': 'Rendered video not found'}), 404

@app.route('/generate-story', methods=['POST'])
def generate_story():
    """Generate story with scenes and timestamps using Gemini AI"""
    try:
        data = request.get_json()
        video_id = data.get('videoId')
        prompt = data.get('prompt')
        mode = data.get('mode', 'positive')  # Default to positive mode
        
        print(f"DEBUG: Story generation request - videoId: {video_id}, prompt: '{prompt}', mode: '{mode}'")
               
        if not video_id or not prompt:
            return jsonify({'error': 'Video ID and prompt are required'}), 400
        
        # Get video metadata
        video_metadata_file = os.path.join(UPLOAD_FOLDER, f"{video_id}_metadata.json")
        if not os.path.exists(video_metadata_file):
            return jsonify({'error': 'Video not found'}), 404
        
        with open(video_metadata_file, 'r') as f:
            video_metadata = json.load(f)
        
        # Get transcription and tags - check both nested and direct locations
        transcription_data = video_metadata.get('transcription', {})
        transcript = transcription_data.get('transcript', '') or video_metadata.get('transcript', '')
        word_timestamps = transcription_data.get('word_timestamps', []) or video_metadata.get('word_timestamps', [])
        visual_tags = video_metadata.get('visual_tags', [])
        
        # If no transcript found in JSON, try to get from database
        if not transcript:
            try:
                db_metadata = get_video_metadata(video_id)
                if db_metadata and db_metadata.get('transcript'):
                    transcript = db_metadata['transcript']
                    print(f"DEBUG: Found transcript in database: {len(transcript)} characters")
                if db_metadata and db_metadata.get('word_timestamps') and not word_timestamps:
                    word_timestamps = db_metadata['word_timestamps']
                    print(f"DEBUG: Found word_timestamps in database: {len(word_timestamps)} timestamps")
            except Exception as e:
                print(f"DEBUG: Could not get transcript from database: {str(e)}")
        
        print(f"DEBUG: Story generation - transcript length: {len(transcript) if transcript else 0}")
        print(f"DEBUG: Story generation - word_timestamps type: {type(word_timestamps)}")
        print(f"DEBUG: Story generation - word_timestamps length: {len(word_timestamps) if word_timestamps else 0}")
        print(f"DEBUG: Story generation - visual_tags: {visual_tags}")
        print(f"DEBUG: Story generation - transcription_data keys: {list(transcription_data.keys()) if transcription_data else 'None'}")
        print(f"DEBUG: Story generation - video_metadata keys: {list(video_metadata.keys())}")
        
        if not transcript:
            return jsonify({'error': 'No transcription available for this video. Please transcribe the video first (Step 2).'}), 400
        
        # Generate story using Gemini AI
        if gemini_client:
            story_data = generate_story_with_gemini(
                transcript, word_timestamps, visual_tags, prompt, video_id, mode
            )
        else:
            # Fallback to mock data
            story_data = generate_mock_story(transcript, word_timestamps, visual_tags, prompt, video_id, mode)
        
        # Save story to metadata
        video_metadata['story'] = story_data
        video_metadata['storyPrompt'] = prompt
        video_metadata['storyGeneratedAt'] = datetime.now().isoformat()
        
        with open(video_metadata_file, 'w') as f:
            json.dump(video_metadata, f, indent=2)
        
        print(f"Story generated for video: {video_id}")
        
        return jsonify({
            'success': True,
            'storyId': story_data['storyId'],
            'scenes': story_data['scenes'],
            'videoId': video_id,
            'message': 'Story generated successfully' if gemini_client else 'Story generated using fallback mode (AI quota exceeded)'
        })
        
    except Exception as e:
        import traceback
        print(f"Story generation error: {str(e)}")
        print(f"Traceback: {traceback.format_exc()}")
        return jsonify({'error': f'Story generation failed: {str(e)}'}), 500


@app.route('/generate_story', methods=['POST'])
def generate_inspirational_story():
    """Generate an inspirational story from a user prompt and mode.

    Request JSON: { "prompt": "...", "mode": "Hopeful|Motivational|Funny|Emotional|Reflective" }
    Response JSON: { "story": "..." }
    """
    try:
        data = request.get_json(silent=True) or {}
        prompt = (data.get('prompt') or '').strip()
        mode = (data.get('mode') or 'Hopeful').strip()

        if not prompt:
            return jsonify({'error': 'Prompt is required'}), 400

        # Normalize and validate mode
        allowed_modes = {"Hopeful", "Motivational", "Funny", "Emotional", "Reflective"}
        if mode not in allowed_modes:
            mode = 'Hopeful'

        system_instructions = (
            "You are an expert AI story generator specializing in creating deeply personal, specific, and non-generic inspirational stories. "
            "Take the user's specific prompt and transform it into a unique, detailed story of 250-350 words that directly addresses their situation. "
            "IMPORTANT: Make the story SPECIFIC to their prompt - include concrete details, realistic scenarios, and authentic emotions related to their exact situation. "
            "Avoid generic, template-like stories. Instead, create a narrative that feels personal and tailored to their specific challenge or goal. "
            "Adjust the tone and style based on the selected story mode while maintaining authenticity and specificity. "
            "Write vividly with concrete imagery, realistic dialogue, specific examples, and natural paragraphing. "
            "Include specific details about their situation, realistic obstacles, and authentic solutions. "
            "Avoid bullet points, lists, headings, emojis, or JSON. Return only the story text as plain paragraphs."
        )

        # Enhanced style guidance for more specific, non-generic content
        mode_guides = {
            'Hopeful': (
                "Tone: warm, uplifting, compassionate but SPECIFIC. Use concrete details and realistic scenarios. "
                "Include specific examples of progress, small wins, and realistic hope. "
                "Preferred approach: Show how their specific situation can improve through realistic steps. "
                "Avoid generic optimism - make hope feel earned and specific to their challenge."
            ),
            'Motivational': (
                "Tone: energetic, determined, action-oriented with SPECIFIC strategies. "
                "Include concrete action steps, specific goals, and realistic milestones. "
                "Preferred approach: Provide specific, actionable advice for their exact situation. "
                "Avoid generic motivation - give them specific tools and strategies for their challenge."
            ),
            'Funny': (
                "Tone: lighthearted, witty, playful but SPECIFIC to their situation. "
                "Include relatable humor about their specific challenge, realistic mishaps, and clever solutions. "
                "Preferred approach: Find the humor in their specific situation while being supportive. "
                "Avoid generic jokes - make humor specific to their exact experience."
            ),
            'Emotional': (
                "Tone: tender, sincere, vulnerable with SPECIFIC emotional details. "
                "Include authentic feelings, specific memories, and realistic emotional growth. "
                "Preferred approach: Address the real emotional challenges of their specific situation. "
                "Avoid generic emotions - make it feel deeply personal to their experience."
            ),
            'Reflective': (
                "Tone: calm, thoughtful, insightful with SPECIFIC observations. "
                "Include specific insights, realistic self-discovery, and practical wisdom. "
                "Preferred approach: Provide specific insights about their particular situation. "
                "Avoid generic reflection - offer specific understanding of their unique challenge."
            ),
        }
        style_guide = mode_guides.get(mode, mode_guides['Hopeful'])

        # Use Gemini when available; otherwise fall back to enhanced local generator
        story_text = None
        if gemini_client:
            try:
                full_prompt = (
                    f"{system_instructions}\n\n"
                    f"STYLE GUIDE FOR {mode}: {style_guide}\n\n"
                    f"USER'S SPECIFIC SITUATION: {prompt}\n"
                    f"STORY MODE: {mode}\n\n"
                    f"Create a story that is SPECIFICALLY about their situation: '{prompt}'. "
                    f"Make it personal, detailed, and directly relevant to their exact challenge or goal. "
                    f"Use concrete examples, realistic scenarios, and authentic emotions related to their specific situation. "
                    f"Write a story that feels like it was written specifically for them and their unique circumstances."
                )
                response = gemini_client.models.generate_content(
                    model="gemini-2.5-flash",
                    contents=full_prompt
                )
                story_text = (response.text or '').strip()
            except Exception as model_err:
                print(f"Gemini inspirational story error: {str(model_err)}")
                # Enhanced fallback to local generator if the model call fails
                story_text = _generate_enhanced_inspirational_story_fallback(prompt, mode)
        else:
            story_text = _generate_enhanced_inspirational_story_fallback(prompt, mode)

        # Ensure target length; attempt refinement if too short
        try:
            import re as _re
            word_count = len([w for w in _re.findall(r"\b\w+\b", story_text)])
            if word_count < 200 and gemini_client:
                refine_prompt = (
                    f"Expand this story to 250-350 words while making it MORE SPECIFIC to '{prompt}'. "
                    f"Add concrete details, realistic scenarios, and authentic emotions related to their exact situation. "
                    f"Follow this style guide: {style_guide}. Make it feel personal and tailored to their specific challenge. "
                    f"Return only the expanded story as plain paragraphs.\n\nCURRENT STORY:\n{story_text}"
                )
                refine_resp = gemini_client.models.generate_content(
                    model="gemini-2.5-flash",
                    contents=refine_prompt
                )
                refined = (refine_resp.text or '').strip()
                if refined:
                    story_text = refined
        except Exception as _:
            pass

        # Best-effort cleanup: strip code fences or accidental formatting
        try:
            cleaned = story_text.strip()
            if cleaned.startswith('```'):
                lines = [l for l in cleaned.split('\n') if not l.strip().startswith('```')]
                cleaned = '\n'.join(lines).strip()
            story_text = cleaned
        except Exception:
            pass

        return jsonify({"story": story_text})

    except Exception as e:
        import traceback
        print("Inspirational story generation error:\n" + traceback.format_exc())
        return jsonify({'error': f'Failed to generate story: {str(e)}'}), 500

def _generate_emotional_journey_fallback(transcript: str) -> str:
    """Local fallback to create an emotional journey with two contrasting paths.

    Produces a concise, readable narrative with a Positive Path and a Negative Path
    derived loosely from the provided transcript text.
    """
    try:
        import textwrap
        excerpt = (transcript or '').strip()
        if len(excerpt) > 600:
            excerpt = excerpt[:600].rstrip() + "..."

        positive = (
            "Positive Path ‚Äî Turning Toward Light:\n"
            "They notice the quiet courage inside the moment. Small choices‚Äîpausing, listening, asking for help‚Äî"
            "begin to stitch a steadier rhythm. With each honest step, the noise softens and what matters grows clearer. "
            "They forgive what can't be changed and commit to what can. The day opens, gently. Possibility shows up in familiar rooms, "
            "like a window unlatched, and they realize the path forward is built from ordinary tenderness repeated on purpose."
        )

        negative = (
            "Negative Path ‚Äî Slipping into Shadows:\n"
            "They hurry past the ache and double down on certainty. Pride hardens; kindness feels optional. "
            "Conversations shrink to sharp edges and long silences. The world narrows until every hallway looks the same, "
            "and sleep offers only thin relief. Regret gathers in corners, whispering what might have been if only they had paused "
            "to listen‚Äîfirst to themselves, then to the ones who stayed."
        )

        intro = (
            "Emotional Journey:\n"
            "Below are two contrasting arcs inspired by the transcript‚Äîhow different choices steer the feeling of the day.\n\n"
        )

        body = f"{positive}\n\n{negative}"
        return textwrap.dedent(intro + body).strip()
    except Exception:
        return (
            "Emotional Journey:\n\n"
            "Positive Path ‚Äî Choosing care, patience, and help, they discover steadier ground and a widening horizon.\n\n"
            "Negative Path ‚Äî Refusing to slow down or listen, they drift toward isolation and regret."
        )


@app.route('/generate_emotional_journey', methods=['POST'])
def generate_emotional_journey():
    """Create an emotional journey story with positive and negative paths from a transcript."""
    try:
        data = request.get_json(silent=True) or {}
        transcript = (data.get("transcript") or "").strip()

        if not transcript:
            return jsonify({"error": "Transcript is required"}), 400

        prompt = (
            "You are an AI storyteller. Create an emotional journey story from this transcript of a personal video.\n\n"
            f"TRANSCRIPT:\n{transcript}\n\n"
            "Guidelines:\n"
            "- Present two contrasting storylines:\n"
            "  1) The positive path (good choices, uplifting outcomes).\n"
            "  2) The negative path (bad choices, challenges, regrets).\n"
            "- Show how decisions shape the emotional arc.\n"
            "- Use vivid, emotional language; keep it meaningful and easy to follow.\n"
            "- Return ONLY plain text with NO markdown formatting (no **, ##, etc.)\n"
            "- Use simple titles like 'The Bright Path' and 'The Shadowed Path'\n"
            "- Write in a natural, conversational tone"
        )

        story_text = None
        if gemini_client:
            try:
                response = gemini_client.models.generate_content(
            model="gemini-2.5-flash",
            contents=prompt
        )
                story_text = (getattr(response, 'text', '') or '').strip()
            except Exception as model_err:
                print(f"Gemini emotional journey error: {str(model_err)}")
                story_text = _generate_emotional_journey_fallback(transcript)
        else:
            story_text = _generate_emotional_journey_fallback(transcript)

        # Clean up markdown formatting and code fences
        try:
            cleaned = story_text.strip()
            
            # Remove code fences
            if cleaned.startswith('```'):
                lines = [l for l in cleaned.split('\n') if not l.strip().startswith('```')]
                cleaned = '\n'.join(lines).strip()
            
            # Remove markdown headers (##, ###, etc.)
            cleaned = re.sub(r'^#+\s*', '', cleaned, flags=re.MULTILINE)
            
            # Remove bold formatting (**text**)
            cleaned = re.sub(r'\*\*(.*?)\*\*', r'\1', cleaned)
            
            # Remove italic formatting (*text*)
            cleaned = re.sub(r'\*(.*?)\*', r'\1', cleaned)
            
            # Clean up extra whitespace
            cleaned = re.sub(r'\n\s*\n', '\n\n', cleaned)
            cleaned = cleaned.strip()
            
            story_text = cleaned
        except Exception:
            pass

        return jsonify({"emotional_journey_story": story_text})

    except Exception as e:
        import traceback
        print("Emotional journey generation error:\n" + traceback.format_exc())
        return jsonify({"error": f"Failed to generate emotional journey: {str(e)}"}), 500

def generate_story_with_gemini(transcript, word_timestamps, visual_tags, prompt, video_id, mode):
    """Generate story using Gemini AI with enhanced prompts and error handling"""
    try:
        # Calculate video duration for better scene planning
        duration = 60  # Default
        if word_timestamps and isinstance(word_timestamps, list) and len(word_timestamps) > 0:
            valid_timestamps = [ts for ts in word_timestamps if ts and isinstance(ts, dict) and 'end_time' in ts]
            if valid_timestamps:
                duration = max(ts['end_time'] for ts in valid_timestamps)
        
        # Enhanced prompt with better context and instructions for focused clips
        # Define mode-specific instructions
        mode_instructions = {
            'normal': 'Create a balanced, neutral story that presents the content objectively with engaging narration.',
            'positive': 'Create an uplifting, optimistic story that highlights positive moments, achievements, and joyful experiences. Use encouraging and inspiring language.',
            'negative': 'Create a dramatic, intense story that emphasizes challenges, conflicts, or emotional moments. Use more serious and dramatic language.'
        }
        
        mode_instruction = mode_instructions.get(mode.lower(), mode_instructions['normal'])
        
        # Extract visual tag names for better context
        visual_tag_names = []
        if visual_tags and isinstance(visual_tags, list):
            for tag in visual_tags:
                if isinstance(tag, dict) and 'tag' in tag:
                    visual_tag_names.append(tag['tag'])
                elif isinstance(tag, str):
                    visual_tag_names.append(tag)
        
        visual_elements_text = ', '.join(visual_tag_names) if visual_tag_names else 'No specific visual elements detected'
        
        context = f"""
        You are a professional video storyteller creating engaging video stories with FOCUSED CLIPS. Analyze the following video content and create a compelling narrative with SHORT, RELEVANT scenes.

        VIDEO CONTENT:
        - Transcript: "{transcript[:800]}{'...' if len(transcript) > 800 else ''}"
        - Visual Elements: {visual_elements_text}
        - Video Duration: {duration} seconds
        - Word Count: {len(word_timestamps) if word_timestamps else 0} words with timing data
        
        USER REQUEST: "{prompt}"
        STORY MODE: {mode.upper()}
        MODE INSTRUCTION: {mode_instruction}
        
        TASK: Create a compelling video story with 3-4 FOCUSED scenes that:
        1. DIRECTLY RESPONDS to the user's specific request: "{prompt}"
        2. Uses the ACTUAL VIDEO TRANSCRIPT content and visual elements
        3. Creates engaging, emotional storytelling that matches the story mode
        4. Provides SHORT, FOCUSED timestamps (NOT covering entire video)
        
        CRITICAL: The story MUST be based on the actual transcript content and MUST address the user's specific request: "{prompt}"
        
        CRITICAL REQUIREMENTS:
        - Create exactly 3-4 scenes (NOT covering entire video)
        - Each scene should be 5-8 seconds long (SHORT and FOCUSED)
        - Use NON-SEQUENTIAL timestamps with gaps between scenes
        - Start scenes at different points in the video (not 0, 15, 30, etc.)
        - Write engaging, emotional captions and narration
        - TOTAL RENDERED VIDEO SHOULD BE 15-25 seconds (not full video)
        
        STORY MODE REQUIREMENTS:
        - NORMAL MODE: Balanced, objective storytelling with engaging narration
        - POSITIVE MODE: Uplifting, optimistic tone highlighting positive moments and achievements
        - NEGATIVE MODE: Dramatic, intense tone emphasizing challenges and emotional moments
        
        RESPONSE FORMAT: Return ONLY a valid JSON object with this exact structure:
        {{
            "storyId": "story_{video_id}_{int(datetime.now().timestamp())}",
            "scenes": [
                {{
                    "start": 5.0,
                    "end": 12.0,
                    "caption": "Engaging scene title with emoji",
                    "narration": "Compelling narration that tells the story with emotion and detail"
                }}
            ]
        }}
        
        IMPORTANT: 
        - Use emojis in captions for visual appeal
        - Make narration emotional and descriptive
        - Ensure timestamps are SHORT and FOCUSED (5-8 seconds each)
        - DO NOT cover the entire video - create highlights only
        - Focus on the user's specific request
        - Create a story that flows naturally with gaps between scenes
        - ADAPT THE TONE AND LANGUAGE TO MATCH THE SELECTED MODE
        """
        
        print(f"DEBUG: Sending request to Gemini with context length: {len(context)}")
        response = gemini_client.models.generate_content(
            model="gemini-2.5-flash",
            contents=context
        )
        story_json = response.text.strip()
        
        print(f"DEBUG: Gemini response received: {story_json[:200]}...")
        
        # Enhanced JSON parsing with better error handling
        import re
        json_match = re.search(r'\{.*\}', story_json, re.DOTALL)
        if json_match:
            story_data = json.loads(json_match.group())
            
            # Validate the response structure
            if 'storyId' not in story_data or 'scenes' not in story_data:
                raise Exception("Invalid story structure from Gemini")
            
            if not isinstance(story_data['scenes'], list) or len(story_data['scenes']) == 0:
                raise Exception("No scenes generated by Gemini")
            
            # Validate each scene
            for i, scene in enumerate(story_data['scenes']):
                if not all(key in scene for key in ['start', 'end', 'caption', 'narration']):
                    raise Exception(f"Invalid scene structure at index {i}")
                
                # Ensure timestamps are valid
                if not isinstance(scene['start'], (int, float)) or not isinstance(scene['end'], (int, float)):
                    raise Exception(f"Invalid timestamps in scene {i}")
                
                if scene['start'] >= scene['end']:
                    raise Exception(f"Invalid timestamp order in scene {i}")
            
            print(f"DEBUG: Successfully parsed Gemini response with {len(story_data['scenes'])} scenes")
            return story_data
        else:
            raise Exception("No valid JSON found in Gemini response")
        
    except Exception as e:
        print(f"Gemini story generation error: {str(e)}")
        print(f"Falling back to enhanced mock story generation...")
        # Fallback to enhanced mock data
        return generate_mock_story(transcript, word_timestamps, visual_tags, prompt, video_id, mode)

def generate_mock_story(transcript, word_timestamps, visual_tags, prompt, video_id, mode):
    """Generate TRULY DYNAMIC story based on actual video content with focused clips"""
    
    print(f"DEBUG: Mock story generation - mode: '{mode}', prompt: '{prompt}'")
    print(f"DEBUG: Using fallback mock story generator (Gemini quota exceeded)")
    
    import random
    import re
    
    # Calculate video duration from timestamps
    duration = 60  # Default duration
    
    try:
        if word_timestamps and isinstance(word_timestamps, list) and len(word_timestamps) > 0:
            # Filter out invalid timestamps
            valid_timestamps = [ts for ts in word_timestamps if ts and isinstance(ts, dict) and 'end_time' in ts]
            if valid_timestamps:
                duration = max(ts['end_time'] for ts in valid_timestamps)
    except Exception as e:
        print(f"Error calculating duration: {str(e)}")
        duration = 60

    # Generate single storyline
    story_id = str(uuid.uuid4())
        
    # Analyze transcript for better context
    transcript_lower = transcript.lower() if transcript else ""
    
    # Extract actual content themes from the video
    content_themes = []
    if any(word in transcript_lower for word in ['cup', 'cups', 'ice', 'snow', 'cone', 'cones']):
        content_themes.append('snow_cones')
    if any(word in transcript_lower for word in ['kitchen', 'cook', 'cooking', 'recipe']):
        content_themes.append('cooking')
    if any(word in transcript_lower for word in ['party', 'celebrate', 'birthday']):
        content_themes.append('celebration')
    if any(word in transcript_lower for word in ['family', 'mom', 'dad', 'kids']):
        content_themes.append('family')
    if any(word in transcript_lower for word in ['friend', 'friends', 'together']):
        content_themes.append('friends')
    
    print(f"DEBUG: Content themes detected: {content_themes}")
    
    # Create focused scenes with shorter durations (3-8 seconds each)
    scenes = []
        
    # Determine scene count and duration based on video length
    if duration <= 30:
        scene_count = 2
        scene_duration = 5  # 5 seconds per scene
    elif duration <= 60:
        scene_count = 3
        scene_duration = 6  # 6 seconds per scene
    elif duration <= 120:
        scene_count = 4
        scene_duration = 7  # 7 seconds per scene
    else:
        scene_count = 5
        scene_duration = 8  # 8 seconds per scene
    
    # Generate mode-specific and content-specific content based on user's prompt
    prompt_lower = prompt.lower() if prompt else ""
    
    # Create content based on user's specific prompt and transcript themes
    if mode.lower() == "positive":
        if 'snow_cones' in content_themes:
            captions = [
                f"üç¶ Wonderful {prompt.title()} Adventure",
                f"‚ùÑÔ∏è Magical {prompt.title()} Moments", 
                f"üéâ Joyful {prompt.title()} Experience",
                f"üåü Perfect {prompt.title()} Delight"
            ]
            narrations = [
                f"Our wonderful {prompt} adventure begins with pure excitement and joy.",
                f"The magical {prompt} moments unfold with amazing energy and happiness.",
                f"Joyful {prompt} experience brings incredible satisfaction and delight.",
                f"Perfect {prompt} delight concludes with fantastic memories and bliss."
            ]
        else:
            captions = [
                f"üåü Wonderful {prompt.title()} Begins",
                f"‚ú® Magical {prompt.title()} Unfolds", 
                f"üéâ Joyful {prompt.title()} Continues",
                f"üí´ Perfect {prompt.title()} Emerges"
            ]
            narrations = [
                f"Our wonderful {prompt} begins with pure excitement and joy.",
                f"Magical {prompt} unfolds with amazing energy and happiness.",
                f"Joyful {prompt} continues with incredible satisfaction and delight.",
                f"Perfect {prompt} emerges with fantastic memories and bliss."
            ]
    elif mode.lower() == "negative":
        if 'snow_cones' in content_themes:
            captions = [
                f"üç¶ Intense {prompt.title()} Challenge",
                f"‚ùÑÔ∏è Dramatic {prompt.title()} Struggle", 
                f"üéØ Powerful {prompt.title()} Conflict",
                f"üåü Compelling {prompt.title()} Drama"
            ]
            narrations = [
                f"Our intense {prompt} challenge begins with dramatic tension and conflict.",
                f"The dramatic {prompt} struggle unfolds with powerful determination and resilience.",
                f"Powerful {prompt} conflict brings emotional intensity and transformation.",
                f"Compelling {prompt} drama concludes with striking impact and growth."
            ]
        else:
            captions = [
                f"üåü Intense {prompt.title()} Challenge",
                f"‚ú® Dramatic {prompt.title()} Struggle", 
                f"üéØ Powerful {prompt.title()} Conflict",
                f"üí´ Compelling {prompt.title()} Drama"
            ]
            narrations = [
                f"Our intense {prompt} challenge begins with dramatic tension and conflict.",
                f"Dramatic {prompt} struggle unfolds with powerful determination and resilience.",
                f"Powerful {prompt} conflict continues with emotional intensity and transformation.",
                f"Compelling {prompt} drama emerges with striking impact and growth."
            ]
    else:  # normal mode
        if 'snow_cones' in content_themes:
            captions = [
                f"üç¶ Dynamic {prompt.title()} Journey",
                f"‚ùÑÔ∏è Engaging {prompt.title()} Experience", 
                f"üéØ Captivating {prompt.title()} Story",
                f"üåü Memorable {prompt.title()} Adventure"
            ]
            narrations = [
                f"Our dynamic {prompt} journey begins with engaging energy and focus.",
                f"The engaging {prompt} experience unfolds with captivating precision and drive.",
                f"Captivating {prompt} story brings memorable presentation and spirit.",
                f"Memorable {prompt} adventure concludes with impressive satisfaction and vitality."
            ]
        else:
            captions = [
                f"üåü Dynamic {prompt.title()} Journey",
                f"‚ú® Engaging {prompt.title()} Experience", 
                f"üéØ Captivating {prompt.title()} Story",
                f"üí´ Memorable {prompt.title()} Adventure"
            ]
            narrations = [
                f"Our dynamic {prompt} journey begins with engaging energy and focus.",
                f"Engaging {prompt} experience unfolds with captivating precision and drive.",
                f"Captivating {prompt} story continues with memorable presentation and spirit.",
                f"Memorable {prompt} adventure emerges with impressive satisfaction and vitality."
            ]
            
    # Create focused scenes with gaps between them (not covering entire video)
    for i in range(scene_count):
        # Calculate start time with gaps to avoid covering entire video
        # Start at 10% of video, then add gaps between scenes
        base_start = duration * 0.1  # Start at 10% of video
        gap_between_scenes = (duration * 0.6) / (scene_count + 1)  # Use 60% of video for gaps
        
        start_time = base_start + (i * gap_between_scenes)
        end_time = start_time + scene_duration
        
        # Ensure we don't exceed video duration
        if end_time > duration:
            end_time = duration
            start_time = max(0, end_time - scene_duration)
        
        # Get caption and narration with random variations
        caption = captions[i % len(captions)]
        narration = narrations[i % len(narrations)]
        
        # Add significant randomness to make each generation unique
        if random.random() < 0.5:
            caption = caption.replace("Begins", random.choice(["Starts", "Unfolds", "Emerges", "Takes Off", "Launches", "Commences"]))
        if random.random() < 0.5:
            caption = caption.replace("Unfolds", random.choice(["Develops", "Progresses", "Advances", "Evolves", "Grows", "Expands"]))
        if random.random() < 0.5:
            caption = caption.replace("Continues", random.choice(["Persists", "Endures", "Sustains", "Maintains", "Keeps Going", "Proceeds"]))
        if random.random() < 0.5:
            caption = caption.replace("Emerges", random.choice(["Appears", "Surfaces", "Rises", "Manifests", "Comes Forth", "Shows Up"]))
        if random.random() < 0.5:
            caption = caption.replace("Wonderful", random.choice(["Amazing", "Incredible", "Fantastic", "Brilliant", "Spectacular", "Marvelous"]))
        if random.random() < 0.5:
            caption = caption.replace("Magical", random.choice(["Enchanting", "Mystical", "Spellbinding", "Fascinating", "Captivating", "Mesmerizing"]))
        if random.random() < 0.5:
            caption = caption.replace("Perfect", random.choice(["Ideal", "Flawless", "Excellent", "Superb", "Outstanding", "Exceptional"]))

        scenes.append({
            "start": round(start_time, 2),
            "end": round(end_time, 2),
            "caption": caption,
            "narration": narration
        })

    return {
        "storyId": story_id,
        "scenes": scenes
    }

def generate_contextual_content(transcript_lower, prompt, scene_index, total_scenes, mode, visual_tags):
    """Generate contextually relevant captions and narration based on ACTUAL video content"""
    
    prompt_lower = prompt.lower()
    
    # Extract key words and phrases from the actual transcript
    import re
    words = re.findall(r'\b\w+\b', transcript_lower)
    word_freq = {}
    for word in words:
        if len(word) > 3:  # Only meaningful words
            word_freq[word] = word_freq.get(word, 0) + 1
    
    # Get top 10 most frequent words from the actual content
    top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:10]
    key_content_words = [word for word, freq in top_words if freq > 1]
    
    print(f"DEBUG: Key content words from transcript: {key_content_words}")
    
    # Extract actual sentences from transcript
    sentences = re.split(r'[.!?]+', transcript_lower)
    sentences = [s.strip() for s in sentences if len(s.strip()) > 10]
    
    # Get actual content themes from the video
    content_themes = []
    if any(word in transcript_lower for word in ['cup', 'cups', 'ice', 'snow', 'cone']):
        content_themes.append('food_drinks')
    if any(word in transcript_lower for word in ['kitchen', 'cook', 'cooking', 'recipe']):
        content_themes.append('cooking')
    if any(word in transcript_lower for word in ['party', 'celebrate', 'birthday']):
        content_themes.append('celebration')
    if any(word in transcript_lower for word in ['family', 'mom', 'dad', 'kids']):
        content_themes.append('family')
    if any(word in transcript_lower for word in ['friend', 'friends', 'together']):
        content_themes.append('friends')
    if any(word in transcript_lower for word in ['work', 'office', 'meeting']):
        content_themes.append('work')
    if any(word in transcript_lower for word in ['travel', 'visit', 'see']):
        content_themes.append('travel')
    
    # Mode-specific tone and emotion words
    if mode.lower() == "positive":
        tone_words = ["wonderful", "amazing", "beautiful", "incredible", "fantastic", "perfect", "magical", "special", "memorable", "joyful", "uplifting", "inspiring"]
        emotion_words = ["excitement", "joy", "happiness", "wonder", "delight", "enthusiasm", "cheer", "bliss", "optimism", "hope"]
    elif mode.lower() == "negative":
        tone_words = ["dramatic", "intense", "powerful", "challenging", "emotional", "confrontational", "serious", "deep", "profound", "impactful", "striking", "compelling"]
        emotion_words = ["tension", "conflict", "struggle", "determination", "resilience", "overcoming", "challenge", "growth", "transformation"]
    else:  # normal mode
        tone_words = ["dynamic", "engaging", "captivating", "interesting", "memorable", "special", "unique", "remarkable", "notable", "impressive", "balanced", "objective"]
        emotion_words = ["energy", "passion", "intensity", "focus", "determination", "drive", "spirit", "vitality", "clarity", "understanding"]
    
    # Generate content-specific captions and narrations based on actual video content
    if 'food_drinks' in content_themes:
        captions = [
            "üçΩÔ∏è Culinary Adventure Begins",
            "ü•§ Refreshment Magic Unfolds", 
            "üçπ Delicious Creations Emerge",
            "üéØ Perfect Food Moments",
            "üåü Culinary Excellence",
            "üí´ Perfect Dining Experience"
        ]
        narrations = [
            f"Our {tone_words[0]} culinary adventure begins with fresh ingredients and {emotion_words[0]}.",
            f"The kitchen comes alive with {tone_words[1]} cooking magic and {emotion_words[1]}.",
            f"Delicious creations emerge with {tone_words[2]} flavors and {emotion_words[2]}.",
            f"Perfect food moments showcase {tone_words[3]} techniques and {emotion_words[3]}.",
            f"Culinary excellence is achieved with {tone_words[4]} presentation and {emotion_words[4]}.",
            f"The perfect dining experience concludes with {tone_words[5]} satisfaction and {emotion_words[5]}."
        ]
    elif 'celebration' in content_themes:
        captions = [
            "üéÇ Celebration Magic Begins",
            "üéâ Party Excitement Peaks",
            "üéÅ Special Moments Unfold",
            "‚ú® Joy Continues",
            "üåü Celebration Highlights",
            "üí´ Perfect Celebration Ending"
        ]
        narrations = [
            f"The {tone_words[0]} celebration begins with pure {emotion_words[0]} and anticipation.",
            f"The party atmosphere reaches new heights with {tone_words[1]} energy and {emotion_words[1]}.",
            f"Special moments unfold with {tone_words[2]} memories being created in real-time.",
            f"The celebration continues with {tone_words[3]} joy and {emotion_words[2]} filling every moment.",
            f"Celebration highlights showcase the {tone_words[4]} moments that make this day special.",
            f"A {tone_words[5]} conclusion to our {emotion_words[3]} celebration."
        ]
    elif 'family' in content_themes:
        captions = [
            "üë®‚Äçüë©‚Äçüëß‚Äçüë¶ Family Time Begins",
            "üíï Loving Moments Shared",
            "üè† Home Filled with Joy",
            "‚ù§Ô∏è Family Bonds Strengthen",
            "üåü Family Highlights",
            "üí´ Perfect Family Day"
        ]
        narrations = [
            f"Precious {tone_words[0]} family time brings everyone together with {emotion_words[0]}.",
            f"Loving moments are shared with {tone_words[1]} warmth and {emotion_words[1]}.",
            f"Home is filled with {tone_words[2]} togetherness and {emotion_words[2]}.",
            f"Family bonds grow stronger with {tone_words[3]} love and {emotion_words[3]}.",
            f"Family highlights showcase {tone_words[4]} connections and {emotion_words[4]}.",
            f"A perfect family day concludes with {tone_words[5]} memories and {emotion_words[5]}."
        ]
    else:
        # Generic but content-aware based on actual words
        captions = [
            "üé¨ Story Begins",
            "üåü Magic Unfolds",
            "‚ú® Special Moments",
            "üéØ Perfect Timing",
            "üí´ Highlights Emerge",
            "üéâ Perfect Ending"
        ]
        narrations = [
            f"Our {tone_words[0]} story begins with {emotion_words[0]} and anticipation.",
            f"The magic unfolds with {tone_words[1]} energy and {emotion_words[1]}.",
            f"Special moments emerge with {tone_words[2]} precision and {emotion_words[2]}.",
            f"Perfect timing showcases {tone_words[3]} techniques and {emotion_words[3]}.",
            f"Highlights emerge with {tone_words[4]} presentation and {emotion_words[4]}.",
            f"The perfect ending concludes with {tone_words[5]} satisfaction and {emotion_words[5]}."
        ]
    
    # Select caption and narration based on scene index
    caption_index = min(scene_index, len(captions) - 1)
    narration_index = min(scene_index, len(narrations) - 1)
    
    return captions[caption_index], narrations[narration_index]

def create_intelligent_scenes(transcript, word_timestamps, visual_tags, prompt, duration, mode):
    """Create intelligent scenes based on actual video content with precise storytelling"""
    scenes = []
    
    # Analyze transcript for natural breaks and topics
    sentences = split_into_sentences(transcript)
    
    # Find key moments and natural story breaks
    try:
        key_moments = find_key_moments(word_timestamps, transcript, prompt)
    except Exception as e:
        print(f"Error in find_key_moments: {str(e)}")
        import traceback
        print(f"Traceback: {traceback.format_exc()}")
        key_moments = []
    
    if key_moments:
        # Create scenes based on key moments
        for i, moment in enumerate(key_moments):
            start_time = moment['start_time']
            end_time = moment['end_time']
            scene_text = moment['text']
            
            scenes.append({
                "start": round(start_time, 2),
                "end": round(end_time, 2),
                "caption": generate_precise_caption(scene_text, visual_tags, prompt, i + 1),
                "narration": generate_precise_narration(scene_text, visual_tags, prompt, i + 1, len(key_moments), mode)
            })
    else:
        # Fallback: create 3 focused scenes
        scene_count = 3
        for i in range(scene_count):
            start_idx = i * len(word_timestamps) // scene_count
            end_idx = (i + 1) * len(word_timestamps) // scene_count
            
            if start_idx < len(word_timestamps) and end_idx <= len(word_timestamps):
                start_time = word_timestamps[start_idx]['start_time']
                end_time = word_timestamps[min(end_idx - 1, len(word_timestamps) - 1)]['end_time']
                
                # Get words for this scene
                scene_words = word_timestamps[start_idx:end_idx]
                scene_text = ' '.join([w['word'] for w in scene_words])
                
                scenes.append({
                    "start": round(start_time, 2),
                    "end": round(end_time, 2),
                    "caption": generate_precise_caption(scene_text, visual_tags, prompt, i + 1),
                    "narration": generate_precise_narration(scene_text, visual_tags, prompt, i + 1, scene_count, mode)
                })
    
    return scenes

def find_key_moments(word_timestamps, transcript, prompt):
    """Find key moments in the video for precise storytelling - SHORT SCENES ONLY"""
    key_moments = []
    
    # Validate input
    if not word_timestamps or not isinstance(word_timestamps, list):
        print(f"DEBUG: Invalid word_timestamps: {type(word_timestamps)}")
        return []
    
    # Look for emotional or action words based on prompt
    if "birthday" in prompt.lower():
        key_words = ['happy', 'celebrate', 'birthday', 'party', 'cake', 'gift', 'joy', 'fun', 'excited']
    elif "food" in prompt.lower() or "cooking" in prompt.lower():
        key_words = ['cook', 'food', 'eat', 'delicious', 'taste', 'kitchen', 'recipe', 'meal']
    elif "travel" in prompt.lower():
        key_words = ['travel', 'visit', 'see', 'beautiful', 'amazing', 'wonderful', 'explore', 'adventure']
    elif "celebration" in prompt.lower() or "joy" in prompt.lower():
        key_words = ['celebration', 'joy', 'happy', 'fun', 'excited', 'great', 'wonderful', 'amazing', 'cool', 'awesome', 'hey', 'check', 'great', 'job', 'snow', 'cones']
    else:
        key_words = ['wow', 'amazing', 'beautiful', 'great', 'wonderful', 'excited', 'happy', 'fun']
    
    # Find moments with key words - create VERY SHORT, FOCUSED moments (3-5 seconds max)
    current_moment = None
    for i, word_info in enumerate(word_timestamps):
        # Validate word_info
        if not word_info or not isinstance(word_info, dict):
            print(f"DEBUG: Invalid word_info at index {i}: {word_info}")
            continue
            
        if 'word' not in word_info or 'start_time' not in word_info or 'end_time' not in word_info:
            print(f"DEBUG: Missing required fields in word_info at index {i}: {word_info}")
            continue
        word_lower = word_info['word'].lower().strip()
        
        # Check if this word is a key moment
        is_key_word = any(key_word in word_lower for key_word in key_words)
        
        if is_key_word:
            # Start or extend current moment
            if current_moment is None:
                current_moment = {
                    'start_time': word_info['start_time'],
                    'end_time': word_info['end_time'],
                    'text': word_info['word'],
                    'words': [word_info['word']]
                }
            else:
                # Extend current moment if it's close in time (VERY SHORT WINDOW)
                if word_info['start_time'] - current_moment['end_time'] < 0.8:  # Within 0.8 seconds
                    current_moment['end_time'] = word_info['end_time']
                    current_moment['text'] += ' ' + word_info['word']
                    current_moment['words'].append(word_info['word'])
                else:
                    # Save current moment and start new one
                    if len(current_moment['words']) >= 2:  # At least 2 words
                        key_moments.append(current_moment)
                    current_moment = {
                        'start_time': word_info['start_time'],
                        'end_time': word_info['end_time'],
                        'text': word_info['word'],
                        'words': [word_info['word']]
                    }
        elif current_moment is not None:
            # Extend current moment with context words (VERY SHORT WINDOW)
            if word_info['start_time'] - current_moment['end_time'] < 0.5:  # Within 0.5 seconds
                current_moment['end_time'] = word_info['end_time']
                current_moment['text'] += ' ' + word_info['word']
                current_moment['words'].append(word_info['word'])
            else:
                # Save current moment if it has enough content
                if len(current_moment['words']) >= 2:  # At least 2 words
                    key_moments.append(current_moment)
                current_moment = None
    
    # Add final moment if exists
    if current_moment is not None and len(current_moment['words']) >= 2:
        key_moments.append(current_moment)
    
    # ENFORCE VERY SHORT SCENES - limit duration to 3-5 seconds max
    filtered_moments = []
    for moment in key_moments:
        duration = moment['end_time'] - moment['start_time']
        if duration <= 5.0:  # Only keep moments under 5 seconds
            filtered_moments.append(moment)
    
    # Limit to 3-4 key moments for focused storytelling
    if len(filtered_moments) > 4:
        filtered_moments = filtered_moments[:4]
    elif len(filtered_moments) < 2:
        # If not enough key moments, create balanced scenes
        return []
    
    return filtered_moments

def split_into_sentences(text):
    """Split transcript into sentences"""
    import re
    # Split by common sentence endings
    sentences = re.split(r'[.!?]+', text)
    return [s.strip() for s in sentences if s.strip()]

def generate_precise_caption(scene_text, visual_tags, prompt, scene_num):
    """Generate precise, focused caption based on actual scene content"""
    # Extract key words from scene text
    key_words = extract_key_words(scene_text)
    
    # Create precise caption based on actual content
    if "birthday" in prompt.lower():
        if any(word in scene_text.lower() for word in ['cake', 'gift', 'party']):
            return f"Scene {scene_num}: Birthday Surprise"
        elif any(word in scene_text.lower() for word in ['happy', 'joy', 'celebrate']):
            return f"Scene {scene_num}: Celebration Joy"
        else:
            return f"Scene {scene_num}: Birthday Moment"
    elif "food" in prompt.lower() or "cooking" in prompt.lower():
        if any(word in scene_text.lower() for word in ['cook', 'kitchen', 'recipe']):
            return f"Scene {scene_num}: Cooking Magic"
        elif any(word in scene_text.lower() for word in ['eat', 'taste', 'delicious']):
            return f"Scene {scene_num}: Culinary Delight"
        else:
            return f"Scene {scene_num}: Food Adventure"
    elif "travel" in prompt.lower():
        if any(word in scene_text.lower() for word in ['beautiful', 'amazing', 'wonderful']):
            return f"Scene {scene_num}: Beautiful Discovery"
        elif any(word in scene_text.lower() for word in ['travel', 'visit', 'explore']):
            return f"Scene {scene_num}: Travel Adventure"
        else:
            return f"Scene {scene_num}: Journey Moment"
    elif "celebration" in prompt.lower() or "joy" in prompt.lower():
        if any(word in scene_text.lower() for word in ['hey', 'check', 'cup', 'chandelier']):
            return f"Scene {scene_num}: Creative Setup"
        elif any(word in scene_text.lower() for word in ['great', 'job', 'snow', 'cones']):
            return f"Scene {scene_num}: Snow Cone Success"
        elif any(word in scene_text.lower() for word in ['ice', 'crushed', 'cubed', 'dry']):
            return f"Scene {scene_num}: Ice Variety"
        else:
            return f"Scene {scene_num}: Celebration Moment"
    else:
        # Generic precise caption based on actual content
        if len(key_words) > 0:
            return f"Scene {scene_num}: {key_words[0].title()} Highlight"
        else:
            return f"Scene {scene_num}: Key Moment"

def generate_caption(scene_text, visual_tags, prompt, scene_num):
    """Generate intelligent caption based on scene content"""
    # Extract key words from scene text
    key_words = extract_key_words(scene_text)
    
    # Combine with visual tags if available
    if visual_tags:
        visual_context = f" featuring {', '.join(visual_tags[:3])}"
    else:
        visual_context = ""
    
    # Create caption based on prompt and content
    if "birthday" in prompt.lower():
        return f"Scene {scene_num}: Birthday Celebration{visual_context}"
    elif "food" in prompt.lower() or "cooking" in prompt.lower():
        return f"Scene {scene_num}: Culinary Adventure{visual_context}"
    elif "travel" in prompt.lower():
        return f"Scene {scene_num}: Travel Moment{visual_context}"
    else:
        # Generic caption based on content
        if len(key_words) > 0:
            return f"Scene {scene_num}: {key_words[0].title()} Moment{visual_context}"
        else:
            return f"Scene {scene_num}: Engaging Content{visual_context}"

def generate_precise_narration(scene_text, visual_tags, prompt, scene_num, total_scenes, mode):
    """Generate precise, focused narration based on actual scene content"""
    # Extract key information from scene text
    key_words = extract_key_words(scene_text)
    
    # Create context-aware narration
    if mode == "positive":
        tone = "uplifting"
    else:
        tone = "dynamic"
    
    # Build precise narration based on actual content
    if scene_text.strip():
        # Use actual content from the scene
        if scene_num == 1:
            # Opening scene
            narration = f"In this {tone} opening, we see {scene_text.strip()}"
        elif scene_num == total_scenes:
            # Closing scene
            narration = f"Finally, we witness {scene_text.strip()}"
        else:
            # Middle scenes - use varied transitions
            transitions = [
                f"Next, we discover {scene_text.strip()}",
                f"As the story unfolds, {scene_text.strip()}",
                f"Moving forward, {scene_text.strip()}",
                f"Then, we see {scene_text.strip()}",
                f"Continuing our journey, {scene_text.strip()}"
            ]
            narration = transitions[scene_num % len(transitions)]
        
        # Add varied context based on prompt and scene number
        if "celebration" in prompt.lower() or "joy" in prompt.lower():
            if scene_num == 1:
                narration += ". The excitement builds as creativity takes center stage."
            elif scene_num == total_scenes:
                narration += ". A perfect celebration of friendship and fun."
            else:
                # Use varied celebration endings
                celebration_endings = [
                    ". The joy of discovery fills the air.",
                    ". Laughter and creativity combine perfectly.",
                    ". A moment of pure celebration magic.",
                    ". The spirit of fun shines through.",
                    ". Friendship and joy intertwine beautifully."
                ]
                narration += celebration_endings[scene_num % len(celebration_endings)]
        elif "birthday" in prompt.lower():
            if scene_num == total_scenes:
                narration += ". This captures the pure joy of celebration."
            else:
                narration += ". A moment of birthday magic."
        elif "food" in prompt.lower():
            if scene_num == total_scenes:
                narration += ". The perfect ending to our culinary journey."
            else:
                narration += ". A taste of culinary excellence."
        else:
            if scene_num == total_scenes:
                narration += ". A perfect conclusion to our story."
            else:
                # Use varied generic endings
                generic_endings = [
                    ". This moment adds depth to our story.",
                    ". The narrative continues to unfold.",
                    ". Each scene builds upon the last.",
                    ". The story takes an interesting turn.",
                    ". We're drawn deeper into the narrative."
                ]
                narration += generic_endings[scene_num % len(generic_endings)]
    else:
        if scene_num == 1:
            narration = f"Our {tone} story begins with this key moment."
        elif scene_num == total_scenes:
            narration = f"Finally, we reach the perfect ending to our story."
        else:
            narration = f"This moment adds depth to our narrative."
    
    return narration

def generate_narration(scene_text, visual_tags, prompt, mode, scene_num, total_scenes):
    """Generate intelligent narration based on scene content"""
    # Extract key information from scene text
    key_words = extract_key_words(scene_text)
    
    # Create context-aware narration
    if mode == "positive":
        tone = "uplifting and positive"
    else:
        tone = "contrasting and dynamic"
    
    # Build narration based on actual content
    if scene_text.strip():
        # Use actual content from the scene
        if scene_num == 1:
            # Opening scene
            narration = f"Our story begins with {scene_text.strip()}"
        elif scene_num == total_scenes:
            # Closing scene
            narration = f"Finally, we witness {scene_text.strip()}"
        else:
            # Middle scenes
            narration = f"As the story continues, {scene_text.strip()}"
        
        if visual_tags:
            narration += f". The visual elements include {', '.join(visual_tags[:2])}"
        
        if "birthday" in prompt.lower():
            if scene_num == total_scenes:
                narration += ". This beautiful moment captures the essence of celebration and joy that makes birthdays so special."
            else:
                narration += ". This captures the joy and celebration of a special day."
        elif "food" in prompt.lower():
            if scene_num == total_scenes:
                narration += ". This final scene showcases the art and passion of culinary creation that brings people together."
            else:
                narration += ". This showcases the art and passion of culinary creation."
        else:
            if scene_num == total_scenes:
                narration += ". This concluding moment adds depth and meaning to our complete story."
            else:
                narration += ". This moment adds depth and meaning to the overall story."
    else:
        if scene_num == 1:
            narration = f"Our {tone} story begins with Scene {scene_num} of {total_scenes}."
        elif scene_num == total_scenes:
            narration = f"Finally, Scene {scene_num} of {total_scenes} presents a {tone} moment that concludes our narrative."
        else:
            narration = f"Scene {scene_num} of {total_scenes} presents a {tone} moment that contributes to the narrative flow."
    
    return narration

def extract_key_words(text):
    """Extract key words from text for caption generation"""
    import re
    
    # Remove common words and punctuation
    words = re.findall(r'\b\w+\b', text.lower())
    
    # Filter out common words
    common_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'can', 'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them', 'my', 'your', 'his', 'her', 'its', 'our', 'their'}
    
    key_words = [word for word in words if word not in common_words and len(word) > 2]
    
    # Return unique words, prioritizing longer words
    return sorted(set(key_words), key=len, reverse=True)[:3]

@app.route('/search', methods=['POST'])
def search_video():
    """Search video content using natural language query"""
    try:
        data = request.get_json()
        query = data.get('query', '').lower().strip()
        video_id = data.get('videoId')
        
        if not query:
            return jsonify({'error': 'Search query is required'}), 400
        
        if not video_id:
            return jsonify({'error': 'Video ID is required'}), 400
        
        # Find the video metadata file
        video_metadata_file = os.path.join(UPLOAD_FOLDER, f"{video_id}_metadata.json")
        if not os.path.exists(video_metadata_file):
            return jsonify({'error': 'Video not found'}), 404
        
        with open(video_metadata_file, 'r') as f:
            video_metadata = json.load(f)
        
        # Get transcription and tags - check both nested and direct locations
        transcription_data = video_metadata.get('transcription', {})
        transcript = transcription_data.get('transcript', '') or video_metadata.get('transcript', '')
        word_timestamps = transcription_data.get('word_timestamps', []) or video_metadata.get('word_timestamps', [])
        visual_tags = video_metadata.get('visual_tags', [])
        
        # If no transcript found in JSON, try to get from database
        if not transcript:
            try:
                db_metadata = get_video_metadata(video_id)
                if db_metadata and db_metadata.get('transcript'):
                    transcript = db_metadata['transcript']
                    print(f"DEBUG: Found transcript in database: {len(transcript)} characters")
                if db_metadata and db_metadata.get('word_timestamps') and not word_timestamps:
                    word_timestamps = db_metadata['word_timestamps']
                    print(f"DEBUG: Found word_timestamps in database: {len(word_timestamps)} timestamps")
            except Exception as e:
                print(f"DEBUG: Could not get transcript from database: {str(e)}")
        
        print(f"DEBUG: Search - transcript length: {len(transcript) if transcript else 0}, word_timestamps: {len(word_timestamps) if word_timestamps else 0}")
        print(f"DEBUG: Search - transcription_data keys: {list(transcription_data.keys()) if transcription_data else 'None'}")
        print(f"DEBUG: Search - video_metadata keys: {list(video_metadata.keys())}")
        
        # Debug word_timestamps structure
        if word_timestamps:
            print(f"DEBUG: word_timestamps type: {type(word_timestamps)}")
            print(f"DEBUG: word_timestamps length: {len(word_timestamps)}")
            if len(word_timestamps) > 0:
                print(f"DEBUG: First word_timestamp entry: {word_timestamps[0]}")
                print(f"DEBUG: First entry type: {type(word_timestamps[0])}")
                if isinstance(word_timestamps[0], dict):
                    print(f"DEBUG: First entry keys: {list(word_timestamps[0].keys())}")
        else:
            print(f"DEBUG: word_timestamps is None or empty")
        
        search_results = []
        
        # Search in transcript with exact timestamps
        if transcript and word_timestamps:
            # Validate word_timestamps structure
            valid_timestamps = []
            if isinstance(word_timestamps, list):
                for ts in word_timestamps:
                    if isinstance(ts, dict) and 'word' in ts and 'start_time' in ts and 'end_time' in ts:
                        valid_timestamps.append(ts)
                    else:
                        print(f"DEBUG: Skipping invalid timestamp entry: {ts}")
                
                print(f"DEBUG: Valid timestamps: {len(valid_timestamps)} out of {len(word_timestamps)}")
                
                if valid_timestamps:
                    print(f"DEBUG: Using exact timestamp search with {len(valid_timestamps)} timestamps")
                    transcript_results = search_transcript_with_timestamps(transcript, valid_timestamps, query, video_id)
                    search_results.extend(transcript_results)
                else:
                    print(f"DEBUG: No valid timestamps, falling back to text search")
                    transcript_results = search_transcript(transcript, query, video_id)
                    search_results.extend(transcript_results)
            else:
                print(f"DEBUG: word_timestamps is not a list, falling back to text search")
                transcript_results = search_transcript(transcript, query, video_id)
                search_results.extend(transcript_results)
        elif transcript:
            # Fallback to old method if no timestamps available
            print(f"DEBUG: Using fallback search (no timestamps available)")
            transcript_results = search_transcript(transcript, query, video_id)
            search_results.extend(transcript_results)
        
        # Search in tags
        if visual_tags:
            tag_results = search_tags(visual_tags, query, video_id)
            search_results.extend(tag_results)
        
        # Sort results by score (highest first)
        search_results.sort(key=lambda x: x['score'], reverse=True)
        
        print(f"DEBUG: Search returned {len(search_results)} results")
        for i, result in enumerate(search_results[:3]):  # Show first 3 results
            print(f"DEBUG: Result {i+1}: {result.get('type')} at {result.get('start_time')}-{result.get('end_time')}s, match_type: {result.get('match_type')}")
            print(f"DEBUG: Result {i+1} preview: {result.get('preview_text', '')[:50]}...")
        
        print(f"DEBUG: Final response - success: True, totalResults: {len(search_results)}")
        return jsonify({
            'success': True,
            'query': query,
            'videoId': video_id,
            'results': search_results,
            'totalResults': len(search_results)
        })
        
    except Exception as e:
        import traceback
        print("Search error:")
        traceback.print_exc()
        return jsonify({'error': f'Search failed: {str(e)}'}), 500

@app.route('/global-search', methods=['POST'])
def global_search():
    """Search across all user videos"""
    try:
        data = request.get_json()
        query = data.get('query', '').strip()
        user_id = data.get('userId')  # Optional: filter by user
        
        if not query:
            return jsonify({'error': 'Search query is required'}), 400
        
        print(f"Global search query: '{query}' for user: {user_id}")
        
        # Search across all videos
        search_results = search_all_videos(query, user_id)
        
        # Format results for frontend
        formatted_results = []
        for video in search_results:
            formatted_results.append({
                'videoId': video['videoId'],
                'filename': video['filename'],
                'userEmail': video['userEmail'],
                'createdAt': video['createdAt'],
                'duration': video['duration'],
                'fileSize': video['fileSize'],
                'status': video['status'],
                'relevance_score': video['relevance_score'],
                'transcript_preview': video['transcript'][:200] + '...' if video['transcript'] and len(video['transcript']) > 200 else video['transcript'],
                'visual_tags': video['visual_tags'][:5] if video['visual_tags'] else [],  # Limit to 5 tags
                'story_count': len(video['story_ids']) if video['story_ids'] and isinstance(video['story_ids'], list) else 0
            })
        
        print(f"Global search found {len(formatted_results)} videos")
        
        return jsonify({
            'success': True,
            'query': query,
            'results': formatted_results,
            'totalResults': len(formatted_results)
        })
        
    except Exception as e:
        import traceback
        print("Global search error:")
        traceback.print_exc()
        return jsonify({'error': f'Global search failed: {str(e)}'}), 500

@app.route('/videos', methods=['GET'])
def get_videos():
    """Get all videos for a user"""
    try:
        user_id = request.args.get('userId')  # Optional: filter by user
        
        print(f"DEBUG: Requested videos for user: {user_id}")
        videos = get_all_videos(user_id)
        print(f"DEBUG: Found {len(videos)} videos in database")
        
        for video in videos:
            print(f"DEBUG: Video ID: {video.get('videoId')}, Filename: {video.get('filename')}, User: {video.get('userEmail')}")
        
        # Format results for frontend
        formatted_videos = []
        for video in videos:
            formatted_videos.append({
                'videoId': video['videoId'],
                'filename': video['filename'],
                'userEmail': video['userEmail'],
                'createdAt': video['createdAt'],
                'duration': video['duration'],
                'fileSize': video['fileSize'],
                'status': video['status'],
                'transcript_preview': video['transcript'][:200] + '...' if video['transcript'] and len(video['transcript']) > 200 else video['transcript'],
                'visual_tags': video['visual_tags'][:5] if video['visual_tags'] else [],
                'story_count': len(video['story_ids']) if video['story_ids'] else 0
            })
        
        return jsonify({
            'success': True,
            'videos': formatted_videos,
            'totalVideos': len(formatted_videos)
        })
        
    except Exception as e:
        import traceback
        print("Get videos error:")
        traceback.print_exc()
        return jsonify({'error': f'Failed to get videos: {str(e)}'}), 500

def get_video_duration(video_path):
    """Get video duration using FFmpeg"""
    try:
        # DEBUG: Check PATH and FFmpeg availability
        import os
        print(f"DEBUG: Current PATH: {os.environ.get('PATH', 'PATH_NOT_SET')}")
        print(f"DEBUG: Looking for ffprobe in PATH...")
        
        # Check if ffprobe exists
        import shutil
        ffprobe_path = shutil.which('ffprobe')
        print(f"DEBUG: ffprobe found at: {ffprobe_path}")
        
        if not ffprobe_path:
            print("DEBUG: ffprobe not found in PATH, trying direct path...")
            # Try direct path
            direct_path = "C:\\ffmpeg\\bin\\ffprobe.exe"
            if os.path.exists(direct_path):
                print(f"DEBUG: Using direct path: {direct_path}")
                cmd = [
                    direct_path, '-v', 'quiet', '-show_entries', 'format=duration',
                    '-of', 'csv=p=0', video_path
                ]
            else:
                print(f"DEBUG: Direct path {direct_path} does not exist")
                return None
        else:
            cmd = [
                'ffprobe', '-v', 'quiet', '-show_entries', 'format=duration',
                '-of', 'csv=p=0', video_path
            ]
        
        print(f"DEBUG: Running command: {' '.join(cmd)}")
        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
        duration = float(result.stdout.strip())
        print(f"DEBUG: Successfully extracted duration: {duration}")
        return duration
    except Exception as e:
        print(f"Error getting video duration: {str(e)}")
        print(f"DEBUG: Full error details: {type(e).__name__}: {str(e)}")
        import traceback
        traceback.print_exc()
        return None

def search_transcript_with_timestamps(transcript, word_timestamps, query, video_id):
    """Search within transcript text using exact word timestamps and return complete phrases"""
    results = []
    
    query_words = [word.lower() for word in query.split()]
    
    # Get video duration for better timestamp validation
    video_metadata_file = os.path.join(UPLOAD_FOLDER, f"{video_id}_metadata.json")
    video_duration = None
    if os.path.exists(video_metadata_file):
        try:
            with open(video_metadata_file, 'r') as f:
                video_metadata = json.load(f)
            video_path = video_metadata.get('localPath')
            if video_path and os.path.exists(video_path):
                video_duration = get_video_duration(video_path)
                print(f"DEBUG: Video duration: {video_duration} seconds")
        except Exception as e:
            print(f"DEBUG: Could not get video duration: {str(e)}")
    
    # Use video duration for validation, or fallback to 1 hour
    max_duration = video_duration if video_duration else 3600
    
    # Validate timestamps - filter out any that seem unreasonable
    valid_word_timestamps = []
    for word_info in word_timestamps:
        # Debug: Check the structure of word_info
        print(f"DEBUG: Processing word_info: {word_info}")
        print(f"DEBUG: word_info type: {type(word_info)}")
        print(f"DEBUG: word_info keys: {list(word_info.keys()) if isinstance(word_info, dict) else 'Not a dict'}")
        
        # Check if word_info has the required structure
        if not isinstance(word_info, dict):
            print(f"DEBUG: Skipping non-dict word_info: {word_info}")
            continue
            
        if 'word' not in word_info:
            print(f"DEBUG: Skipping word_info without 'word' key: {word_info}")
            continue
            
        start_time = word_info.get('start_time', 0)
        end_time = word_info.get('end_time', 0)
        
        # Basic validation: timestamps should be reasonable
        if (start_time >= 0 and 
            end_time >= start_time and 
            end_time < max_duration and
            start_time < max_duration):
            valid_word_timestamps.append(word_info)
        else:
            print(f"DEBUG: Skipping invalid timestamp: {word_info} (max_duration: {max_duration})")
    
    print(f"DEBUG: Valid timestamps: {len(valid_word_timestamps)} out of {len(word_timestamps)}")
    
    if not valid_word_timestamps:
        print(f"DEBUG: No valid timestamps found, falling back to text search")
        return search_transcript(transcript, query, video_id)
    
    # Find all words that match any part of the query
    matching_word_indices = []
    for i, word_info in enumerate(valid_word_timestamps):
        try:
            word_lower = word_info['word'].lower()
            for query_word in query_words:
                if query_word in word_lower:
                    matching_word_indices.append(i)
                    break
        except KeyError as e:
            print(f"DEBUG: KeyError accessing word_info[{i}]: {e}")
            print(f"DEBUG: word_info[{i}] content: {word_info}")
            continue
        except Exception as e:
            print(f"DEBUG: Error processing word_info[{i}]: {e}")
            continue
    
    if not matching_word_indices:
        return results
    
    # Add individual word matches first (more precise)
    for match_idx in matching_word_indices:
        matching_word_info = valid_word_timestamps[match_idx]
        
        # Get context around this word (3 words before and after)
        context_start_idx = max(0, match_idx - 3)
        context_end_idx = min(len(valid_word_timestamps), match_idx + 4)
        context_words = valid_word_timestamps[context_start_idx:context_end_idx]
        
        # Filter context words to ensure they don't exceed video duration
        if video_duration:
            context_words = [w for w in context_words if w['end_time'] <= video_duration]
        
        if context_words:
            context_text = ' '.join([w['word'] for w in context_words])
            context_start_time = context_words[0]['start_time']
            context_end_time = context_words[-1]['end_time']
            
            # Calculate word match score
            word_score = 0.95  # High score for exact word matches
            
            results.append({
                'type': 'transcript',
                'start_time': matching_word_info['start_time'],
                'end_time': matching_word_info['end_time'],
                'score': word_score,
                'preview_text': context_text[:100] + '...' if len(context_text) > 100 else context_text,
                'full_text': context_text,
                'match_type': 'word_match',
                'matched_word': matching_word_info['word'],
                'context_start': context_start_time,
                'context_end': context_end_time
            })
    
    # Then add sentence-level results for better context
    for match_idx in matching_word_indices:
        matching_word_info = valid_word_timestamps[match_idx]

        # SENTENCE-LEVEL MATCH - expand to nearest sentence boundaries using punctuation
        # Helper to detect end of sentence
        def _ends_sentence(token_word: str) -> bool:
            if not token_word:
                return False
            stripped = token_word.strip()
            return stripped in ['.', '!', '?'] or stripped.endswith(('.', '!', '?'))

        # Find sentence start (the word after the previous terminator)
        i = match_idx - 1
        while i >= 0 and not _ends_sentence(valid_word_timestamps[i]['word']):
            i -= 1
        sentence_start_idx = i + 1 if i < match_idx else match_idx

        # Find sentence end (the word that is or ends with terminator)
        j = match_idx
        while j < len(valid_word_timestamps) - 1 and not _ends_sentence(valid_word_timestamps[j]['word']):
            j += 1
        sentence_end_idx = j
        
        # Ensure sentence boundaries don't exceed video duration
        if video_duration:
            # Filter out any words that exceed video duration
            sentence_words = []
            for word_info in valid_word_timestamps[sentence_start_idx:sentence_end_idx + 1]:
                if word_info['end_time'] <= video_duration:
                    sentence_words.append(word_info)
                else:
                    # Stop at first word that exceeds duration
                    break
            
            if sentence_words:
                sentence_start_idx = valid_word_timestamps.index(sentence_words[0])
                sentence_end_idx = valid_word_timestamps.index(sentence_words[-1])
            else:
                # If no valid words found, skip this sentence
                continue

        # Extract sentence words and timestamps
        sentence_words = valid_word_timestamps[sentence_start_idx:sentence_end_idx + 1]
        if sentence_words:
            sent_start_time = sentence_words[0]['start_time']
            sent_end_time = sentence_words[-1]['end_time']
            sentence_text = ' '.join([w['word'] for w in sentence_words])

            # Score favors coverage and concise sentences
            sentence_text_lower = sentence_text.lower()
            sentence_coverage = sum(1 for qw in query_words if qw in sentence_text_lower)
            coverage_ratio = sentence_coverage / max(1, len(query_words))
            length_factor = min(1.0, 30 / max(1, len(sentence_words)))
            sentence_score = 0.85 * coverage_ratio + 0.15 * length_factor

            # Avoid duplicates with other sentence results
            is_duplicate_sentence = False
            for existing in results:
                if existing.get('match_type') == 'sentence_match':
                    if abs(existing['start_time'] - sent_start_time) < 0.25 and abs(existing['end_time'] - sent_end_time) < 0.25:
                        is_duplicate_sentence = True
                        break

            if not is_duplicate_sentence:
                results.append({
                    'type': 'transcript',
                    'start_time': sent_start_time,
                    'end_time': sent_end_time,
                    'score': sentence_score,
                    'preview_text': sentence_text if len(sentence_text) <= 120 else sentence_text[:117] + '...',
                    'full_text': sentence_text,
                    'match_type': 'sentence_match',
                    'word_count': len(sentence_words),
                    'query_coverage': sentence_coverage,
                    'total_query_words': len(query_words)
                })
    
    # Sort results by score (highest first) and limit to top results
    results.sort(key=lambda x: x['score'], reverse=True)
    results = results[:10]  # Limit to top 10 results
    
    return results

def search_transcript(transcript, query, video_id):
    """Search within transcript text (fallback method without timestamps)"""
    results = []
    
    # Split transcript into sentences/segments - use multiple delimiters
    import re
    sentences = re.split(r'[.!?]+', transcript)
    
    for i, sentence in enumerate(sentences):
        sentence = sentence.strip()
        if not sentence:
            continue
        
        # Calculate match score
        sentence_lower = sentence.lower()
        query_words = query.lower().split()
        
        # Count matching words
        matches = sum(1 for word in query_words if word in sentence_lower)
        if matches == 0:
            continue
        
        # Calculate score based on word matches and position
        score = (matches / len(query_words)) * 0.8  # Base score for transcript matches
        
        # Estimate timestamp (rough approximation)
        # Assuming each sentence takes about 3-5 seconds
        estimated_start = i * 3
        estimated_end = estimated_start + 5
        
        results.append({
            'type': 'transcript',
            'start_time': estimated_start,
            'end_time': estimated_end,
            'score': score,
            'preview_text': sentence[:100] + '...' if len(sentence) > 100 else sentence,
            'full_text': sentence,
            'match_type': 'text_match'
        })
    
    return results

def search_tags(tags, query, video_id):
    """Search within visual tags"""
    results = []
    
    query_words = query.split()
    
    for tag in tags:
        tag_text = tag.get('tag', '').lower()
        timestamp = tag.get('timestamp', 0)
        
        # Check if any query word matches the tag
        matches = sum(1 for word in query_words if word in tag_text)
        if matches == 0:
            continue
        
        # Calculate score
        score = (matches / len(query_words)) * 0.9  # Higher score for tag matches
        
        # Create a 2-second segment around the tag timestamp
        start_time = max(0, timestamp - 1)
        end_time = timestamp + 1
        
        results.append({
            'type': 'tag',
            'start_time': start_time,
            'end_time': end_time,
            'score': score,
            'preview_text': f"Tag: {tag.get('tag', '')}",
            'full_text': f"Visual tag '{tag.get('tag', '')}' detected",
            'match_type': 'tag_match',
            'tag_confidence': tag.get('score', 0)
        })
    
    return results

def render_video_with_scenes(video_path, scenes, output_path, transition_duration=0.5):
    """Render video from scenes with transitions"""
    try:
        print(f"Starting video render: {video_path}")
        print(f"Output path: {output_path}")
        print(f"Scenes: {len(scenes)}")
        print(f"Transition duration: {transition_duration}")
        
        # Verify input video exists
        if not os.path.exists(video_path):
            print(f"ERROR: Input video file not found: {video_path}")
            return False
        
        # Create temporary directory
        temp_dir = tempfile.mkdtemp()
        print(f"Created temp directory: {temp_dir}")
        print(f"Temp directory exists: {os.path.exists(temp_dir)}")
        
        # Extract clips for each scene
        clip_paths = []
        for i, scene in enumerate(scenes):
            start_time = scene.get('start', 0)
            end_time = scene.get('end', 0)
            duration = end_time - start_time
            
            print(f"Processing scene {i+1}: start={start_time}, end={end_time}, duration={duration}")
            
            if duration <= 0:
                print(f"Skipping scene {i+1}: invalid duration")
                continue
            
            clip_path = os.path.join(temp_dir, f'clip_{i+1:03d}.mp4')
            
            # Use more robust FFmpeg command with optimized compression
            # Check if ffmpeg is in PATH, otherwise use direct path
            ffmpeg_path = 'ffmpeg'
            try:
                import shutil
                if not shutil.which('ffmpeg'):
                    direct_path = "C:\\ffmpeg\\bin\\ffmpeg.exe"
                    if os.path.exists(direct_path):
                        ffmpeg_path = direct_path
                        print(f"Using direct FFmpeg path: {ffmpeg_path}")
                    else:
                        print("ERROR: FFmpeg not found in PATH or direct path")
                        continue
            except Exception as e:
                print(f"Warning: Could not check FFmpeg path: {e}")
            
            cmd = [
                ffmpeg_path, '-i', video_path,
                '-ss', str(start_time),
                '-t', str(duration),
                '-c:v', 'libx264',  # Use H.264 codec instead of copy
                '-c:a', 'aac',      # Use AAC audio codec
                '-preset', 'medium', # Better compression than 'fast'
                '-crf', '28',       # Higher CRF for smaller file size
                '-maxrate', '2M',   # Limit bitrate to 2Mbps
                '-bufsize', '4M',   # Buffer size for rate limiting
                '-y',               # Overwrite output
                clip_path
            ]
            
            try:
                print(f"Running FFmpeg command: {' '.join(cmd)}")
                result = subprocess.run(cmd, capture_output=True, text=True, check=True)
                print(f"Clip {i+1} extracted successfully")
                
                if os.path.exists(clip_path):
                    file_size = os.path.getsize(clip_path)
                    print(f"Clip {i+1} file size: {file_size} bytes")
                    if file_size > 0:
                        clip_paths.append(clip_path)
                    else:
                        print(f"Clip {i+1} file is empty, skipping")
                else:
                    print(f"Clip {i+1} file was not created")
                    
            except subprocess.CalledProcessError as e:
                print(f"Error extracting clip {i+1}: {e.stderr}")
                print(f"FFmpeg return code: {e.returncode}")
                print(f"FFmpeg stdout: {e.stdout}")
                continue
        
        if not clip_paths:
            print("No clips were successfully extracted")
            return False
        
        print(f"Successfully extracted {len(clip_paths)} clips")
        
        # Apply transitions if multiple clips, otherwise use simple concatenation
        if len(clip_paths) > 1 and transition_duration > 0:
            print(f"Applying transitions with duration: {transition_duration}s")
            success = apply_transitions(clip_paths, output_path, temp_dir, transition_duration)
        else:
            print("Using simple concatenation (no transitions)")
            success = simple_concat(clip_paths, output_path, temp_dir)
        
        # Clean up temp files
        try:
            for clip_path in clip_paths:
                if os.path.exists(clip_path):
                    os.remove(clip_path)
            # Remove temp directory
            if os.path.exists(temp_dir):
                import shutil
                shutil.rmtree(temp_dir)
        except Exception as e:
            print(f"Warning: Error cleaning up temp files: {e}")
        
        print(f"Video rendering {'successful' if success else 'failed'}")
        return success
        
    except Exception as e:
        import traceback
        print(f"Render error: {str(e)}")
        print(f"Render traceback: {traceback.format_exc()}")
        return False

def apply_transitions(clip_paths, output_path, temp_dir, transition_duration):
    """Apply crossfade transitions between clips"""
    try:
        print(f"Applying transitions to {len(clip_paths)} clips with {transition_duration}s duration")
        print(f"Expected total duration: {len(clip_paths) * 5 + (len(clip_paths) - 1) * transition_duration}s (5s per scene + transitions)")
        
        # Create inputs list for FFmpeg
        inputs = []
        for clip_path in clip_paths:
            inputs.extend(['-i', clip_path])
        
        # Build filter complex for crossfade transitions
        if len(clip_paths) == 2:
            # Simple crossfade between 2 clips
            filter_str = f'[0:v][0:a][1:v][1:a]xfade=transition=fade:duration={transition_duration}:offset=1[v][a]'
        else:
            # For 3+ clips, use a simpler approach with proper offsets
            filter_parts = []
            current_offset = 0
            
            for i in range(len(clip_paths)):
                if i == 0:
                    # First clip: just label it
                    filter_parts.append(f'[{i}:v][{i}:a]')
                else:
                    # Subsequent clips: crossfade with calculated offset
                    filter_parts.append(f'[{i}:v][{i}:a]xfade=transition=fade:duration={transition_duration}:offset={current_offset}[v][a];')
                    current_offset += 1
            
            # Build the complete filter string
            filter_str = ''.join(filter_parts)
            
            # Ensure proper output labels
            if not filter_str.endswith('[v][a]'):
                # If the last part doesn't end with [v][a], add it
                filter_str = filter_str.rstrip(';') + '[v][a]'
        
        print(f"Filter complex: {filter_str}")
        
        # Execute FFmpeg command with transitions and optimized compression
        # Check if ffmpeg is in PATH, otherwise use direct path
        ffmpeg_path = 'ffmpeg'
        try:
            import shutil
            if not shutil.which('ffmpeg'):
                direct_path = "C:\\ffmpeg\\bin\\ffmpeg.exe"
                if os.path.exists(direct_path):
                    ffmpeg_path = direct_path
                    print(f"Using direct FFmpeg path for transitions: {ffmpeg_path}")
                else:
                    print("ERROR: FFmpeg not found in PATH or direct path for transitions")
                    return False
        except Exception as e:
            print(f"Warning: Could not check FFmpeg path for transitions: {e}")
        
        ffmpeg_cmd = [
            ffmpeg_path
        ] + inputs + [
            '-filter_complex', filter_str,
            '-map', '[v]',
            '-map', '[a]',
            '-c:v', 'libx264',
            '-c:a', 'aac',
            '-preset', 'medium',  # Better compression than 'fast'
            '-crf', '28',         # Higher CRF for smaller file size (18-28 is good)
            '-maxrate', '2M',     # Limit bitrate to 2Mbps
            '-bufsize', '4M',     # Buffer size for rate limiting
            '-movflags', '+faststart',  # Optimize for web streaming
            '-y',
            output_path
        ]
        
        print(f"Running transition command: {' '.join(ffmpeg_cmd)}")
        
        try:
            result = subprocess.run(ffmpeg_cmd, capture_output=True, text=True, check=True)
            print("Transitions applied successfully")
            
            if os.path.exists(output_path):
                file_size = os.path.getsize(output_path)
                print(f"Output file size: {file_size} bytes ({file_size/1024/1024:.2f} MB)")
                return True
            else:
                print("Output file was not created")
                return False
                
        except subprocess.CalledProcessError as e:
            print(f"Transition error: {e.stderr}")
            print(f"FFmpeg return code: {e.returncode}")
            print("Falling back to simple concatenation...")
            return simple_concat(clip_paths, output_path, temp_dir)
        
    except Exception as e:
        print(f"Transition error: {str(e)}")
        import traceback
        print(f"Traceback: {traceback.format_exc()}")
        print("Falling back to simple concatenation...")
        return simple_concat(clip_paths, output_path, temp_dir)

def simple_concat(clip_paths, output_path, temp_dir):
    """Simple concatenation without transitions"""
    try:
        print(f"Starting concatenation of {len(clip_paths)} clips")
        print(f"Output path: {output_path}")
        
        # Verify all clip files exist
        for i, clip_path in enumerate(clip_paths):
            if not os.path.exists(clip_path):
                print(f"ERROR: Clip {i+1} not found: {clip_path}")
                return False
            file_size = os.path.getsize(clip_path)
            print(f"Clip {i+1}: {clip_path} ({file_size} bytes)")
        
        # Create concat file with absolute paths
        concat_file = os.path.join(temp_dir, 'concat.txt')
        with open(concat_file, 'w', encoding='utf-8') as f:
            for clip_path in clip_paths:
                # Use absolute path to avoid issues
                abs_path = os.path.abspath(clip_path)
                # Escape single quotes in path
                escaped_path = abs_path.replace("'", "\\'")
                f.write(f"file '{escaped_path}'\n")
        
        print(f"Created concat file: {concat_file}")
        with open(concat_file, 'r') as f:
            print(f"Concat file contents:\n{f.read()}")
        
        # Use more robust concatenation command with optimized compression
        # Check if ffmpeg is in PATH, otherwise use direct path
        ffmpeg_path = 'ffmpeg'
        try:
            import shutil
            if not shutil.which('ffmpeg'):
                direct_path = "C:\\ffmpeg\\bin\\ffmpeg.exe"
                if os.path.exists(direct_path):
                    ffmpeg_path = direct_path
                    print(f"Using direct FFmpeg path for concatenation: {ffmpeg_path}")
                else:
                    print("ERROR: FFmpeg not found in PATH or direct path for concatenation")
                    return False
        except Exception as e:
            print(f"Warning: Could not check FFmpeg path for concatenation: {e}")
        
        ffmpeg_cmd = [
                    ffmpeg_path,
                    '-f', 'concat',
                    '-safe', '0',
                    '-i', concat_file,
                    '-c:v', 'libx264',  # Re-encode to ensure compatibility
                    '-c:a', 'aac',      # Re-encode audio
                    '-preset', 'medium', # Better compression than 'fast'
                    '-crf', '28',       # Higher CRF for smaller file size
                    '-maxrate', '2M',   # Limit bitrate to 2Mbps
                    '-bufsize', '4M',   # Buffer size for rate limiting
                    '-movflags', '+faststart',  # Optimize for web streaming
                    '-y',               # Overwrite output
                    output_path
                ]
        
        print(f"Running concatenation command: {' '.join(ffmpeg_cmd)}")
        
        # Run FFmpeg command
        result = subprocess.run(ffmpeg_cmd, capture_output=True, text=True, check=True)
        
        print("Concatenation completed successfully")
        print(f"Output file exists: {os.path.exists(output_path)}")
        
        if os.path.exists(output_path):
            file_size = os.path.getsize(output_path)
            print(f"Output file size: {file_size} bytes")
            
            if file_size > 0:
                print("‚úÖ Concatenation successful!")
                return True
            else:
                print("‚ùå Output file is empty")
                return False
        else:
            print("‚ùå Output file was not created")
            return False
            
    except subprocess.CalledProcessError as e:
        print(f"‚ùå Concatenation error: {e.stderr}")
        print(f"FFmpeg return code: {e.returncode}")
        print(f"FFmpeg stdout: {e.stdout}")
        return False
    except Exception as e:
        print(f"‚ùå Simple concat error: {str(e)}")
        import traceback
        print(f"Traceback: {traceback.format_exc()}")
        return False





def get_total_duration(scenes, transition_duration):
    """Calculate total duration of all scenes plus transitions"""
    total_duration = 0
    for i, scene in enumerate(scenes):
        scene_duration = scene.get('end', 0) - scene.get('start', 0)
        total_duration += scene_duration
        
        # Add transition duration (except for last scene)
        if i < len(scenes) - 1:
            total_duration += transition_duration
    
    return total_duration

@app.route('/add-test-data', methods=['POST'])
def add_test_data():
    """Add test data to the database for testing global search"""
    try:
        test_videos = [
            {
                'video_id': 'test_video_001',
                'filename': 'interview_ai.mp4',
                'transcript': 'This is an interview about artificial intelligence and machine learning. We discuss the future of AI technology and its impact on society.',
                'visual_tags': ['interview', 'artificial intelligence', 'machine learning', 'technology', 'future'],
                'content_type': 'interview'
            },
            {
                'video_id': 'test_video_002', 
                'filename': 'tutorial_editing.mp4',
                'transcript': 'In this video editing tutorial, we cover transitions, effects, and advanced editing techniques. Learn how to create professional videos.',
                'visual_tags': ['tutorial', 'video editing', 'transitions', 'effects', 'professional'],
                'content_type': 'tutorial'
            },
            {
                'video_id': 'test_video_003',
                'filename': 'presentation_data.mp4', 
                'transcript': 'This presentation covers data analysis and statistics. We examine trends, patterns, and insights from large datasets.',
                'visual_tags': ['presentation', 'data analysis', 'statistics', 'trends', 'patterns'],
                'content_type': 'presentation'
            },
            {
                'video_id': 'test_video_004',
                'filename': 'general_tech.mp4',
                'transcript': 'General discussion about technology trends, software development, and digital transformation in modern businesses.',
                'visual_tags': ['technology', 'software development', 'digital transformation', 'business'],
                'content_type': 'general'
            }
        ]
        
        inserted_count = 0
        for video in test_videos:
            video_metadata = {
                'videoId': video['video_id'],
                'userId': 'test_user_123',
                'userEmail': 'test@example.com',
                'filename': video['filename'],
                'localPath': f"uploads/videos/{video['filename']}",
                'fileSize': 1024000,  # 1MB
                'fileType': 'video/mp4',
                'createdAt': datetime.now().isoformat(),
                'status': 'processed',
                'duration': 120.0,  # 2 minutes
                'transcript': video['transcript'],
                'visual_tags': video['visual_tags'],
                'story_ids': [f"story_{video['video_id']}_1", f"story_{video['video_id']}_2"]
            }
            
            if save_video_metadata(video_metadata):
                inserted_count += 1
                print(f"‚úÖ Inserted {video['filename']} ({video['content_type']})")
            else:
                print(f"‚ùå Failed to insert {video['filename']}")
        
        return jsonify({
            'success': True,
            'message': f'Inserted {inserted_count} test videos',
            'inserted_count': inserted_count
        })
        
    except Exception as e:
        print(f"Error adding test data: {str(e)}")
        return jsonify({'error': f'Failed to add test data: {str(e)}'}), 500

def _generate_enhanced_inspirational_story_fallback(prompt: str, mode: str) -> str:
    """Return a specific, non-generic inspirational story without external AI, styled by `mode`.

    This enhanced version creates stories that are more specific to the user's prompt
    and less generic than the original fallback.
    """
    import random
    from hashlib import sha256

    prompt_clean = (prompt or 'your journey').strip().lower()
    
    # Extract key themes from the prompt for more specific storytelling
    prompt_keywords = prompt_clean.split()
    
    # Identify common themes for more specific content
    themes = {
        'learning': ['learn', 'study', 'practice', 'skill', 'knowledge', 'education'],
        'career': ['work', 'job', 'career', 'business', 'professional', 'success'],
        'health': ['fitness', 'health', 'exercise', 'diet', 'wellness', 'mental'],
        'relationships': ['love', 'relationship', 'family', 'friend', 'marriage', 'dating'],
        'creativity': ['art', 'music', 'write', 'create', 'design', 'creative'],
        'personal': ['confidence', 'fear', 'anxiety', 'growth', 'change', 'goal'],
        'social': ['speaking', 'social', 'network', 'communication', 'public'],
        'financial': ['money', 'finance', 'budget', 'saving', 'investment', 'debt']
    }
    
    # Determine the primary theme
    primary_theme = 'personal'  # default
    for theme, keywords in themes.items():
        if any(keyword in prompt_keywords for keyword in keywords):
            primary_theme = theme
            break

    # Seed pseudo-randomness using prompt+mode for consistency
    seed = int(sha256(f"{prompt_clean}|{mode}".encode("utf-8")).hexdigest(), 16) % (2**32)
    rnd = random.Random(seed)

    # Enhanced vocabulary with more specific, theme-aware content
    vocab = {
        'Hopeful': {
            'learning': {
                'openers': [
                    f"You opened the first page of {prompt_clean}, and something shifted.",
                    f"The moment you decided to tackle {prompt_clean}, a new chapter began.",
                    f"Learning {prompt_clean} started with a single, brave step forward."
                ],
                'challenges': [
                    f"The first attempts at {prompt_clean} felt clumsy and uncertain.",
                    f"You stumbled through the basics of {prompt_clean}, making mistakes.",
                    f"Progress with {prompt_clean} came slowly, one small victory at a time."
                ],
                'breakthroughs': [
                    f"Then came the day when {prompt_clean} started making sense.",
                    f"Something clicked with {prompt_clean}, and you felt a spark of confidence.",
                    f"You realized that {prompt_clean} wasn't impossible‚Äîjust unfamiliar."
                ]
            },
            'career': {
                'openers': [
                    f"You looked at your {prompt_clean} goals and felt both excited and nervous.",
                    f"The path toward {prompt_clean} success began with honest self-assessment.",
                    f"Your {prompt_clean} journey started when you dared to dream bigger."
                ],
                'challenges': [
                    f"The road to {prompt_clean} wasn't smooth‚Äîthere were setbacks and doubts.",
                    f"You faced rejection and criticism while pursuing {prompt_clean}.",
                    f"Balancing {prompt_clean} ambitions with daily responsibilities felt overwhelming."
                ],
                'breakthroughs': [
                    f"But you kept showing up for {prompt_clean}, even on the hard days.",
                    f"Small wins in {prompt_clean} started adding up to real progress.",
                    f"You discovered that {prompt_clean} success came from consistent effort."
                ]
            },
            'health': {
                'openers': [
                    f"You decided that {prompt_clean} was worth the effort, no matter how small.",
                    f"Your {prompt_clean} journey began with a simple promise to yourself.",
                    f"Taking care of your {prompt_clean} felt like an act of self-love."
                ],
                'challenges': [
                    f"Old habits around {prompt_clean} were hard to break.",
                    f"You faced days when {prompt_clean} felt impossible to prioritize.",
                    f"Progress with {prompt_clean} came in waves, not steady lines."
                ],
                'breakthroughs': [
                    f"But you learned that {prompt_clean} was about consistency, not perfection.",
                    f"Small changes in {prompt_clean} started creating bigger results.",
                    f"You discovered that {prompt_clean} was a journey, not a destination."
                ]
            },
            'relationships': {
                'openers': [
                    f"You realized that {prompt_clean} relationships required vulnerability.",
                    f"Your {prompt_clean} journey began with honest communication.",
                    f"Building better {prompt_clean} connections started with self-reflection."
                ],
                'challenges': [
                    f"Opening up about {prompt_clean} felt scary and uncertain.",
                    f"You faced misunderstandings and conflicts in your {prompt_clean} journey.",
                    f"Trusting others with your {prompt_clean} feelings took courage."
                ],
                'breakthroughs': [
                    f"But you learned that {prompt_clean} relationships grow through honesty.",
                    f"Small moments of connection in {prompt_clean} became meaningful.",
                    f"You discovered that {prompt_clean} love and friendship require patience."
                ]
            },
            'personal': {
                'openers': [
                    f"You looked at {prompt_clean} and decided it was time for change.",
                    f"Your {prompt_clean} journey began with a moment of honest self-reflection.",
                    f"Facing your {prompt_clean} fears felt like stepping into unknown territory."
                ],
                'challenges': [
                    f"The path through {prompt_clean} was filled with uncertainty and doubt.",
                    f"You faced setbacks and moments of wanting to give up on {prompt_clean}.",
                    f"Progress with {prompt_clean} came slowly, testing your patience."
                ],
                'breakthroughs': [
                    f"But you discovered that {prompt_clean} growth happens in small steps.",
                    f"Each day of working on {prompt_clean} made you stronger.",
                    f"You learned that {prompt_clean} transformation takes time and kindness."
                ]
            }
        },
        'Motivational': {
            'learning': {
                'openers': [
                    f"You committed to mastering {prompt_clean}, no matter what it took.",
                    f"The decision to excel at {prompt_clean} became your driving force.",
                    f"You set your sights on {prompt_clean} and refused to look back."
                ],
                'strategies': [
                    f"You broke down {prompt_clean} into manageable, daily actions.",
                    f"Every morning, you focused on one specific aspect of {prompt_clean}.",
                    f"You created a system for practicing {prompt_clean} consistently."
                ],
                'results': [
                    f"Your dedication to {prompt_clean} started showing real results.",
                    f"People noticed your growing expertise in {prompt_clean}.",
                    f"You became the person others turned to for {prompt_clean} advice."
                ]
            },
            'career': {
                'openers': [
                    f"You mapped out your {prompt_clean} success plan with precision.",
                    f"The vision of your {prompt_clean} future became your daily motivation.",
                    f"You decided that {prompt_clean} excellence was non-negotiable."
                ],
                'strategies': [
                    f"You invested in {prompt_clean} skills that would set you apart.",
                    f"Every decision you made aligned with your {prompt_clean} goals.",
                    f"You built a network of {prompt_clean} professionals who inspired you."
                ],
                'results': [
                    f"Your {prompt_clean} efforts started opening new opportunities.",
                    f"Recognition for your {prompt_clean} work began to flow naturally.",
                    f"You became known as someone who delivered {prompt_clean} results."
                ]
            },
            'health': {
                'openers': [
                    f"You committed to transforming your {prompt_clean} habits permanently.",
                    f"The vision of your {prompt_clean} future self became your motivation.",
                    f"You decided that {prompt_clean} excellence was worth every effort."
                ],
                'strategies': [
                    f"You created a {prompt_clean} routine that worked for your lifestyle.",
                    f"Every choice you made supported your {prompt_clean} goals.",
                    f"You surrounded yourself with {prompt_clean} inspiration and support."
                ],
                'results': [
                    f"Your {prompt_clean} transformation became visible to others.",
                    f"You started feeling stronger and more confident in your {prompt_clean} journey.",
                    f"Your {prompt_clean} success inspired others to make changes too."
                ]
            },
            'relationships': {
                'openers': [
                    f"You committed to building the {prompt_clean} relationships you deserved.",
                    f"The vision of deeper {prompt_clean} connections drove your actions.",
                    f"You decided that {prompt_clean} love and friendship were worth fighting for."
                ],
                'strategies': [
                    f"You learned to communicate {prompt_clean} needs clearly and kindly.",
                    f"Every interaction became an opportunity to strengthen {prompt_clean} bonds.",
                    f"You invested time and energy in the {prompt_clean} relationships that mattered."
                ],
                'results': [
                    f"Your {prompt_clean} relationships started growing deeper and stronger.",
                    f"You became known as someone who nurtured {prompt_clean} connections.",
                    f"Your {prompt_clean} love and friendship became a source of strength."
                ]
            },
            'personal': {
                'openers': [
                    f"You committed to conquering {prompt_clean} once and for all.",
                    f"The vision of your {prompt_clean} future self became your daily motivation.",
                    f"You decided that {prompt_clean} growth was non-negotiable."
                ],
                'strategies': [
                    f"You faced {prompt_clean} challenges head-on, one day at a time.",
                    f"Every setback in {prompt_clean} became a lesson in resilience.",
                    f"You built a support system for your {prompt_clean} journey."
                ],
                'results': [
                    f"Your {prompt_clean} transformation became visible to everyone around you.",
                    f"You became an inspiration to others facing similar {prompt_clean} challenges.",
                    f"Your {prompt_clean} success story became a testament to perseverance."
                ]
            }
        }
    }

    # Get the appropriate theme content for the mode
    mode_content = vocab.get(mode, vocab['Hopeful'])
    theme_content = mode_content.get(primary_theme, mode_content.get('personal', mode_content))
    
    # Generate the story using theme-specific content
    if mode == 'Hopeful':
        opener = rnd.choice(theme_content['openers'])
        challenge = rnd.choice(theme_content['challenges'])
        breakthrough = rnd.choice(theme_content['breakthroughs'])
        
        story = (
            f"{opener} At first, the path seemed overwhelming, but you took that first step anyway. "
            f"{challenge} There were moments when you wanted to give up, when the progress felt too slow. "
            f"But you reminded yourself that every expert was once a beginner, every master started with uncertainty. "
            f"{breakthrough} You learned to celebrate small victories and to be patient with the process. "
            f"Today, you're not the same person who started this journey. You're stronger, wiser, and more resilient. "
            f"Your {prompt_clean} story is still being written, but you're the author now, and every chapter gets better."
        )
    
    elif mode == 'Motivational':
        opener = rnd.choice(theme_content['openers'])
        strategy = rnd.choice(theme_content['strategies'])
        result = rnd.choice(theme_content['results'])
        
        story = (
            f"{opener} You knew that success in {prompt_clean} wouldn't come from wishing‚Äîit would come from doing. "
            f"{strategy} You refused to let excuses stand in your way. Every day, you showed up for your {prompt_clean} goals. "
            f"When others doubted, you doubled down. When obstacles appeared, you found ways around them. "
            f"{result} Your commitment to {prompt_clean} excellence became your signature. "
            f"People started noticing your dedication, your consistency, your refusal to settle for anything less than your best. "
            f"Your {prompt_clean} journey proves that when you commit fully to your goals, the universe conspires to help you succeed."
        )
    
    else:
        # For other modes, use a more generic but still specific approach
        story = (
            f"Your journey with {prompt_clean} began with a simple decision to change. "
            f"You faced challenges, learned from setbacks, and kept moving forward. "
            f"Today, you're stronger because of your {prompt_clean} experience. "
            f"Your story inspires others who are on similar paths. "
            f"Remember: every expert was once a beginner, and every success story started with a single step."
        )

    return story

@app.route('/generate-content-story', methods=['POST'])
def generate_content_based_inspirational_story():
    """Generate an inspirational story based on ACTUAL video content (transcript, visual tags, timestamps).
    
    Request JSON: { 
        "videoId": "...", 
        "mode": "Hopeful|Motivational|Funny|Emotional|Reflective",
        "prompt": "optional additional context"
    }
    Response JSON: { "story": "..." }
    """
    try:
        data = request.get_json(silent=True) or {}
        video_id = data.get('videoId', '').strip()
        mode = (data.get('mode') or 'Hopeful').strip()
        additional_prompt = (data.get('prompt') or '').strip()

        if not video_id:
            return jsonify({'error': 'Video ID is required'}), 400

        # Normalize and validate mode
        allowed_modes = {"Hopeful", "Motivational", "Funny", "Emotional", "Reflective"}
        if mode not in allowed_modes:
            mode = 'Hopeful'

        # Load video metadata to get actual content
        metadata_file = os.path.join('uploads', 'videos', f"{video_id}_metadata.json")
        if not os.path.exists(metadata_file):
            return jsonify({'error': 'Video not found'}), 404

        with open(metadata_file, 'r', encoding='utf-8') as f:
            video_metadata = json.load(f)

        # Extract actual video content
        transcript = video_metadata.get('transcript', '')
        visual_tags = video_metadata.get('visual_tags', [])
        word_timestamps = video_metadata.get('word_timestamps', [])
        duration = video_metadata.get('duration', 0)
        
        # Get key moments from timestamps
        key_moments = []
        if word_timestamps:
            # Extract key moments at different time intervals
            total_words = len(word_timestamps)
            if total_words > 0:
                # Get words at 25%, 50%, 75% of the video
                key_indices = [int(total_words * 0.25), int(total_words * 0.5), int(total_words * 0.75)]
                for idx in key_indices:
                    if idx < total_words:
                        moment = word_timestamps[idx]
                        key_moments.append({
                            'word': moment.get('word', ''),
                            'timestamp': moment.get('timestamp', 0),
                            'time_str': f"{moment.get('timestamp', 0):.1f}s"
                        })

        # Create content-based story using actual video data
        system_instructions = (
            "You are an expert AI story generator that creates inspirational stories based on ACTUAL video content. "
            "Use the provided video transcript, visual tags, and key moments to create a story that directly reflects "
            "what actually happened in the video. Make the story specific to the video content, not generic. "
            "Write a story of 250-350 words that captures the essence, emotions, and key moments from the video. "
            "Adjust the tone based on the selected mode while staying true to the actual video content. "
            "Include specific details from the transcript and visual elements. "
            "Avoid generic content - make it feel like a story about what actually happened in this specific video."
        )

        # Enhanced style guidance for content-based storytelling
        mode_guides = {
            'Hopeful': (
                "Tone: warm, uplifting, compassionate. Focus on positive moments, growth, and potential in the video content. "
                "Highlight moments of connection, learning, or progress shown in the video. "
                "Use the actual transcript and visual elements to show hope and possibility."
            ),
            'Motivational': (
                "Tone: energetic, determined, action-oriented. Focus on moments of effort, achievement, or determination in the video. "
                "Use the actual content to inspire action and show what's possible. "
                "Highlight specific actions, words, or moments that demonstrate motivation and drive."
            ),
            'Funny': (
                "Tone: lighthearted, witty, playful. Find humor in the actual video content, interactions, or situations. "
                "Use the transcript and visual elements to create relatable, amusing observations. "
                "Keep humor kind and supportive while being specific to the video content."
            ),
            'Emotional': (
                "Tone: tender, sincere, vulnerable. Focus on emotional moments, connections, or feelings expressed in the video. "
                "Use the actual transcript to capture authentic emotions and relationships. "
                "Highlight moments of vulnerability, love, or deep connection shown in the video."
            ),
            'Reflective': (
                "Tone: calm, thoughtful, insightful. Focus on moments of realization, learning, or wisdom in the video. "
                "Use the actual content to provide insights and deeper understanding. "
                "Highlight moments of self-discovery or meaningful observations from the video."
            ),
        }
        style_guide = mode_guides.get(mode, mode_guides['Hopeful'])

        # Prepare content summary for AI
        content_summary = f"""
VIDEO CONTENT ANALYSIS:
- Transcript: "{transcript[:500]}{'...' if len(transcript) > 500 else ''}"
- Visual Elements: {', '.join([tag.get('tag', '') for tag in visual_tags[:10]])}
- Duration: {duration:.1f} seconds
- Key Moments: {[f"{m['word']} at {m['time_str']}" for m in key_moments]}
- Additional Context: {additional_prompt if additional_prompt else 'None provided'}
"""

        # Generate content-based story using Gemini AI
        story_text = None
        if gemini_client:
            try:
                full_prompt = (
                    f"{system_instructions}\n\n"
                    f"STYLE GUIDE FOR {mode}: {style_guide}\n\n"
                    f"{content_summary}\n\n"
                    f"STORY MODE: {mode}\n\n"
                    f"Create an inspirational story that is SPECIFICALLY based on this video content. "
                    f"Use the actual transcript, visual elements, and key moments to tell a story about what happened in this video. "
                    f"Make it feel personal and authentic to the actual content, not generic. "
                    f"Write a story that captures the essence and meaning of what was shared in this specific video."
                )
                response = gemini_client.models.generate_content(
                    model="gemini-2.5-flash",
                    contents=full_prompt
                )
                story_text = (response.text or '').strip()
            except Exception as model_err:
                print(f"Gemini content-based story error: {str(model_err)}")
                # Fallback to content-based local generator
                story_text = _generate_content_based_story_fallback(transcript, visual_tags, key_moments, mode, additional_prompt)
        else:
            story_text = _generate_content_based_story_fallback(transcript, visual_tags, key_moments, mode, additional_prompt)

        # Ensure target length
        try:
            import re as _re
            word_count = len([w for w in _re.findall(r"\b\w+\b", story_text)])
            if word_count < 200 and gemini_client:
                refine_prompt = (
                    f"Expand this story to 250-350 words while keeping it SPECIFIC to the video content. "
                    f"Add more details from the transcript and visual elements. "
                    f"Follow this style guide: {style_guide}. "
                    f"Return only the expanded story as plain paragraphs.\n\nCURRENT STORY:\n{story_text}"
                )
                refine_resp = gemini_client.models.generate_content(
                    model="gemini-2.5-flash",
                    contents=refine_prompt
                )
                refined = (refine_resp.text or '').strip()
                if refined:
                    story_text = refined
        except Exception as _:
            pass

        # Cleanup
        try:
            cleaned = story_text.strip()
            if cleaned.startswith('```'):
                lines = [l for l in cleaned.split('\n') if not l.strip().startswith('```')]
                cleaned = '\n'.join(lines).strip()
            story_text = cleaned
        except Exception:
            pass

        return jsonify({
            "story": story_text,
            "videoId": video_id,
            "mode": mode,
            "contentUsed": {
                "transcriptLength": len(transcript),
                "visualTags": len(visual_tags),
                "keyMoments": len(key_moments),
                "duration": duration
            }
        })

    except Exception as e:
        import traceback
        print("Content-based inspirational story generation error:\n" + traceback.format_exc())
        return jsonify({'error': f'Failed to generate content-based story: {str(e)}'}), 500

def _generate_content_based_story_fallback(transcript: str, visual_tags: list, key_moments: list, mode: str, additional_prompt: str = "") -> str:
    """Generate a content-based inspirational story using actual video content when Gemini AI is unavailable.
    
    This function creates stories that are specifically based on the video's transcript, visual elements, and key moments.
    """
    import random
    from hashlib import sha256

    # Clean and prepare content
    transcript_clean = (transcript or '').strip()
    visual_elements = [tag.get('tag', '') for tag in visual_tags if tag.get('tag')]
    key_words = [moment.get('word', '') for moment in key_moments if moment.get('word')]
    
    # Extract key themes from transcript
    transcript_words = transcript_clean.lower().split()
    
    # Identify content themes
    content_themes = {
        'personal': ['i', 'me', 'my', 'myself', 'personal', 'experience'],
        'relationship': ['love', 'relationship', 'partner', 'family', 'friend', 'together'],
        'achievement': ['success', 'achieved', 'accomplished', 'goal', 'dream', 'reached'],
        'learning': ['learned', 'discovered', 'realized', 'understood', 'found'],
        'challenge': ['difficult', 'challenge', 'struggle', 'overcame', 'faced'],
        'emotion': ['feel', 'felt', 'happy', 'sad', 'excited', 'nervous', 'proud']
    }
    
    # Determine primary content theme
    primary_theme = 'personal'  # default
    theme_scores = {}
    for theme, keywords in content_themes.items():
        score = sum(1 for word in transcript_words if word in keywords)
        theme_scores[theme] = score
    
    if theme_scores:
        primary_theme = max(theme_scores, key=theme_scores.get)

    # Seed for consistency
    content_hash = f"{transcript_clean[:100]}|{mode}|{primary_theme}"
    seed = int(sha256(content_hash.encode("utf-8")).hexdigest(), 16) % (2**32)
    rnd = random.Random(seed)

    # Get key content elements
    key_visuals = visual_elements[:5] if visual_elements else ['video content']
    key_transcript_words = transcript_words[:20] if transcript_words else ['content']
    
    # Extract meaningful phrases from transcript
    meaningful_phrases = []
    if transcript_clean:
        sentences = transcript_clean.split('.')
        meaningful_phrases = [s.strip() for s in sentences if len(s.strip()) > 10][:3]

    # Create content-based story structure
    if mode == 'Hopeful':
        story = _create_hopeful_content_story(transcript_clean, key_visuals, meaningful_phrases, primary_theme, rnd)
    elif mode == 'Motivational':
        story = _create_motivational_content_story(transcript_clean, key_visuals, meaningful_phrases, primary_theme, rnd)
    elif mode == 'Funny':
        story = _create_funny_content_story(transcript_clean, key_visuals, meaningful_phrases, primary_theme, rnd)
    elif mode == 'Emotional':
        story = _create_emotional_content_story(transcript_clean, key_visuals, meaningful_phrases, primary_theme, rnd)
    elif mode == 'Reflective':
        story = _create_reflective_content_story(transcript_clean, key_visuals, meaningful_phrases, primary_theme, rnd)
    else:
        story = _create_hopeful_content_story(transcript_clean, key_visuals, meaningful_phrases, primary_theme, rnd)

    return story

def _create_hopeful_content_story(transcript: str, visuals: list, phrases: list, theme: str, rnd) -> str:
    """Create a hopeful story based on actual video content."""
    
    # Extract positive elements from content
    positive_words = ['love', 'happy', 'good', 'great', 'wonderful', 'amazing', 'beautiful', 'success', 'achievement']
    found_positives = [word for word in transcript.lower().split() if word in positive_words]
    
    visual_desc = ', '.join(visuals[:3]) if visuals else 'this moment'
    key_phrase = phrases[0] if phrases else 'this experience'
    
    story = (
        f"In this video, we see {visual_desc} come to life through authentic moments and genuine expression. "
        f"The words shared‚Äî'{key_phrase}'‚Äîreveal a journey of growth and discovery. "
    )
    
    if found_positives:
        story += (
            f"There's something beautiful about how {found_positives[0]} emerges naturally from this experience. "
            f"It's a reminder that even in our everyday moments, there's potential for connection and meaning. "
        )
    
    story += (
        f"What makes this content special is its authenticity. It's not about perfection‚Äîit's about real moments, "
        f"real emotions, and real connections. The visual elements of {visual_desc} create a backdrop for "
        f"stories that resonate with our own experiences. "
        f"Every video like this reminds us that hope isn't found in grand gestures, but in the small, "
        f"genuine moments we share with others. This content captures that truth beautifully."
    )
    
    return story

def _create_motivational_content_story(transcript: str, visuals: list, phrases: list, theme: str, rnd) -> str:
    """Create a motivational story based on actual video content."""
    
    action_words = ['did', 'made', 'created', 'built', 'achieved', 'accomplished', 'reached', 'overcame']
    found_actions = [word for word in transcript.lower().split() if word in action_words]
    
    visual_desc = ', '.join(visuals[:3]) if visuals else 'this content'
    key_phrase = phrases[0] if phrases else 'this achievement'
    
    story = (
        f"This video captures the essence of action and determination. Through {visual_desc}, "
        f"we witness the power of showing up and doing the work. "
        f"The message here‚Äî'{key_phrase}'‚Äîspeaks to the importance of taking steps forward. "
    )
    
    if found_actions:
        story += (
            f"What's inspiring is how this content demonstrates that {found_actions[0]} is possible "
            f"when we commit to our goals. It's not about having all the answers‚Äîit's about "
            f"starting with what we have and building from there. "
        )
    
    story += (
        f"The visual elements of {visual_desc} serve as a powerful reminder that our actions "
        f"create our reality. Every frame of this video shows what's possible when we "
        f"refuse to let fear or doubt stop us from moving forward. "
        f"This content proves that motivation isn't something we wait for‚Äîit's something we create "
        f"through consistent action and unwavering commitment to our vision."
    )
    
    return story

def _create_funny_content_story(transcript: str, visuals: list, phrases: list, theme: str, rnd) -> str:
    """Create a funny story based on actual video content."""
    
    visual_desc = ', '.join(visuals[:3]) if visuals else 'this delightful moment'
    key_phrase = phrases[0] if phrases else 'this amusing situation'
    
    story = (
        f"There's something wonderfully human about this video. Through {visual_desc}, "
        f"we get a front-row seat to the kind of moments that make life entertaining. "
        f"The content here‚Äî'{key_phrase}'‚Äîcaptures that perfect blend of effort and "
        f"the inevitable plot twists that make any good story worth watching. "
    )
    
    story += (
        f"What makes this content so relatable is its authenticity. It's not trying to be perfect‚Äî"
        f"it's just being real, and that's where the humor naturally emerges. "
        f"The visual elements of {visual_desc} create a backdrop for the kind of "
        f"everyday adventures that we all experience but rarely capture on camera. "
        f"This video reminds us that sometimes the best comedy comes from simply "
        f"showing up and being willing to laugh at ourselves along the way."
    )
    
    return story

def _create_emotional_content_story(transcript: str, visuals: list, phrases: list, theme: str, rnd) -> str:
    """Create an emotional story based on actual video content."""
    
    emotion_words = ['love', 'feel', 'heart', 'sad', 'happy', 'miss', 'care', 'important']
    found_emotions = [word for word in transcript.lower().split() if word in emotion_words]
    
    visual_desc = ', '.join(visuals[:3]) if visuals else 'this intimate moment'
    key_phrase = phrases[0] if phrases else 'this emotional experience'
    
    story = (
        f"This video captures something deeply human and profoundly moving. Through {visual_desc}, "
        f"we witness the raw beauty of authentic emotion and genuine connection. "
        f"The words shared‚Äî'{key_phrase}'‚Äîspeak to the heart of what makes us human. "
    )
    
    if found_emotions:
        story += (
            f"There's a tenderness in how this content explores {found_emotions[0]}, "
            f"reminding us that vulnerability is not weakness‚Äîit's strength in its purest form. "
            f"It's about the courage to feel deeply and share those feelings with others. "
        )
    
    story += (
        f"The visual elements of {visual_desc} create a safe space for emotions to unfold naturally. "
        f"This content reminds us that the most meaningful moments in life aren't always "
        f"the loudest or most dramatic‚Äîthey're often the quiet ones where we allow ourselves "
        f"to be seen and heard exactly as we are."
    )
    
    return story

def _create_reflective_content_story(transcript: str, visuals: list, phrases: list, theme: str, rnd) -> str:
    """Create a reflective story based on actual video content."""
    
    insight_words = ['realized', 'learned', 'discovered', 'understood', 'found', 'saw', 'recognized']
    found_insights = [word for word in transcript.lower().split() if word in insight_words]
    
    visual_desc = ', '.join(visuals[:3]) if visuals else 'this thoughtful moment'
    key_phrase = phrases[0] if phrases else 'this realization'
    
    story = (
        f"This video invites us into a moment of genuine reflection and self-discovery. "
        f"Through {visual_desc}, we witness the kind of insight that comes from "
        f"paying attention to our experiences. The content here‚Äî'{key_phrase}'‚Äî"
        f"captures a moment of clarity that many of us can relate to. "
    )
    
    if found_insights:
        story += (
            f"There's wisdom in how this content explores what it means to {found_insights[0]}, "
            f"reminding us that growth often comes from simply being present with our experiences. "
            f"It's about the quiet moments of understanding that change everything. "
        )
    
    story += (
        f"The visual elements of {visual_desc} serve as a metaphor for the way we process "
        f"our own experiences. This content reminds us that reflection isn't about "
        f"finding all the answers‚Äîit's about asking better questions and being open "
        f"to the insights that emerge when we slow down and pay attention."
    )
    
    return story

@app.route('/generate-content-emotional-journey', methods=['POST'])
def generate_content_based_emotional_journey():
    """Generate an emotional journey analysis and contrasting stories based on ACTUAL video content.
    
    Request JSON: { 
        "videoId": "...", 
        "analysisType": "emotional_patterns|contrasting_stories|both"
    }
    Response JSON: { 
        "emotionalAnalysis": "...",
        "contrastingStories": {...},
        "videoId": "..."
    }
    """
    try:
        data = request.get_json(silent=True) or {}
        video_id = data.get('videoId', '').strip()
        analysis_type = data.get('analysisType', 'both').strip()

        if not video_id:
            return jsonify({'error': 'Video ID is required'}), 400

        # Load video metadata to get actual content
        metadata_file = os.path.join('uploads', 'videos', f"{video_id}_metadata.json")
        if not os.path.exists(metadata_file):
            return jsonify({'error': 'Video not found'}), 404

        with open(metadata_file, 'r', encoding='utf-8') as f:
            video_metadata = json.load(f)

        # Extract actual video content
        transcript = video_metadata.get('transcript', '')
        visual_tags = video_metadata.get('visual_tags', [])
        word_timestamps = video_metadata.get('word_timestamps', [])
        duration = video_metadata.get('duration', 0)
        
        # Get emotional keywords from transcript
        emotional_keywords = analyze_emotions_from_text(transcript)
        
        # Prepare content summary for AI
        content_summary = f"""
VIDEO CONTENT ANALYSIS:
- Transcript: "{transcript[:800]}{'...' if len(transcript) > 800 else ''}"
- Visual Elements: {', '.join([tag.get('tag', '') for tag in visual_tags[:15]])}
- Duration: {duration:.1f} seconds
- Emotional Keywords Detected: {', '.join(emotional_keywords[:10])}
- Word Count: {len(transcript.split())}
"""

        emotional_analysis = ""
        contrasting_stories = {}

        # Generate emotional analysis using Gemini AI
        if analysis_type in ['emotional_patterns', 'both']:
            emotional_analysis = _generate_content_based_emotional_analysis(
                transcript, visual_tags, emotional_keywords, content_summary
            )

        # Generate contrasting stories using Gemini AI
        if analysis_type in ['contrasting_stories', 'both']:
            contrasting_stories = _generate_content_based_contrasting_stories(
                transcript, visual_tags, emotional_keywords, content_summary
            )

        return jsonify({
            "emotionalAnalysis": emotional_analysis,
            "contrastingStories": contrasting_stories,
            "videoId": video_id,
            "analysisType": analysis_type,
            "contentUsed": {
                "transcriptLength": len(transcript),
                "visualTags": len(visual_tags),
                "emotionalKeywords": len(emotional_keywords),
                "duration": duration
            }
        })

    except Exception as e:
        import traceback
        print("Content-based emotional journey generation error:\n" + traceback.format_exc())
        return jsonify({'error': f'Failed to generate content-based emotional journey: {str(e)}'}), 500

def _generate_content_based_emotional_analysis(transcript: str, visual_tags: list, emotional_keywords: list, content_summary: str) -> str:
    """Generate emotional analysis based on actual video content using Gemini AI."""
    
    system_instructions = (
        "You are an expert emotional intelligence analyst specializing in video content analysis. "
        "Analyze the provided video content (transcript, visual elements, emotional keywords) to create "
        "a comprehensive emotional analysis. Focus on the actual emotions, themes, and patterns present in the video. "
        "Write a detailed analysis of 200-300 words that captures the emotional journey and psychological insights "
        "from the video content. Be specific to the actual content, not generic."
    )

    analysis_prompt = (
        f"{system_instructions}\n\n"
        f"{content_summary}\n\n"
        f"EMOTIONAL ANALYSIS TASK:\n"
        f"Create a detailed emotional analysis of this video content. Consider:\n"
        f"- Emotional themes and patterns in the transcript\n"
        f"- Visual elements that contribute to emotional tone\n"
        f"- Psychological insights about the speaker's emotional state\n"
        f"- Key emotional moments and transitions\n"
        f"- Overall emotional arc of the content\n\n"
        f"Write a professional, insightful analysis that would help someone understand "
        f"the emotional depth and psychological aspects of this specific video content."
    )

    if gemini_client:
        try:
            response = gemini_client.models.generate_content(
                model="gemini-2.5-flash",
                contents=analysis_prompt
            )
            analysis = (response.text or '').strip()
            return analysis
        except Exception as model_err:
            print(f"Gemini emotional analysis error: {str(model_err)}")
            return _generate_emotional_analysis_fallback(transcript, visual_tags, emotional_keywords)
    else:
        return _generate_emotional_analysis_fallback(transcript, visual_tags, emotional_keywords)

def _generate_content_based_contrasting_stories(transcript: str, visual_tags: list, emotional_keywords: list, content_summary: str) -> dict:
    """Generate contrasting stories based on actual video content using Gemini AI."""
    
    system_instructions = (
        "You are an expert storyteller specializing in creating contrasting narrative paths based on video content. "
        "Using the provided video content (transcript, visual elements, emotional keywords), create two contrasting stories: "
        "1. A POSITIVE PATH story showing growth, resilience, and positive outcomes "
        "2. A NEGATIVE PATH story showing struggles, setbacks, and learning opportunities "
        "Both stories should be based on the actual content and emotions present in the video. "
        "Write each story in 150-200 words, making them specific to the video's content and themes. "
        "IMPORTANT: Both stories must be equally detailed and engaging. Do not leave either story incomplete."
    )

    stories_prompt = (
        f"{system_instructions}\n\n"
        f"{content_summary}\n\n"
        f"CONTRASTING STORIES TASK:\n"
        f"Create two contrasting narrative paths based on this video content:\n\n"
        f"POSITIVE PATH:\n"
        f"Write a detailed story (150-200 words) showing how the themes and emotions in this video could lead to "
        f"growth, success, and positive transformation. Focus on resilience, learning, and positive outcomes.\n\n"
        f"NEGATIVE PATH:\n"
        f"Write a detailed story (150-200 words) showing how the same themes and emotions could lead to "
        f"struggles, setbacks, and difficult lessons. Focus on challenges, obstacles, and learning opportunities.\n\n"
        f"FORMAT REQUIREMENTS:\n"
        f"- Start each story with 'POSITIVE PATH:' and 'NEGATIVE PATH:' headers\n"
        f"- Make both stories equally detailed and engaging\n"
        f"- Base stories directly on the video content provided\n"
        f"- Ensure both stories are complete and well-developed"
    )

    if gemini_client:
        try:
            response = gemini_client.models.generate_content(
                model="gemini-2.5-flash",
                contents=stories_prompt
            )
            stories_text = (response.text or '').strip()
            
            # Parse the response to extract both stories
            stories = _parse_contrasting_stories(stories_text)
            return stories
        except Exception as model_err:
            print(f"Gemini contrasting stories error: {str(model_err)}")
            return _generate_contrasting_stories_fallback(transcript, visual_tags, emotional_keywords)
    else:
        return _generate_contrasting_stories_fallback(transcript, visual_tags, emotional_keywords)

def _parse_contrasting_stories(stories_text: str) -> dict:
    """Parse the AI response to extract positive and negative path stories."""
    
    # Try to extract stories based on specific headers
    positive_story = ""
    negative_story = ""
    
    # Split by lines and look for specific headers
    lines = stories_text.split('\n')
    current_section = None
    current_content = []
    
    for line in lines:
        line_lower = line.strip().lower()
        
        # Check for section headers
        if 'positive path' in line_lower:
            if current_section == 'positive' and current_content:
                positive_story = '\n'.join(current_content).strip()
            current_section = 'positive'
            current_content = []
        elif 'negative path' in line_lower or 'challenging path' in line_lower:
            if current_section == 'positive' and current_content:
                positive_story = '\n'.join(current_content).strip()
            current_section = 'negative'
            current_content = []
        else:
            if current_section:
                current_content.append(line)
    
    # Save the last section
    if current_section == 'positive' and current_content:
        positive_story = '\n'.join(current_content).strip()
    elif current_section == 'negative' and current_content:
        negative_story = '\n'.join(current_content).strip()
    
    # If we still don't have both stories, try alternative parsing
    if not positive_story or not negative_story:
        # Try splitting by double newlines
        sections = stories_text.split('\n\n')
        if len(sections) >= 2:
            positive_story = sections[0].strip()
            negative_story = sections[1].strip()
        else:
            # Final fallback: split by length
            mid_point = len(stories_text) // 2
            positive_story = stories_text[:mid_point].strip()
            negative_story = stories_text[mid_point:].strip()
    
    return {
        "positivePath": positive_story,
        "negativePath": negative_story
    }

def _generate_emotional_analysis_fallback(transcript: str, visual_tags: list, emotional_keywords: list) -> str:
    """Generate emotional analysis fallback using actual video content."""
    
    visual_elements = [tag.get('tag', '') for tag in visual_tags if tag.get('tag')]
    key_emotions = emotional_keywords[:5] if emotional_keywords else ['content']
    
    analysis = (
        f"This video content reveals a complex emotional landscape through both spoken words and visual elements. "
        f"The transcript contains {len(transcript.split())} words that convey various emotional themes, "
        f"while the visual elements of {', '.join(visual_elements[:3]) if visual_elements else 'this content'} "
        f"create an additional layer of emotional context.\n\n"
        f"Key emotional patterns detected include {', '.join(key_emotions)}, suggesting a narrative that "
        f"touches on universal human experiences. The content appears to explore themes of personal growth, "
        f"relationships, and self-discovery, with moments of vulnerability and strength interwoven throughout.\n\n"
        f"This emotional analysis provides insight into the psychological depth of the content, "
        f"revealing how personal experiences can resonate with broader human emotions and experiences."
    )
    
    return analysis

def _generate_contrasting_stories_fallback(transcript: str, visual_tags: list, emotional_keywords: list) -> dict:
    """Generate contrasting stories fallback using actual video content."""
    
    visual_elements = [tag.get('tag', '') for tag in visual_tags if tag.get('tag')]
    key_emotions = emotional_keywords[:3] if emotional_keywords else ['experience']
    
    positive_story = (
        f"Based on the themes present in this video content, a positive path emerges where "
        f"the experiences shared become catalysts for growth and transformation. "
        f"The emotional elements of {', '.join(key_emotions)} evolve into sources of strength, "
        f"leading to deeper self-understanding and meaningful connections with others. "
        f"Through the visual journey of {', '.join(visual_elements[:2]) if visual_elements else 'this content'}, "
        f"we see how vulnerability can become a foundation for resilience, and how "
        f"personal challenges can transform into opportunities for positive change."
    )
    
    negative_story = (
        f"Alternatively, the same emotional themes in this video content could lead down a more "
        f"negative path, where the experiences shared become sources of ongoing struggle. "
        f"The emotions of {', '.join(key_emotions)} might intensify, creating barriers to "
        f"personal growth and connection. The visual elements of {', '.join(visual_elements[:2]) if visual_elements else 'this content'} "
        f"could represent moments of isolation rather than connection, where "
        f"vulnerability feels like weakness rather than strength. This path shows how "
        f"the same experiences can lead to different outcomes depending on perspective and support."
    )
    
    return {
        "positivePath": positive_story,
        "negativePath": negative_story
    }

def analyze_emotions_from_text(transcript: str) -> list:
    """Analyze emotions from text and return emotional keywords."""
    if not transcript:
        return []
    
    transcript_lower = transcript.lower()
    
    # Define emotional keyword categories
    emotion_categories = {
        'happy': ['happy', 'joy', 'fun', 'great', 'awesome', 'love', 'excited', 'wonderful', 'amazing', 'delight', 'smile', 'celebrate'],
        'sad': ['sad', 'sorry', 'cry', 'bad', 'upset', 'lonely', 'pain', 'tears', 'loss', 'grief', 'depressed'],
        'angry': ['angry', 'mad', 'furious', 'rage', 'annoyed', 'frustrated', 'irritated', 'furious'],
        'calm': ['calm', 'relax', 'peace', 'serene', 'quiet', 'gentle', 'soothing', 'tranquil'],
        'excited': ['excited', 'thrill', 'wow', 'incredible', 'epic', 'energy', 'hype', 'amazing'],
        'nervous': ['nervous', 'anxious', 'worried', 'scared', 'fear', 'afraid', 'tense'],
        'confident': ['confident', 'proud', 'strong', 'capable', 'sure', 'certain', 'determined'],
        'grateful': ['grateful', 'thankful', 'blessed', 'appreciate', 'thank', 'gratitude'],
        'surprised': ['surprised', 'shocked', 'amazed', 'astonished', 'unexpected', 'wow'],
        'hopeful': ['hopeful', 'hope', 'optimistic', 'positive', 'future', 'dream', 'aspire']
    }
    
    # Count emotional keywords in transcript
    emotion_counts = {}
    for emotion, keywords in emotion_categories.items():
        count = sum(1 for keyword in keywords if keyword in transcript_lower)
        if count > 0:
            emotion_counts[emotion] = count
    
    # Convert to list of emotional keywords
    emotional_keywords = []
    for emotion, count in emotion_counts.items():
        emotional_keywords.append(emotion)
        # Add some variations based on count
        if count > 2:
            emotional_keywords.append(f"strong_{emotion}")
        if count > 5:
            emotional_keywords.append(f"intense_{emotion}")
    
    return emotional_keywords

if __name__ == "__main__":
    # Initialize database and users table
    init_database()
    ensure_users_table()
    
    print("üöÄ Starting Footage Flow Backend Server...")
    print("üìç Server will run on: http://127.0.0.1:5000")
    print("üîç Health Check: http://127.0.0.1:5000/health")
    print("üìù API Documentation: http://127.0.0.1:5000/docs")
    print("\n‚èπÔ∏è  Press Ctrl+C to stop the server...")
    
    # Run the Flask app
    app.run(host='127.0.0.1', port=5000, debug=True, threaded=True)
